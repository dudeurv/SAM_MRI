{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dudeurv/SAM_MRI/blob/main/RISE_MICCAI_Winter_School_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medical Image Segmentation Tutorial for RISE-MICCAI Winter School\n",
        "\n",
        "Author of this tutorial: [Wenjia Bai](https://www.doc.ic.ac.uk/~wbai)\n",
        "\n",
        "Last modified: 11/11/2022\n",
        "\n",
        "In this tutorial, you will develop and train a convolutional neural network for brain tumour image segmentation. Please read the text and code and fill in the missing code blocks that look like this:\n",
        "\n",
        "```\n",
        "### Insert your code ###\n",
        "[Insert your code here]\n",
        "### End of your code ###\n",
        "```\n",
        "\n",
        "When you train the network, you may need to use the GPU resources on Google Colab. Please go to the menu, Runtime - Change runtime type, and select **GPU** as the hardware acceleartor.\n"
      ],
      "metadata": {
        "id": "6XRxHiKdGHiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "# These libraries should be sufficient for this tutorial.\n",
        "# However, if any other library is needed, please install by yourself.\n",
        "import tarfile\n",
        "import imageio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors"
      ],
      "metadata": {
        "id": "Eq1KWmR3HWYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the imaging dataset\n",
        "\n",
        "The dataset is curated from the brain imaging dataset in [Medical Decathlon Challenge](http://medicaldecathlon.com/). To save the storage and reduce the computational cost for this tutorial, we extract 2D image slices from T1-Gd contrast enhanced 3D brain volumes and downsample the images.\n",
        "\n",
        "The dataset consists of a training set and a test set. Each image is of dimension 120 x 120, with a corresponding label map of the same dimension. There are four number of classes in the label map:\n",
        "\n",
        "- 0: background\n",
        "- 1: edema\n",
        "- 2: non-enhancing tumour\n",
        "- 3: enhancing tumour"
      ],
      "metadata": {
        "id": "w4TX-CXBHW4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!wget https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
        "\n",
        "# Unzip the '.tar.gz' file to the current directory\n",
        "datafile = tarfile.open('Task01_BrainTumour_2D.tar.gz')\n",
        "datafile.extractall()\n",
        "datafile.close()"
      ],
      "metadata": {
        "id": "mt93oQ8xZkE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa967bcc-9cae-4c79-901a-f7e5aab9ea79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-22 11:22:03--  https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz [following]\n",
            "--2022-11-22 11:22:03--  https://www.dropbox.com/s/raw/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb62b0e098a8098974da3a55a58.dl.dropboxusercontent.com/cd/0/inline/BxNpp_kcz7uB3MCng_wAuf9uD-cKltwqcu5bRt4v6JzxZMcV-IjxQDB9Pe7mod75xdlh7tb-qEiETUX2TXPyGDN9NIhyiQpGGgZ1bLTa83VUxTBUwdK0exnAp-Olikyi65WU_Z9zc8efV7VVy_BpHtGXmBVpmG-TDx2sAijWgNtbPA/file# [following]\n",
            "--2022-11-22 11:22:04--  https://ucb62b0e098a8098974da3a55a58.dl.dropboxusercontent.com/cd/0/inline/BxNpp_kcz7uB3MCng_wAuf9uD-cKltwqcu5bRt4v6JzxZMcV-IjxQDB9Pe7mod75xdlh7tb-qEiETUX2TXPyGDN9NIhyiQpGGgZ1bLTa83VUxTBUwdK0exnAp-Olikyi65WU_Z9zc8efV7VVy_BpHtGXmBVpmG-TDx2sAijWgNtbPA/file\n",
            "Resolving ucb62b0e098a8098974da3a55a58.dl.dropboxusercontent.com (ucb62b0e098a8098974da3a55a58.dl.dropboxusercontent.com)... 162.125.69.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to ucb62b0e098a8098974da3a55a58.dl.dropboxusercontent.com (ucb62b0e098a8098974da3a55a58.dl.dropboxusercontent.com)|162.125.69.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BxO2rYx_IKXBccYl52rlubAuzb7jMxJwgbN1g9JJ5XigFNHaSDHUgg1ylWOz53f0WuqTGMzuqvXPEx8x2MoisL8oUFuHSrocFDt6VJGL65wXVj2YocNB_-k2oNYphY0sDW0siC0eAgdLas6VS2joGKmxjkd9Y6AA03XjuKXoCzy49MvKadn9dlCUBv9gRhiZrMpiyWorDwXgZVhROaW4UyIM_KJEnE0UL8bwXpezRcfs5J9uN_5_85pRGRcwRiskc-Bn96YZtiasSaMrLLICnddrE1WT9K-QIYHn2ZX7c3Jix-sUgASfIkMmPNkAMWpg-zW0kOJzw5ATLzY-aYin9N758UTZGRo1NW5QTPmufGdWPK8ofpnBbjnhlhCEl8TqxjGTUYehfpBgZCiNoagacMCCgzDGnqDJ3SD0ljmAi-MgLA/file [following]\n",
            "--2022-11-22 11:22:05--  https://ucb62b0e098a8098974da3a55a58.dl.dropboxusercontent.com/cd/0/inline2/BxO2rYx_IKXBccYl52rlubAuzb7jMxJwgbN1g9JJ5XigFNHaSDHUgg1ylWOz53f0WuqTGMzuqvXPEx8x2MoisL8oUFuHSrocFDt6VJGL65wXVj2YocNB_-k2oNYphY0sDW0siC0eAgdLas6VS2joGKmxjkd9Y6AA03XjuKXoCzy49MvKadn9dlCUBv9gRhiZrMpiyWorDwXgZVhROaW4UyIM_KJEnE0UL8bwXpezRcfs5J9uN_5_85pRGRcwRiskc-Bn96YZtiasSaMrLLICnddrE1WT9K-QIYHn2ZX7c3Jix-sUgASfIkMmPNkAMWpg-zW0kOJzw5ATLzY-aYin9N758UTZGRo1NW5QTPmufGdWPK8ofpnBbjnhlhCEl8TqxjGTUYehfpBgZCiNoagacMCCgzDGnqDJ3SD0ljmAi-MgLA/file\n",
            "Reusing existing connection to ucb62b0e098a8098974da3a55a58.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9251149 (8.8M) [application/octet-stream]\n",
            "Saving to: ‘Task01_BrainTumour_2D.tar.gz’\n",
            "\n",
            "Task01_BrainTumour_ 100%[===================>]   8.82M  4.54MB/s    in 1.9s    \n",
            "\n",
            "2022-11-22 11:22:08 (4.54 MB/s) - ‘Task01_BrainTumour_2D.tar.gz’ saved [9251149/9251149]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement a dataset class\n",
        "\n",
        "It can read the imaging dataset and get items, pairs of images and label maps, as training batches."
      ],
      "metadata": {
        "id": "5xWGT3KaML-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalise_intensity(image, thres_roi=1.0):\n",
        "    \"\"\" Normalise the image intensity by the mean and standard deviation \"\"\"\n",
        "    # ROI defines the image foreground\n",
        "    val_l = np.percentile(image, thres_roi)\n",
        "    roi = (image >= val_l)\n",
        "    mu, sigma = np.mean(image[roi]), np.std(image[roi])\n",
        "    eps = 1e-6\n",
        "    image2 = (image - mu) / (sigma + eps)\n",
        "    return image2\n",
        "\n",
        "\n",
        "class BrainImageSet(Dataset):\n",
        "    \"\"\" Brain image set \"\"\"\n",
        "    def __init__(self, image_path, label_path='', deploy=False):\n",
        "        self.image_path = image_path\n",
        "        self.deploy = deploy\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        image_names = sorted(os.listdir(image_path))\n",
        "        for image_name in image_names:\n",
        "            # Read the image\n",
        "            image = imageio.imread(os.path.join(image_path, image_name))\n",
        "            self.images += [image]\n",
        "\n",
        "            # Read the label map\n",
        "            if not self.deploy:\n",
        "                label_name = os.path.join(label_path, image_name)\n",
        "                label = imageio.imread(label_name)\n",
        "                self.labels += [label]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get an image and perform intensity normalisation\n",
        "        # Dimension: XY\n",
        "        image = normalise_intensity(self.images[idx])\n",
        "\n",
        "        # Get its label map\n",
        "        # Dimension: XY\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "    def get_random_batch(self, batch_size):\n",
        "        # Get a batch of paired images and label maps\n",
        "        # Dimension of images: NCXY\n",
        "        # Dimension of labels: NXY\n",
        "        images, labels = [], []\n",
        "\n",
        "        ### Insert your code ###\n",
        "        batch_idx = random.sample(range(0, self.__len__()), batch_size)\n",
        "        for i in range(batch_size):\n",
        "            image, label = self.__getitem__(batch_idx[i])\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "\n",
        "        ### End of your code ###\n",
        "        images, labels = np.array(images), np.array(labels)\n",
        "        images = np.expand_dims(images, 1)\n",
        "        return images, labels"
      ],
      "metadata": {
        "id": "6p6wFZ3na5z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.sample(range(0, 20), 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzq4AsyPHw_g",
        "outputId": "1333dd6b-023a-4893-cd61-9a6f390b086c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 7, 15, 8, 16, 5, 14, 4, 17, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a U-net architecture\n",
        "\n",
        "You will implement a U-net architecture. If you are not familiar with U-net, please read this paper:\n",
        "\n",
        "[1] Olaf Ronneberger et al. [U-Net: Convolutional networks for biomedical image segmentation](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28). MICCAI, 2015.\n"
      ],
      "metadata": {
        "id": "pa4ZpawDNmwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" U-net \"\"\"\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_channel=1, output_channel=1, num_filter=16):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # BatchNorm: by default during training this layer keeps running estimates\n",
        "        # of its computed mean and variance, which are then used for normalization\n",
        "        # during evaluation.\n",
        "\n",
        "        # Encoder path\n",
        "        n = num_filter  # 16\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n *= 2  # 32\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n *= 2  # 64\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n *= 2  # 128\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder path\n",
        "        n = int(n / 2)  # 64\n",
        "        self.up3 = nn.ConvTranspose2d(n * 2, n, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.conv_up3 = nn.Sequential(\n",
        "            nn.Conv2d(n * 2, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n = int(n / 2)  # 32\n",
        "        self.up2 = nn.ConvTranspose2d(n * 2, n, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.conv_up2 = nn.Sequential(\n",
        "            nn.Conv2d(n * 2, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n = int(n / 2)  # 16\n",
        "        self.up1 = nn.ConvTranspose2d(n * 2, n, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.conv_up1 = nn.Sequential(\n",
        "            nn.Conv2d(n * 2, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Output\n",
        "        self.out = nn.Conv2d(n, output_channel, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use the convolutional operators defined above to build the U-net\n",
        "        # The encoder part is already done for you.\n",
        "        # You need to complete the decoder part.\n",
        "        # Encoder\n",
        "        x = self.conv1(x)\n",
        "        conv1_skip = x\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        conv2_skip = x\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        conv3_skip = x\n",
        "\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        # Decoder\n",
        "        ### Insert your code ###\n",
        "        x = torch.cat((self.up3(x), conv3_skip), dim=1)\n",
        "        x = self.conv_up3(x)\n",
        "        x = torch.cat((self.up2(x), conv2_skip), dim=1)\n",
        "        x = self.conv_up2(x)\n",
        "        x = torch.cat((self.up1(x), conv1_skip), dim=1)\n",
        "        x = self.conv_up1(x)\n",
        "\n",
        "        ### End of your code ###\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "IMPmBZVGb1aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the segmentation model"
      ],
      "metadata": {
        "id": "NcNWZS08d47P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device: {0}'.format(device))\n",
        "\n",
        "# Build the model\n",
        "num_class = 4\n",
        "model = UNet(input_channel=1, output_channel=num_class, num_filter=16)\n",
        "model = model.to(device)\n",
        "params = list(model.parameters())\n",
        "\n",
        "model_dir = 'saved_models'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(params, lr=1e-3)\n",
        "\n",
        "# Segmentation loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Datasets\n",
        "train_set = BrainImageSet('Task01_BrainTumour_2D/training_images', 'Task01_BrainTumour_2D/training_labels')\n",
        "test_set = BrainImageSet('Task01_BrainTumour_2D/test_images', 'Task01_BrainTumour_2D/test_labels')\n",
        "\n",
        "# Train the model\n",
        "# Note: when you debug the model, you may reduce the number of iterations or batch size to save time.\n",
        "num_iter = 10000\n",
        "train_batch_size = 16\n",
        "eval_batch_size = 16\n",
        "start = time.time()\n",
        "for it in range(1, 1 + num_iter):\n",
        "    # Set the modules in training mode, which will have effects on certain modules, e.g. dropout or batchnorm.\n",
        "    start_iter = time.time()\n",
        "    model.train()\n",
        "\n",
        "    # Get a batch of images and labels\n",
        "    images, labels = train_set.get_random_batch(train_batch_size)\n",
        "    images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
        "    images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
        "    logits = model(images)\n",
        "\n",
        "    # Note that optimizer.zero_grad() is equivalent to net.zero_grad() if it optimises all the net parameters.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform optimisation\n",
        "    ### Insert your code ###\n",
        "    loss = criterion(logits, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    ### End of your code ###\n",
        "\n",
        "    print('--- Iteration {0}: Training loss = {1:.4f}, {2:.4f} s ---'.format(it, loss.item(), time.time() - start_iter))\n",
        "\n",
        "    # Evaluate\n",
        "    if it % 10 == 0:\n",
        "        model.eval()\n",
        "        # Disabling gradient calculation during reference to reduce memory consumption\n",
        "        with torch.no_grad():\n",
        "            images, labels = test_set.get_random_batch(eval_batch_size)\n",
        "            images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
        "            images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "            print('--- Iteration {0}: Test loss = {1:.4f} ---\\n'.format(it, loss.item()))\n",
        "\n",
        "    # Save the model\n",
        "    if it % 5000 == 0:\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir, 'model_{0}.pt'.format(it)))\n",
        "print('Training took {:.3f}s in total.'.format(time.time() - start))"
      ],
      "metadata": {
        "id": "xaGGkKQndIaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0386b3b0-abcf-4f7d-ac7b-4e4229e2a4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "--- Iteration 5836: Training loss = 0.0181, 0.0440 s ---\n",
            "--- Iteration 5837: Training loss = 0.0132, 0.0429 s ---\n",
            "--- Iteration 5838: Training loss = 0.0186, 0.0435 s ---\n",
            "--- Iteration 5839: Training loss = 0.0102, 0.0433 s ---\n",
            "--- Iteration 5840: Training loss = 0.0215, 0.0438 s ---\n",
            "--- Iteration 5840: Test loss = 0.0356 ---\n",
            "\n",
            "--- Iteration 5841: Training loss = 0.0162, 0.0452 s ---\n",
            "--- Iteration 5842: Training loss = 0.0152, 0.0449 s ---\n",
            "--- Iteration 5843: Training loss = 0.0113, 0.0429 s ---\n",
            "--- Iteration 5844: Training loss = 0.0084, 0.0426 s ---\n",
            "--- Iteration 5845: Training loss = 0.0073, 0.0420 s ---\n",
            "--- Iteration 5846: Training loss = 0.0112, 0.0426 s ---\n",
            "--- Iteration 5847: Training loss = 0.0121, 0.0436 s ---\n",
            "--- Iteration 5848: Training loss = 0.0121, 0.0465 s ---\n",
            "--- Iteration 5849: Training loss = 0.0119, 0.0426 s ---\n",
            "--- Iteration 5850: Training loss = 0.0090, 0.0424 s ---\n",
            "--- Iteration 5850: Test loss = 0.0376 ---\n",
            "\n",
            "--- Iteration 5851: Training loss = 0.0171, 0.0410 s ---\n",
            "--- Iteration 5852: Training loss = 0.0151, 0.0422 s ---\n",
            "--- Iteration 5853: Training loss = 0.0123, 0.0428 s ---\n",
            "--- Iteration 5854: Training loss = 0.0107, 0.0429 s ---\n",
            "--- Iteration 5855: Training loss = 0.0112, 0.0430 s ---\n",
            "--- Iteration 5856: Training loss = 0.0114, 0.0443 s ---\n",
            "--- Iteration 5857: Training loss = 0.0110, 0.0444 s ---\n",
            "--- Iteration 5858: Training loss = 0.0139, 0.0440 s ---\n",
            "--- Iteration 5859: Training loss = 0.0119, 0.0419 s ---\n",
            "--- Iteration 5860: Training loss = 0.0143, 0.0425 s ---\n",
            "--- Iteration 5860: Test loss = 0.0533 ---\n",
            "\n",
            "--- Iteration 5861: Training loss = 0.0222, 0.0412 s ---\n",
            "--- Iteration 5862: Training loss = 0.0148, 0.0420 s ---\n",
            "--- Iteration 5863: Training loss = 0.0120, 0.0432 s ---\n",
            "--- Iteration 5864: Training loss = 0.0174, 0.0433 s ---\n",
            "--- Iteration 5865: Training loss = 0.0132, 0.0435 s ---\n",
            "--- Iteration 5866: Training loss = 0.0144, 0.0453 s ---\n",
            "--- Iteration 5867: Training loss = 0.0077, 0.0427 s ---\n",
            "--- Iteration 5868: Training loss = 0.0161, 0.0429 s ---\n",
            "--- Iteration 5869: Training loss = 0.0228, 0.0427 s ---\n",
            "--- Iteration 5870: Training loss = 0.0195, 0.0425 s ---\n",
            "--- Iteration 5870: Test loss = 0.0245 ---\n",
            "\n",
            "--- Iteration 5871: Training loss = 0.0119, 0.0426 s ---\n",
            "--- Iteration 5872: Training loss = 0.0113, 0.0472 s ---\n",
            "--- Iteration 5873: Training loss = 0.0087, 0.0423 s ---\n",
            "--- Iteration 5874: Training loss = 0.0154, 0.0426 s ---\n",
            "--- Iteration 5875: Training loss = 0.0104, 0.0425 s ---\n",
            "--- Iteration 5876: Training loss = 0.0141, 0.0429 s ---\n",
            "--- Iteration 5877: Training loss = 0.0143, 0.0435 s ---\n",
            "--- Iteration 5878: Training loss = 0.0154, 0.0455 s ---\n",
            "--- Iteration 5879: Training loss = 0.0112, 0.0438 s ---\n",
            "--- Iteration 5880: Training loss = 0.0102, 0.0443 s ---\n",
            "--- Iteration 5880: Test loss = 0.0161 ---\n",
            "\n",
            "--- Iteration 5881: Training loss = 0.0108, 0.0411 s ---\n",
            "--- Iteration 5882: Training loss = 0.0139, 0.0472 s ---\n",
            "--- Iteration 5883: Training loss = 0.0101, 0.0444 s ---\n",
            "--- Iteration 5884: Training loss = 0.0079, 0.0450 s ---\n",
            "--- Iteration 5885: Training loss = 0.0190, 0.0441 s ---\n",
            "--- Iteration 5886: Training loss = 0.0090, 0.0418 s ---\n",
            "--- Iteration 5887: Training loss = 0.0083, 0.0422 s ---\n",
            "--- Iteration 5888: Training loss = 0.0131, 0.0425 s ---\n",
            "--- Iteration 5889: Training loss = 0.0233, 0.0433 s ---\n",
            "--- Iteration 5890: Training loss = 0.0139, 0.0428 s ---\n",
            "--- Iteration 5890: Test loss = 0.0526 ---\n",
            "\n",
            "--- Iteration 5891: Training loss = 0.0105, 0.0436 s ---\n",
            "--- Iteration 5892: Training loss = 0.0122, 0.0469 s ---\n",
            "--- Iteration 5893: Training loss = 0.0186, 0.0447 s ---\n",
            "--- Iteration 5894: Training loss = 0.0196, 0.0429 s ---\n",
            "--- Iteration 5895: Training loss = 0.0129, 0.0427 s ---\n",
            "--- Iteration 5896: Training loss = 0.0101, 0.0426 s ---\n",
            "--- Iteration 5897: Training loss = 0.0121, 0.0435 s ---\n",
            "--- Iteration 5898: Training loss = 0.0154, 0.0430 s ---\n",
            "--- Iteration 5899: Training loss = 0.0191, 0.0432 s ---\n",
            "--- Iteration 5900: Training loss = 0.0119, 0.0426 s ---\n",
            "--- Iteration 5900: Test loss = 0.0376 ---\n",
            "\n",
            "--- Iteration 5901: Training loss = 0.0124, 0.0411 s ---\n",
            "--- Iteration 5902: Training loss = 0.0134, 0.0420 s ---\n",
            "--- Iteration 5903: Training loss = 0.0169, 0.0426 s ---\n",
            "--- Iteration 5904: Training loss = 0.0167, 0.0432 s ---\n",
            "--- Iteration 5905: Training loss = 0.0084, 0.0424 s ---\n",
            "--- Iteration 5906: Training loss = 0.0123, 0.0434 s ---\n",
            "--- Iteration 5907: Training loss = 0.0104, 0.0452 s ---\n",
            "--- Iteration 5908: Training loss = 0.0103, 0.0444 s ---\n",
            "--- Iteration 5909: Training loss = 0.0177, 0.0428 s ---\n",
            "--- Iteration 5910: Training loss = 0.0125, 0.0425 s ---\n",
            "--- Iteration 5910: Test loss = 0.0278 ---\n",
            "\n",
            "--- Iteration 5911: Training loss = 0.0102, 0.0414 s ---\n",
            "--- Iteration 5912: Training loss = 0.0129, 0.0420 s ---\n",
            "--- Iteration 5913: Training loss = 0.0139, 0.0428 s ---\n",
            "--- Iteration 5914: Training loss = 0.0117, 0.0424 s ---\n",
            "--- Iteration 5915: Training loss = 0.0126, 0.0466 s ---\n",
            "--- Iteration 5916: Training loss = 0.0113, 0.0447 s ---\n",
            "--- Iteration 5917: Training loss = 0.0128, 0.0430 s ---\n",
            "--- Iteration 5918: Training loss = 0.0130, 0.0427 s ---\n",
            "--- Iteration 5919: Training loss = 0.0130, 0.0458 s ---\n",
            "--- Iteration 5920: Training loss = 0.0125, 0.0440 s ---\n",
            "--- Iteration 5920: Test loss = 0.0496 ---\n",
            "\n",
            "--- Iteration 5921: Training loss = 0.0150, 0.0433 s ---\n",
            "--- Iteration 5922: Training loss = 0.0138, 0.0485 s ---\n",
            "--- Iteration 5923: Training loss = 0.0139, 0.0424 s ---\n",
            "--- Iteration 5924: Training loss = 0.0127, 0.0425 s ---\n",
            "--- Iteration 5925: Training loss = 0.0081, 0.0431 s ---\n",
            "--- Iteration 5926: Training loss = 0.0147, 0.0435 s ---\n",
            "--- Iteration 5927: Training loss = 0.0276, 0.0460 s ---\n",
            "--- Iteration 5928: Training loss = 0.0102, 0.0437 s ---\n",
            "--- Iteration 5929: Training loss = 0.0131, 0.0425 s ---\n",
            "--- Iteration 5930: Training loss = 0.0099, 0.0422 s ---\n",
            "--- Iteration 5930: Test loss = 0.0436 ---\n",
            "\n",
            "--- Iteration 5931: Training loss = 0.0164, 0.0405 s ---\n",
            "--- Iteration 5932: Training loss = 0.0217, 0.0421 s ---\n",
            "--- Iteration 5933: Training loss = 0.0146, 0.0428 s ---\n",
            "--- Iteration 5934: Training loss = 0.0085, 0.0427 s ---\n",
            "--- Iteration 5935: Training loss = 0.0096, 0.0435 s ---\n",
            "--- Iteration 5936: Training loss = 0.0119, 0.0442 s ---\n",
            "--- Iteration 5937: Training loss = 0.0121, 0.0445 s ---\n",
            "--- Iteration 5938: Training loss = 0.0064, 0.0500 s ---\n",
            "--- Iteration 5939: Training loss = 0.0182, 0.0440 s ---\n",
            "--- Iteration 5940: Training loss = 0.0103, 0.0451 s ---\n",
            "--- Iteration 5940: Test loss = 0.0469 ---\n",
            "\n",
            "--- Iteration 5941: Training loss = 0.0155, 0.0428 s ---\n",
            "--- Iteration 5942: Training loss = 0.0124, 0.0474 s ---\n",
            "--- Iteration 5943: Training loss = 0.0120, 0.0427 s ---\n",
            "--- Iteration 5944: Training loss = 0.0152, 0.0461 s ---\n",
            "--- Iteration 5945: Training loss = 0.0118, 0.0447 s ---\n",
            "--- Iteration 5946: Training loss = 0.0173, 0.0435 s ---\n",
            "--- Iteration 5947: Training loss = 0.0135, 0.0435 s ---\n",
            "--- Iteration 5948: Training loss = 0.0145, 0.0421 s ---\n",
            "--- Iteration 5949: Training loss = 0.0100, 0.0424 s ---\n",
            "--- Iteration 5950: Training loss = 0.0127, 0.0424 s ---\n",
            "--- Iteration 5950: Test loss = 0.0823 ---\n",
            "\n",
            "--- Iteration 5951: Training loss = 0.0122, 0.0420 s ---\n",
            "--- Iteration 5952: Training loss = 0.0119, 0.0437 s ---\n",
            "--- Iteration 5953: Training loss = 0.0110, 0.0450 s ---\n",
            "--- Iteration 5954: Training loss = 0.0088, 0.0441 s ---\n",
            "--- Iteration 5955: Training loss = 0.0121, 0.0441 s ---\n",
            "--- Iteration 5956: Training loss = 0.0165, 0.0426 s ---\n",
            "--- Iteration 5957: Training loss = 0.0117, 0.0418 s ---\n",
            "--- Iteration 5958: Training loss = 0.0113, 0.0436 s ---\n",
            "--- Iteration 5959: Training loss = 0.0139, 0.0442 s ---\n",
            "--- Iteration 5960: Training loss = 0.0145, 0.0533 s ---\n",
            "--- Iteration 5960: Test loss = 0.0506 ---\n",
            "\n",
            "--- Iteration 5961: Training loss = 0.0087, 0.0448 s ---\n",
            "--- Iteration 5962: Training loss = 0.0139, 0.0468 s ---\n",
            "--- Iteration 5963: Training loss = 0.0081, 0.0466 s ---\n",
            "--- Iteration 5964: Training loss = 0.0118, 0.0635 s ---\n",
            "--- Iteration 5965: Training loss = 0.0156, 0.0558 s ---\n",
            "--- Iteration 5966: Training loss = 0.0107, 0.0480 s ---\n",
            "--- Iteration 5967: Training loss = 0.0066, 0.0460 s ---\n",
            "--- Iteration 5968: Training loss = 0.0156, 0.0468 s ---\n",
            "--- Iteration 5969: Training loss = 0.0077, 0.0548 s ---\n",
            "--- Iteration 5970: Training loss = 0.0158, 0.0462 s ---\n",
            "--- Iteration 5970: Test loss = 0.0464 ---\n",
            "\n",
            "--- Iteration 5971: Training loss = 0.0148, 0.0434 s ---\n",
            "--- Iteration 5972: Training loss = 0.0115, 0.0461 s ---\n",
            "--- Iteration 5973: Training loss = 0.0093, 0.0475 s ---\n",
            "--- Iteration 5974: Training loss = 0.0164, 0.0507 s ---\n",
            "--- Iteration 5975: Training loss = 0.0124, 0.0472 s ---\n",
            "--- Iteration 5976: Training loss = 0.0085, 0.0467 s ---\n",
            "--- Iteration 5977: Training loss = 0.0107, 0.0522 s ---\n",
            "--- Iteration 5978: Training loss = 0.0181, 0.0545 s ---\n",
            "--- Iteration 5979: Training loss = 0.0142, 0.0496 s ---\n",
            "--- Iteration 5980: Training loss = 0.0182, 0.0530 s ---\n",
            "--- Iteration 5980: Test loss = 0.0562 ---\n",
            "\n",
            "--- Iteration 5981: Training loss = 0.0138, 0.0479 s ---\n",
            "--- Iteration 5982: Training loss = 0.0132, 0.0474 s ---\n",
            "--- Iteration 5983: Training loss = 0.0099, 0.0494 s ---\n",
            "--- Iteration 5984: Training loss = 0.0096, 0.0480 s ---\n",
            "--- Iteration 5985: Training loss = 0.0148, 0.0467 s ---\n",
            "--- Iteration 5986: Training loss = 0.0087, 0.0468 s ---\n",
            "--- Iteration 5987: Training loss = 0.0129, 0.0531 s ---\n",
            "--- Iteration 5988: Training loss = 0.0093, 0.0473 s ---\n",
            "--- Iteration 5989: Training loss = 0.0093, 0.0462 s ---\n",
            "--- Iteration 5990: Training loss = 0.0182, 0.0467 s ---\n",
            "--- Iteration 5990: Test loss = 0.0501 ---\n",
            "\n",
            "--- Iteration 5991: Training loss = 0.0156, 0.0460 s ---\n",
            "--- Iteration 5992: Training loss = 0.0154, 0.0458 s ---\n",
            "--- Iteration 5993: Training loss = 0.0129, 0.0456 s ---\n",
            "--- Iteration 5994: Training loss = 0.0111, 0.0488 s ---\n",
            "--- Iteration 5995: Training loss = 0.0105, 0.0465 s ---\n",
            "--- Iteration 5996: Training loss = 0.0115, 0.0511 s ---\n",
            "--- Iteration 5997: Training loss = 0.0168, 0.0495 s ---\n",
            "--- Iteration 5998: Training loss = 0.0136, 0.0467 s ---\n",
            "--- Iteration 5999: Training loss = 0.0128, 0.0525 s ---\n",
            "--- Iteration 6000: Training loss = 0.0105, 0.0575 s ---\n",
            "--- Iteration 6000: Test loss = 0.0290 ---\n",
            "\n",
            "--- Iteration 6001: Training loss = 0.0132, 0.0457 s ---\n",
            "--- Iteration 6002: Training loss = 0.0124, 0.0496 s ---\n",
            "--- Iteration 6003: Training loss = 0.0134, 0.0435 s ---\n",
            "--- Iteration 6004: Training loss = 0.0106, 0.0427 s ---\n",
            "--- Iteration 6005: Training loss = 0.0108, 0.0479 s ---\n",
            "--- Iteration 6006: Training loss = 0.0202, 0.0445 s ---\n",
            "--- Iteration 6007: Training loss = 0.0153, 0.0440 s ---\n",
            "--- Iteration 6008: Training loss = 0.0127, 0.0428 s ---\n",
            "--- Iteration 6009: Training loss = 0.0178, 0.0414 s ---\n",
            "--- Iteration 6010: Training loss = 0.0097, 0.0428 s ---\n",
            "--- Iteration 6010: Test loss = 0.0312 ---\n",
            "\n",
            "--- Iteration 6011: Training loss = 0.0145, 0.0419 s ---\n",
            "--- Iteration 6012: Training loss = 0.0129, 0.0427 s ---\n",
            "--- Iteration 6013: Training loss = 0.0099, 0.0432 s ---\n",
            "--- Iteration 6014: Training loss = 0.0083, 0.0456 s ---\n",
            "--- Iteration 6015: Training loss = 0.0128, 0.0446 s ---\n",
            "--- Iteration 6016: Training loss = 0.0120, 0.0432 s ---\n",
            "--- Iteration 6017: Training loss = 0.0147, 0.0434 s ---\n",
            "--- Iteration 6018: Training loss = 0.0070, 0.0432 s ---\n",
            "--- Iteration 6019: Training loss = 0.0100, 0.0429 s ---\n",
            "--- Iteration 6020: Training loss = 0.0127, 0.0430 s ---\n",
            "--- Iteration 6020: Test loss = 0.0382 ---\n",
            "\n",
            "--- Iteration 6021: Training loss = 0.0158, 0.0436 s ---\n",
            "--- Iteration 6022: Training loss = 0.0168, 0.0439 s ---\n",
            "--- Iteration 6023: Training loss = 0.0159, 0.0430 s ---\n",
            "--- Iteration 6024: Training loss = 0.0178, 0.0438 s ---\n",
            "--- Iteration 6025: Training loss = 0.0105, 0.0423 s ---\n",
            "--- Iteration 6026: Training loss = 0.0120, 0.0422 s ---\n",
            "--- Iteration 6027: Training loss = 0.0097, 0.0433 s ---\n",
            "--- Iteration 6028: Training loss = 0.0134, 0.0471 s ---\n",
            "--- Iteration 6029: Training loss = 0.0134, 0.0433 s ---\n",
            "--- Iteration 6030: Training loss = 0.0136, 0.0428 s ---\n",
            "--- Iteration 6030: Test loss = 0.0571 ---\n",
            "\n",
            "--- Iteration 6031: Training loss = 0.0166, 0.0414 s ---\n",
            "--- Iteration 6032: Training loss = 0.0101, 0.0430 s ---\n",
            "--- Iteration 6033: Training loss = 0.0169, 0.0430 s ---\n",
            "--- Iteration 6034: Training loss = 0.0161, 0.0427 s ---\n",
            "--- Iteration 6035: Training loss = 0.0116, 0.0456 s ---\n",
            "--- Iteration 6036: Training loss = 0.0091, 0.0445 s ---\n",
            "--- Iteration 6037: Training loss = 0.0096, 0.0436 s ---\n",
            "--- Iteration 6038: Training loss = 0.0075, 0.0457 s ---\n",
            "--- Iteration 6039: Training loss = 0.0152, 0.0434 s ---\n",
            "--- Iteration 6040: Training loss = 0.0087, 0.0455 s ---\n",
            "--- Iteration 6040: Test loss = 0.0606 ---\n",
            "\n",
            "--- Iteration 6041: Training loss = 0.0147, 0.0426 s ---\n",
            "--- Iteration 6042: Training loss = 0.0093, 0.0433 s ---\n",
            "--- Iteration 6043: Training loss = 0.0111, 0.0425 s ---\n",
            "--- Iteration 6044: Training loss = 0.0076, 0.0445 s ---\n",
            "--- Iteration 6045: Training loss = 0.0069, 0.0417 s ---\n",
            "--- Iteration 6046: Training loss = 0.0142, 0.0429 s ---\n",
            "--- Iteration 6047: Training loss = 0.0115, 0.0436 s ---\n",
            "--- Iteration 6048: Training loss = 0.0178, 0.0453 s ---\n",
            "--- Iteration 6049: Training loss = 0.0102, 0.0442 s ---\n",
            "--- Iteration 6050: Training loss = 0.0141, 0.0422 s ---\n",
            "--- Iteration 6050: Test loss = 0.0297 ---\n",
            "\n",
            "--- Iteration 6051: Training loss = 0.0122, 0.0412 s ---\n",
            "--- Iteration 6052: Training loss = 0.0148, 0.0421 s ---\n",
            "--- Iteration 6053: Training loss = 0.0087, 0.0420 s ---\n",
            "--- Iteration 6054: Training loss = 0.0100, 0.0430 s ---\n",
            "--- Iteration 6055: Training loss = 0.0143, 0.0429 s ---\n",
            "--- Iteration 6056: Training loss = 0.0145, 0.0435 s ---\n",
            "--- Iteration 6057: Training loss = 0.0104, 0.0443 s ---\n",
            "--- Iteration 6058: Training loss = 0.0112, 0.0438 s ---\n",
            "--- Iteration 6059: Training loss = 0.0153, 0.0434 s ---\n",
            "--- Iteration 6060: Training loss = 0.0140, 0.0437 s ---\n",
            "--- Iteration 6060: Test loss = 0.0308 ---\n",
            "\n",
            "--- Iteration 6061: Training loss = 0.0117, 0.0433 s ---\n",
            "--- Iteration 6062: Training loss = 0.0174, 0.0445 s ---\n",
            "--- Iteration 6063: Training loss = 0.0132, 0.0446 s ---\n",
            "--- Iteration 6064: Training loss = 0.0175, 0.0465 s ---\n",
            "--- Iteration 6065: Training loss = 0.0150, 0.0422 s ---\n",
            "--- Iteration 6066: Training loss = 0.0103, 0.0458 s ---\n",
            "--- Iteration 6067: Training loss = 0.0197, 0.0433 s ---\n",
            "--- Iteration 6068: Training loss = 0.0165, 0.0443 s ---\n",
            "--- Iteration 6069: Training loss = 0.0124, 0.0448 s ---\n",
            "--- Iteration 6070: Training loss = 0.0107, 0.0433 s ---\n",
            "--- Iteration 6070: Test loss = 0.0282 ---\n",
            "\n",
            "--- Iteration 6071: Training loss = 0.0128, 0.0408 s ---\n",
            "--- Iteration 6072: Training loss = 0.0085, 0.0422 s ---\n",
            "--- Iteration 6073: Training loss = 0.0086, 0.0430 s ---\n",
            "--- Iteration 6074: Training loss = 0.0103, 0.0434 s ---\n",
            "--- Iteration 6075: Training loss = 0.0128, 0.0436 s ---\n",
            "--- Iteration 6076: Training loss = 0.0146, 0.0439 s ---\n",
            "--- Iteration 6077: Training loss = 0.0139, 0.0449 s ---\n",
            "--- Iteration 6078: Training loss = 0.0088, 0.0434 s ---\n",
            "--- Iteration 6079: Training loss = 0.0142, 0.0419 s ---\n",
            "--- Iteration 6080: Training loss = 0.0109, 0.0448 s ---\n",
            "--- Iteration 6080: Test loss = 0.0299 ---\n",
            "\n",
            "--- Iteration 6081: Training loss = 0.0135, 0.0454 s ---\n",
            "--- Iteration 6082: Training loss = 0.0079, 0.0518 s ---\n",
            "--- Iteration 6083: Training loss = 0.0121, 0.0593 s ---\n",
            "--- Iteration 6084: Training loss = 0.0145, 0.0508 s ---\n",
            "--- Iteration 6085: Training loss = 0.0130, 0.0433 s ---\n",
            "--- Iteration 6086: Training loss = 0.0118, 0.0429 s ---\n",
            "--- Iteration 6087: Training loss = 0.0118, 0.0433 s ---\n",
            "--- Iteration 6088: Training loss = 0.0121, 0.0436 s ---\n",
            "--- Iteration 6089: Training loss = 0.0191, 0.0465 s ---\n",
            "--- Iteration 6090: Training loss = 0.0116, 0.0444 s ---\n",
            "--- Iteration 6090: Test loss = 0.0327 ---\n",
            "\n",
            "--- Iteration 6091: Training loss = 0.0130, 0.0419 s ---\n",
            "--- Iteration 6092: Training loss = 0.0128, 0.0450 s ---\n",
            "--- Iteration 6093: Training loss = 0.0098, 0.0472 s ---\n",
            "--- Iteration 6094: Training loss = 0.0064, 0.0455 s ---\n",
            "--- Iteration 6095: Training loss = 0.0080, 0.0440 s ---\n",
            "--- Iteration 6096: Training loss = 0.0126, 0.0426 s ---\n",
            "--- Iteration 6097: Training loss = 0.0200, 0.0426 s ---\n",
            "--- Iteration 6098: Training loss = 0.0131, 0.0419 s ---\n",
            "--- Iteration 6099: Training loss = 0.0109, 0.0451 s ---\n",
            "--- Iteration 6100: Training loss = 0.0125, 0.0441 s ---\n",
            "--- Iteration 6100: Test loss = 0.0388 ---\n",
            "\n",
            "--- Iteration 6101: Training loss = 0.0123, 0.0429 s ---\n",
            "--- Iteration 6102: Training loss = 0.0107, 0.0434 s ---\n",
            "--- Iteration 6103: Training loss = 0.0085, 0.0464 s ---\n",
            "--- Iteration 6104: Training loss = 0.0092, 0.0429 s ---\n",
            "--- Iteration 6105: Training loss = 0.0147, 0.0431 s ---\n",
            "--- Iteration 6106: Training loss = 0.0109, 0.0444 s ---\n",
            "--- Iteration 6107: Training loss = 0.0096, 0.0444 s ---\n",
            "--- Iteration 6108: Training loss = 0.0166, 0.0433 s ---\n",
            "--- Iteration 6109: Training loss = 0.0214, 0.0430 s ---\n",
            "--- Iteration 6110: Training loss = 0.0119, 0.0458 s ---\n",
            "--- Iteration 6110: Test loss = 0.0281 ---\n",
            "\n",
            "--- Iteration 6111: Training loss = 0.0175, 0.0434 s ---\n",
            "--- Iteration 6112: Training loss = 0.0177, 0.0440 s ---\n",
            "--- Iteration 6113: Training loss = 0.0118, 0.0487 s ---\n",
            "--- Iteration 6114: Training loss = 0.0099, 0.0425 s ---\n",
            "--- Iteration 6115: Training loss = 0.0125, 0.0423 s ---\n",
            "--- Iteration 6116: Training loss = 0.0132, 0.0426 s ---\n",
            "--- Iteration 6117: Training loss = 0.0093, 0.0427 s ---\n",
            "--- Iteration 6118: Training loss = 0.0152, 0.0437 s ---\n",
            "--- Iteration 6119: Training loss = 0.0107, 0.0449 s ---\n",
            "--- Iteration 6120: Training loss = 0.0079, 0.0451 s ---\n",
            "--- Iteration 6120: Test loss = 0.0379 ---\n",
            "\n",
            "--- Iteration 6121: Training loss = 0.0185, 0.0414 s ---\n",
            "--- Iteration 6122: Training loss = 0.0134, 0.0426 s ---\n",
            "--- Iteration 6123: Training loss = 0.0109, 0.0454 s ---\n",
            "--- Iteration 6124: Training loss = 0.0121, 0.0445 s ---\n",
            "--- Iteration 6125: Training loss = 0.0129, 0.0453 s ---\n",
            "--- Iteration 6126: Training loss = 0.0168, 0.0432 s ---\n",
            "--- Iteration 6127: Training loss = 0.0139, 0.0428 s ---\n",
            "--- Iteration 6128: Training loss = 0.0085, 0.0422 s ---\n",
            "--- Iteration 6129: Training loss = 0.0111, 0.0423 s ---\n",
            "--- Iteration 6130: Training loss = 0.0161, 0.0431 s ---\n",
            "--- Iteration 6130: Test loss = 0.0561 ---\n",
            "\n",
            "--- Iteration 6131: Training loss = 0.0123, 0.0432 s ---\n",
            "--- Iteration 6132: Training loss = 0.0135, 0.0463 s ---\n",
            "--- Iteration 6133: Training loss = 0.0146, 0.0482 s ---\n",
            "--- Iteration 6134: Training loss = 0.0169, 0.0433 s ---\n",
            "--- Iteration 6135: Training loss = 0.0117, 0.0434 s ---\n",
            "--- Iteration 6136: Training loss = 0.0148, 0.0441 s ---\n",
            "--- Iteration 6137: Training loss = 0.0123, 0.0443 s ---\n",
            "--- Iteration 6138: Training loss = 0.0159, 0.0433 s ---\n",
            "--- Iteration 6139: Training loss = 0.0125, 0.0430 s ---\n",
            "--- Iteration 6140: Training loss = 0.0141, 0.0432 s ---\n",
            "--- Iteration 6140: Test loss = 0.0254 ---\n",
            "\n",
            "--- Iteration 6141: Training loss = 0.0192, 0.0425 s ---\n",
            "--- Iteration 6142: Training loss = 0.0135, 0.0447 s ---\n",
            "--- Iteration 6143: Training loss = 0.0113, 0.0473 s ---\n",
            "--- Iteration 6144: Training loss = 0.0140, 0.0430 s ---\n",
            "--- Iteration 6145: Training loss = 0.0117, 0.0424 s ---\n",
            "--- Iteration 6146: Training loss = 0.0107, 0.0427 s ---\n",
            "--- Iteration 6147: Training loss = 0.0134, 0.0437 s ---\n",
            "--- Iteration 6148: Training loss = 0.0127, 0.0451 s ---\n",
            "--- Iteration 6149: Training loss = 0.0126, 0.0463 s ---\n",
            "--- Iteration 6150: Training loss = 0.0124, 0.0439 s ---\n",
            "--- Iteration 6150: Test loss = 0.0210 ---\n",
            "\n",
            "--- Iteration 6151: Training loss = 0.0102, 0.0413 s ---\n",
            "--- Iteration 6152: Training loss = 0.0104, 0.0438 s ---\n",
            "--- Iteration 6153: Training loss = 0.0142, 0.0499 s ---\n",
            "--- Iteration 6154: Training loss = 0.0131, 0.0462 s ---\n",
            "--- Iteration 6155: Training loss = 0.0116, 0.0430 s ---\n",
            "--- Iteration 6156: Training loss = 0.0158, 0.0433 s ---\n",
            "--- Iteration 6157: Training loss = 0.0079, 0.0511 s ---\n",
            "--- Iteration 6158: Training loss = 0.0134, 0.0435 s ---\n",
            "--- Iteration 6159: Training loss = 0.0108, 0.0439 s ---\n",
            "--- Iteration 6160: Training loss = 0.0131, 0.0433 s ---\n",
            "--- Iteration 6160: Test loss = 0.0291 ---\n",
            "\n",
            "--- Iteration 6161: Training loss = 0.0109, 0.0419 s ---\n",
            "--- Iteration 6162: Training loss = 0.0143, 0.0418 s ---\n",
            "--- Iteration 6163: Training loss = 0.0087, 0.0431 s ---\n",
            "--- Iteration 6164: Training loss = 0.0074, 0.0439 s ---\n",
            "--- Iteration 6165: Training loss = 0.0174, 0.0451 s ---\n",
            "--- Iteration 6166: Training loss = 0.0108, 0.0445 s ---\n",
            "--- Iteration 6167: Training loss = 0.0117, 0.0436 s ---\n",
            "--- Iteration 6168: Training loss = 0.0148, 0.0434 s ---\n",
            "--- Iteration 6169: Training loss = 0.0102, 0.0424 s ---\n",
            "--- Iteration 6170: Training loss = 0.0083, 0.0453 s ---\n",
            "--- Iteration 6170: Test loss = 0.0471 ---\n",
            "\n",
            "--- Iteration 6171: Training loss = 0.0113, 0.0429 s ---\n",
            "--- Iteration 6172: Training loss = 0.0175, 0.0441 s ---\n",
            "--- Iteration 6173: Training loss = 0.0149, 0.0463 s ---\n",
            "--- Iteration 6174: Training loss = 0.0084, 0.0420 s ---\n",
            "--- Iteration 6175: Training loss = 0.0106, 0.0428 s ---\n",
            "--- Iteration 6176: Training loss = 0.0127, 0.0489 s ---\n",
            "--- Iteration 6177: Training loss = 0.0138, 0.0438 s ---\n",
            "--- Iteration 6178: Training loss = 0.0143, 0.0430 s ---\n",
            "--- Iteration 6179: Training loss = 0.0166, 0.0447 s ---\n",
            "--- Iteration 6180: Training loss = 0.0113, 0.0426 s ---\n",
            "--- Iteration 6180: Test loss = 0.0446 ---\n",
            "\n",
            "--- Iteration 6181: Training loss = 0.0114, 0.0430 s ---\n",
            "--- Iteration 6182: Training loss = 0.0161, 0.0440 s ---\n",
            "--- Iteration 6183: Training loss = 0.0115, 0.0467 s ---\n",
            "--- Iteration 6184: Training loss = 0.0136, 0.0436 s ---\n",
            "--- Iteration 6185: Training loss = 0.0141, 0.0424 s ---\n",
            "--- Iteration 6186: Training loss = 0.0124, 0.0426 s ---\n",
            "--- Iteration 6187: Training loss = 0.0075, 0.0432 s ---\n",
            "--- Iteration 6188: Training loss = 0.0095, 0.0431 s ---\n",
            "--- Iteration 6189: Training loss = 0.0131, 0.0456 s ---\n",
            "--- Iteration 6190: Training loss = 0.0135, 0.0439 s ---\n",
            "--- Iteration 6190: Test loss = 0.0364 ---\n",
            "\n",
            "--- Iteration 6191: Training loss = 0.0083, 0.0412 s ---\n",
            "--- Iteration 6192: Training loss = 0.0140, 0.0430 s ---\n",
            "--- Iteration 6193: Training loss = 0.0149, 0.0445 s ---\n",
            "--- Iteration 6194: Training loss = 0.0090, 0.0432 s ---\n",
            "--- Iteration 6195: Training loss = 0.0144, 0.0430 s ---\n",
            "--- Iteration 6196: Training loss = 0.0136, 0.0480 s ---\n",
            "--- Iteration 6197: Training loss = 0.0137, 0.0429 s ---\n",
            "--- Iteration 6198: Training loss = 0.0106, 0.0418 s ---\n",
            "--- Iteration 6199: Training loss = 0.0089, 0.0416 s ---\n",
            "--- Iteration 6200: Training loss = 0.0097, 0.0432 s ---\n",
            "--- Iteration 6200: Test loss = 0.0424 ---\n",
            "\n",
            "--- Iteration 6201: Training loss = 0.0120, 0.0428 s ---\n",
            "--- Iteration 6202: Training loss = 0.0163, 0.0445 s ---\n",
            "--- Iteration 6203: Training loss = 0.0119, 0.0469 s ---\n",
            "--- Iteration 6204: Training loss = 0.0191, 0.0429 s ---\n",
            "--- Iteration 6205: Training loss = 0.0093, 0.0435 s ---\n",
            "--- Iteration 6206: Training loss = 0.0104, 0.0439 s ---\n",
            "--- Iteration 6207: Training loss = 0.0115, 0.0439 s ---\n",
            "--- Iteration 6208: Training loss = 0.0135, 0.0443 s ---\n",
            "--- Iteration 6209: Training loss = 0.0140, 0.0436 s ---\n",
            "--- Iteration 6210: Training loss = 0.0110, 0.0431 s ---\n",
            "--- Iteration 6210: Test loss = 0.0274 ---\n",
            "\n",
            "--- Iteration 6211: Training loss = 0.0136, 0.0413 s ---\n",
            "--- Iteration 6212: Training loss = 0.0166, 0.0429 s ---\n",
            "--- Iteration 6213: Training loss = 0.0167, 0.0435 s ---\n",
            "--- Iteration 6214: Training loss = 0.0149, 0.0473 s ---\n",
            "--- Iteration 6215: Training loss = 0.0142, 0.0445 s ---\n",
            "--- Iteration 6216: Training loss = 0.0120, 0.0435 s ---\n",
            "--- Iteration 6217: Training loss = 0.0120, 0.0424 s ---\n",
            "--- Iteration 6218: Training loss = 0.0124, 0.0492 s ---\n",
            "--- Iteration 6219: Training loss = 0.0133, 0.0440 s ---\n",
            "--- Iteration 6220: Training loss = 0.0162, 0.0445 s ---\n",
            "--- Iteration 6220: Test loss = 0.0581 ---\n",
            "\n",
            "--- Iteration 6221: Training loss = 0.0121, 0.0425 s ---\n",
            "--- Iteration 6222: Training loss = 0.0087, 0.0431 s ---\n",
            "--- Iteration 6223: Training loss = 0.0114, 0.0483 s ---\n",
            "--- Iteration 6224: Training loss = 0.0109, 0.0449 s ---\n",
            "--- Iteration 6225: Training loss = 0.0131, 0.0433 s ---\n",
            "--- Iteration 6226: Training loss = 0.0106, 0.0428 s ---\n",
            "--- Iteration 6227: Training loss = 0.0090, 0.0422 s ---\n",
            "--- Iteration 6228: Training loss = 0.0167, 0.0424 s ---\n",
            "--- Iteration 6229: Training loss = 0.0093, 0.0424 s ---\n",
            "--- Iteration 6230: Training loss = 0.0178, 0.0425 s ---\n",
            "--- Iteration 6230: Test loss = 0.0545 ---\n",
            "\n",
            "--- Iteration 6231: Training loss = 0.0126, 0.0419 s ---\n",
            "--- Iteration 6232: Training loss = 0.0103, 0.0430 s ---\n",
            "--- Iteration 6233: Training loss = 0.0184, 0.0507 s ---\n",
            "--- Iteration 6234: Training loss = 0.0126, 0.0420 s ---\n",
            "--- Iteration 6235: Training loss = 0.0070, 0.0439 s ---\n",
            "--- Iteration 6236: Training loss = 0.0094, 0.0433 s ---\n",
            "--- Iteration 6237: Training loss = 0.0110, 0.0434 s ---\n",
            "--- Iteration 6238: Training loss = 0.0136, 0.0445 s ---\n",
            "--- Iteration 6239: Training loss = 0.0076, 0.0427 s ---\n",
            "--- Iteration 6240: Training loss = 0.0142, 0.0424 s ---\n",
            "--- Iteration 6240: Test loss = 0.0228 ---\n",
            "\n",
            "--- Iteration 6241: Training loss = 0.0173, 0.0564 s ---\n",
            "--- Iteration 6242: Training loss = 0.0106, 0.0882 s ---\n",
            "--- Iteration 6243: Training loss = 0.0110, 0.0533 s ---\n",
            "--- Iteration 6244: Training loss = 0.0117, 0.0510 s ---\n",
            "--- Iteration 6245: Training loss = 0.0140, 0.0545 s ---\n",
            "--- Iteration 6246: Training loss = 0.0162, 0.0666 s ---\n",
            "--- Iteration 6247: Training loss = 0.0132, 0.0636 s ---\n",
            "--- Iteration 6248: Training loss = 0.0118, 0.0490 s ---\n",
            "--- Iteration 6249: Training loss = 0.0130, 0.0562 s ---\n",
            "--- Iteration 6250: Training loss = 0.0099, 0.0562 s ---\n",
            "--- Iteration 6250: Test loss = 0.0654 ---\n",
            "\n",
            "--- Iteration 6251: Training loss = 0.0155, 0.0474 s ---\n",
            "--- Iteration 6252: Training loss = 0.0121, 0.0490 s ---\n",
            "--- Iteration 6253: Training loss = 0.0114, 0.0490 s ---\n",
            "--- Iteration 6254: Training loss = 0.0110, 0.0723 s ---\n",
            "--- Iteration 6255: Training loss = 0.0106, 0.0436 s ---\n",
            "--- Iteration 6256: Training loss = 0.0140, 0.0439 s ---\n",
            "--- Iteration 6257: Training loss = 0.0119, 0.0423 s ---\n",
            "--- Iteration 6258: Training loss = 0.0102, 0.0418 s ---\n",
            "--- Iteration 6259: Training loss = 0.0165, 0.0457 s ---\n",
            "--- Iteration 6260: Training loss = 0.0123, 0.0441 s ---\n",
            "--- Iteration 6260: Test loss = 0.0344 ---\n",
            "\n",
            "--- Iteration 6261: Training loss = 0.0133, 0.0439 s ---\n",
            "--- Iteration 6262: Training loss = 0.0127, 0.0437 s ---\n",
            "--- Iteration 6263: Training loss = 0.0112, 0.0431 s ---\n",
            "--- Iteration 6264: Training loss = 0.0081, 0.0449 s ---\n",
            "--- Iteration 6265: Training loss = 0.0064, 0.0476 s ---\n",
            "--- Iteration 6266: Training loss = 0.0130, 0.0443 s ---\n",
            "--- Iteration 6267: Training loss = 0.0088, 0.0468 s ---\n",
            "--- Iteration 6268: Training loss = 0.0173, 0.0425 s ---\n",
            "--- Iteration 6269: Training loss = 0.0085, 0.0422 s ---\n",
            "--- Iteration 6270: Training loss = 0.0097, 0.0419 s ---\n",
            "--- Iteration 6270: Test loss = 0.0246 ---\n",
            "\n",
            "--- Iteration 6271: Training loss = 0.0120, 0.0420 s ---\n",
            "--- Iteration 6272: Training loss = 0.0104, 0.0432 s ---\n",
            "--- Iteration 6273: Training loss = 0.0143, 0.0435 s ---\n",
            "--- Iteration 6274: Training loss = 0.0131, 0.0443 s ---\n",
            "--- Iteration 6275: Training loss = 0.0163, 0.0438 s ---\n",
            "--- Iteration 6276: Training loss = 0.0184, 0.0432 s ---\n",
            "--- Iteration 6277: Training loss = 0.0114, 0.0429 s ---\n",
            "--- Iteration 6278: Training loss = 0.0087, 0.0430 s ---\n",
            "--- Iteration 6279: Training loss = 0.0112, 0.0427 s ---\n",
            "--- Iteration 6280: Training loss = 0.0112, 0.0428 s ---\n",
            "--- Iteration 6280: Test loss = 0.0664 ---\n",
            "\n",
            "--- Iteration 6281: Training loss = 0.0082, 0.0435 s ---\n",
            "--- Iteration 6282: Training loss = 0.0118, 0.0440 s ---\n",
            "--- Iteration 6283: Training loss = 0.0187, 0.0436 s ---\n",
            "--- Iteration 6284: Training loss = 0.0146, 0.0431 s ---\n",
            "--- Iteration 6285: Training loss = 0.0108, 0.0416 s ---\n",
            "--- Iteration 6286: Training loss = 0.0110, 0.0425 s ---\n",
            "--- Iteration 6287: Training loss = 0.0113, 0.0434 s ---\n",
            "--- Iteration 6288: Training loss = 0.0094, 0.0442 s ---\n",
            "--- Iteration 6289: Training loss = 0.0102, 0.0450 s ---\n",
            "--- Iteration 6290: Training loss = 0.0156, 0.0438 s ---\n",
            "--- Iteration 6290: Test loss = 0.0325 ---\n",
            "\n",
            "--- Iteration 6291: Training loss = 0.0110, 0.0410 s ---\n",
            "--- Iteration 6292: Training loss = 0.0139, 0.0429 s ---\n",
            "--- Iteration 6293: Training loss = 0.0135, 0.0429 s ---\n",
            "--- Iteration 6294: Training loss = 0.0100, 0.0431 s ---\n",
            "--- Iteration 6295: Training loss = 0.0095, 0.0430 s ---\n",
            "--- Iteration 6296: Training loss = 0.0109, 0.0427 s ---\n",
            "--- Iteration 6297: Training loss = 0.0083, 0.0450 s ---\n",
            "--- Iteration 6298: Training loss = 0.0128, 0.0439 s ---\n",
            "--- Iteration 6299: Training loss = 0.0093, 0.0426 s ---\n",
            "--- Iteration 6300: Training loss = 0.0068, 0.0425 s ---\n",
            "--- Iteration 6300: Test loss = 0.0325 ---\n",
            "\n",
            "--- Iteration 6301: Training loss = 0.0095, 0.0415 s ---\n",
            "--- Iteration 6302: Training loss = 0.0062, 0.0420 s ---\n",
            "--- Iteration 6303: Training loss = 0.0168, 0.0430 s ---\n",
            "--- Iteration 6304: Training loss = 0.0105, 0.0456 s ---\n",
            "--- Iteration 6305: Training loss = 0.0141, 0.0483 s ---\n",
            "--- Iteration 6306: Training loss = 0.0125, 0.0422 s ---\n",
            "--- Iteration 6307: Training loss = 0.0154, 0.0423 s ---\n",
            "--- Iteration 6308: Training loss = 0.0110, 0.0425 s ---\n",
            "--- Iteration 6309: Training loss = 0.0118, 0.0417 s ---\n",
            "--- Iteration 6310: Training loss = 0.0085, 0.0430 s ---\n",
            "--- Iteration 6310: Test loss = 0.0438 ---\n",
            "\n",
            "--- Iteration 6311: Training loss = 0.0116, 0.0421 s ---\n",
            "--- Iteration 6312: Training loss = 0.0090, 0.0427 s ---\n",
            "--- Iteration 6313: Training loss = 0.0089, 0.0437 s ---\n",
            "--- Iteration 6314: Training loss = 0.0199, 0.0449 s ---\n",
            "--- Iteration 6315: Training loss = 0.0106, 0.0430 s ---\n",
            "--- Iteration 6316: Training loss = 0.0151, 0.0420 s ---\n",
            "--- Iteration 6317: Training loss = 0.0085, 0.0421 s ---\n",
            "--- Iteration 6318: Training loss = 0.0149, 0.0437 s ---\n",
            "--- Iteration 6319: Training loss = 0.0106, 0.0425 s ---\n",
            "--- Iteration 6320: Training loss = 0.0176, 0.0455 s ---\n",
            "--- Iteration 6320: Test loss = 0.0491 ---\n",
            "\n",
            "--- Iteration 6321: Training loss = 0.0096, 0.0472 s ---\n",
            "--- Iteration 6322: Training loss = 0.0084, 0.0431 s ---\n",
            "--- Iteration 6323: Training loss = 0.0096, 0.0427 s ---\n",
            "--- Iteration 6324: Training loss = 0.0138, 0.0498 s ---\n",
            "--- Iteration 6325: Training loss = 0.0120, 0.0442 s ---\n",
            "--- Iteration 6326: Training loss = 0.0184, 0.0441 s ---\n",
            "--- Iteration 6327: Training loss = 0.0085, 0.0447 s ---\n",
            "--- Iteration 6328: Training loss = 0.0123, 0.0432 s ---\n",
            "--- Iteration 6329: Training loss = 0.0134, 0.0427 s ---\n",
            "--- Iteration 6330: Training loss = 0.0104, 0.0427 s ---\n",
            "--- Iteration 6330: Test loss = 0.0254 ---\n",
            "\n",
            "--- Iteration 6331: Training loss = 0.0087, 0.0436 s ---\n",
            "--- Iteration 6332: Training loss = 0.0108, 0.0433 s ---\n",
            "--- Iteration 6333: Training loss = 0.0147, 0.0435 s ---\n",
            "--- Iteration 6334: Training loss = 0.0134, 0.0502 s ---\n",
            "--- Iteration 6335: Training loss = 0.0132, 0.0439 s ---\n",
            "--- Iteration 6336: Training loss = 0.0142, 0.0451 s ---\n",
            "--- Iteration 6337: Training loss = 0.0117, 0.0433 s ---\n",
            "--- Iteration 6338: Training loss = 0.0127, 0.0421 s ---\n",
            "--- Iteration 6339: Training loss = 0.0120, 0.0422 s ---\n",
            "--- Iteration 6340: Training loss = 0.0160, 0.0418 s ---\n",
            "--- Iteration 6340: Test loss = 0.0450 ---\n",
            "\n",
            "--- Iteration 6341: Training loss = 0.0138, 0.0420 s ---\n",
            "--- Iteration 6342: Training loss = 0.0152, 0.0435 s ---\n",
            "--- Iteration 6343: Training loss = 0.0112, 0.0448 s ---\n",
            "--- Iteration 6344: Training loss = 0.0132, 0.0477 s ---\n",
            "--- Iteration 6345: Training loss = 0.0200, 0.0418 s ---\n",
            "--- Iteration 6346: Training loss = 0.0137, 0.0432 s ---\n",
            "--- Iteration 6347: Training loss = 0.0153, 0.0456 s ---\n",
            "--- Iteration 6348: Training loss = 0.0129, 0.0442 s ---\n",
            "--- Iteration 6349: Training loss = 0.0116, 0.0421 s ---\n",
            "--- Iteration 6350: Training loss = 0.0147, 0.0452 s ---\n",
            "--- Iteration 6350: Test loss = 0.0485 ---\n",
            "\n",
            "--- Iteration 6351: Training loss = 0.0095, 0.0407 s ---\n",
            "--- Iteration 6352: Training loss = 0.0073, 0.0458 s ---\n",
            "--- Iteration 6353: Training loss = 0.0160, 0.0478 s ---\n",
            "--- Iteration 6354: Training loss = 0.0149, 0.0727 s ---\n",
            "--- Iteration 6355: Training loss = 0.0114, 0.0499 s ---\n",
            "--- Iteration 6356: Training loss = 0.0117, 0.0537 s ---\n",
            "--- Iteration 6357: Training loss = 0.0063, 0.0517 s ---\n",
            "--- Iteration 6358: Training loss = 0.0143, 0.0561 s ---\n",
            "--- Iteration 6359: Training loss = 0.0110, 0.0475 s ---\n",
            "--- Iteration 6360: Training loss = 0.0180, 0.0531 s ---\n",
            "--- Iteration 6360: Test loss = 0.0692 ---\n",
            "\n",
            "--- Iteration 6361: Training loss = 0.0134, 0.0459 s ---\n",
            "--- Iteration 6362: Training loss = 0.0101, 0.0476 s ---\n",
            "--- Iteration 6363: Training loss = 0.0110, 0.0604 s ---\n",
            "--- Iteration 6364: Training loss = 0.0125, 0.0496 s ---\n",
            "--- Iteration 6365: Training loss = 0.0112, 0.0868 s ---\n",
            "--- Iteration 6366: Training loss = 0.0143, 0.1275 s ---\n",
            "--- Iteration 6367: Training loss = 0.0107, 0.0672 s ---\n",
            "--- Iteration 6368: Training loss = 0.0096, 0.0520 s ---\n",
            "--- Iteration 6369: Training loss = 0.0079, 0.0436 s ---\n",
            "--- Iteration 6370: Training loss = 0.0080, 0.0422 s ---\n",
            "--- Iteration 6370: Test loss = 0.0276 ---\n",
            "\n",
            "--- Iteration 6371: Training loss = 0.0128, 0.0407 s ---\n",
            "--- Iteration 6372: Training loss = 0.0109, 0.0424 s ---\n",
            "--- Iteration 6373: Training loss = 0.0140, 0.0422 s ---\n",
            "--- Iteration 6374: Training loss = 0.0182, 0.0426 s ---\n",
            "--- Iteration 6375: Training loss = 0.0106, 0.0427 s ---\n",
            "--- Iteration 6376: Training loss = 0.0134, 0.0434 s ---\n",
            "--- Iteration 6377: Training loss = 0.0144, 0.0444 s ---\n",
            "--- Iteration 6378: Training loss = 0.0150, 0.0438 s ---\n",
            "--- Iteration 6379: Training loss = 0.0071, 0.0425 s ---\n",
            "--- Iteration 6380: Training loss = 0.0092, 0.0424 s ---\n",
            "--- Iteration 6380: Test loss = 0.0231 ---\n",
            "\n",
            "--- Iteration 6381: Training loss = 0.0128, 0.0407 s ---\n",
            "--- Iteration 6382: Training loss = 0.0134, 0.0422 s ---\n",
            "--- Iteration 6383: Training loss = 0.0140, 0.0424 s ---\n",
            "--- Iteration 6384: Training loss = 0.0126, 0.0425 s ---\n",
            "--- Iteration 6385: Training loss = 0.0076, 0.0521 s ---\n",
            "--- Iteration 6386: Training loss = 0.0109, 0.0420 s ---\n",
            "--- Iteration 6387: Training loss = 0.0160, 0.0461 s ---\n",
            "--- Iteration 6388: Training loss = 0.0183, 0.0426 s ---\n",
            "--- Iteration 6389: Training loss = 0.0099, 0.0430 s ---\n",
            "--- Iteration 6390: Training loss = 0.0152, 0.0427 s ---\n",
            "--- Iteration 6390: Test loss = 0.0211 ---\n",
            "\n",
            "--- Iteration 6391: Training loss = 0.0116, 0.0460 s ---\n",
            "--- Iteration 6392: Training loss = 0.0122, 0.0429 s ---\n",
            "--- Iteration 6393: Training loss = 0.0089, 0.0431 s ---\n",
            "--- Iteration 6394: Training loss = 0.0065, 0.0423 s ---\n",
            "--- Iteration 6395: Training loss = 0.0129, 0.0440 s ---\n",
            "--- Iteration 6396: Training loss = 0.0138, 0.0423 s ---\n",
            "--- Iteration 6397: Training loss = 0.0094, 0.0439 s ---\n",
            "--- Iteration 6398: Training loss = 0.0097, 0.0454 s ---\n",
            "--- Iteration 6399: Training loss = 0.0139, 0.0434 s ---\n",
            "--- Iteration 6400: Training loss = 0.0071, 0.0426 s ---\n",
            "--- Iteration 6400: Test loss = 0.0614 ---\n",
            "\n",
            "--- Iteration 6401: Training loss = 0.0125, 0.0418 s ---\n",
            "--- Iteration 6402: Training loss = 0.0112, 0.0426 s ---\n",
            "--- Iteration 6403: Training loss = 0.0195, 0.0425 s ---\n",
            "--- Iteration 6404: Training loss = 0.0115, 0.0425 s ---\n",
            "--- Iteration 6405: Training loss = 0.0090, 0.0427 s ---\n",
            "--- Iteration 6406: Training loss = 0.0177, 0.0422 s ---\n",
            "--- Iteration 6407: Training loss = 0.0087, 0.0462 s ---\n",
            "--- Iteration 6408: Training loss = 0.0093, 0.0427 s ---\n",
            "--- Iteration 6409: Training loss = 0.0087, 0.0422 s ---\n",
            "--- Iteration 6410: Training loss = 0.0115, 0.0428 s ---\n",
            "--- Iteration 6410: Test loss = 0.0560 ---\n",
            "\n",
            "--- Iteration 6411: Training loss = 0.0106, 0.0411 s ---\n",
            "--- Iteration 6412: Training loss = 0.0103, 0.0417 s ---\n",
            "--- Iteration 6413: Training loss = 0.0113, 0.0422 s ---\n",
            "--- Iteration 6414: Training loss = 0.0147, 0.0419 s ---\n",
            "--- Iteration 6415: Training loss = 0.0147, 0.0426 s ---\n",
            "--- Iteration 6416: Training loss = 0.0125, 0.0469 s ---\n",
            "--- Iteration 6417: Training loss = 0.0131, 0.0440 s ---\n",
            "--- Iteration 6418: Training loss = 0.0152, 0.0430 s ---\n",
            "--- Iteration 6419: Training loss = 0.0094, 0.0432 s ---\n",
            "--- Iteration 6420: Training loss = 0.0079, 0.0437 s ---\n",
            "--- Iteration 6420: Test loss = 0.0561 ---\n",
            "\n",
            "--- Iteration 6421: Training loss = 0.0096, 0.0426 s ---\n",
            "--- Iteration 6422: Training loss = 0.0132, 0.0437 s ---\n",
            "--- Iteration 6423: Training loss = 0.0168, 0.0451 s ---\n",
            "--- Iteration 6424: Training loss = 0.0093, 0.0439 s ---\n",
            "--- Iteration 6425: Training loss = 0.0096, 0.0459 s ---\n",
            "--- Iteration 6426: Training loss = 0.0129, 0.0424 s ---\n",
            "--- Iteration 6427: Training loss = 0.0120, 0.0433 s ---\n",
            "--- Iteration 6428: Training loss = 0.0137, 0.0439 s ---\n",
            "--- Iteration 6429: Training loss = 0.0128, 0.0447 s ---\n",
            "--- Iteration 6430: Training loss = 0.0139, 0.0425 s ---\n",
            "--- Iteration 6430: Test loss = 0.0226 ---\n",
            "\n",
            "--- Iteration 6431: Training loss = 0.0109, 0.0413 s ---\n",
            "--- Iteration 6432: Training loss = 0.0072, 0.0441 s ---\n",
            "--- Iteration 6433: Training loss = 0.0112, 0.0421 s ---\n",
            "--- Iteration 6434: Training loss = 0.0155, 0.0428 s ---\n",
            "--- Iteration 6435: Training loss = 0.0130, 0.0428 s ---\n",
            "--- Iteration 6436: Training loss = 0.0138, 0.0425 s ---\n",
            "--- Iteration 6437: Training loss = 0.0113, 0.0439 s ---\n",
            "--- Iteration 6438: Training loss = 0.0088, 0.0445 s ---\n",
            "--- Iteration 6439: Training loss = 0.0114, 0.0473 s ---\n",
            "--- Iteration 6440: Training loss = 0.0085, 0.0426 s ---\n",
            "--- Iteration 6440: Test loss = 0.0570 ---\n",
            "\n",
            "--- Iteration 6441: Training loss = 0.0108, 0.0427 s ---\n",
            "--- Iteration 6442: Training loss = 0.0143, 0.0434 s ---\n",
            "--- Iteration 6443: Training loss = 0.0060, 0.0439 s ---\n",
            "--- Iteration 6444: Training loss = 0.0192, 0.0447 s ---\n",
            "--- Iteration 6445: Training loss = 0.0104, 0.0464 s ---\n",
            "--- Iteration 6446: Training loss = 0.0086, 0.0422 s ---\n",
            "--- Iteration 6447: Training loss = 0.0069, 0.0422 s ---\n",
            "--- Iteration 6448: Training loss = 0.0163, 0.0431 s ---\n",
            "--- Iteration 6449: Training loss = 0.0118, 0.0442 s ---\n",
            "--- Iteration 6450: Training loss = 0.0072, 0.0443 s ---\n",
            "--- Iteration 6450: Test loss = 0.0687 ---\n",
            "\n",
            "--- Iteration 6451: Training loss = 0.0115, 0.0407 s ---\n",
            "--- Iteration 6452: Training loss = 0.0133, 0.0420 s ---\n",
            "--- Iteration 6453: Training loss = 0.0128, 0.0430 s ---\n",
            "--- Iteration 6454: Training loss = 0.0110, 0.0458 s ---\n",
            "--- Iteration 6455: Training loss = 0.0121, 0.0467 s ---\n",
            "--- Iteration 6456: Training loss = 0.0110, 0.0428 s ---\n",
            "--- Iteration 6457: Training loss = 0.0135, 0.0485 s ---\n",
            "--- Iteration 6458: Training loss = 0.0180, 0.0471 s ---\n",
            "--- Iteration 6459: Training loss = 0.0126, 0.0491 s ---\n",
            "--- Iteration 6460: Training loss = 0.0105, 0.0503 s ---\n",
            "--- Iteration 6460: Test loss = 0.0269 ---\n",
            "\n",
            "--- Iteration 6461: Training loss = 0.0161, 0.0744 s ---\n",
            "--- Iteration 6462: Training loss = 0.0098, 0.0484 s ---\n",
            "--- Iteration 6463: Training loss = 0.0124, 0.0554 s ---\n",
            "--- Iteration 6464: Training loss = 0.0092, 0.0480 s ---\n",
            "--- Iteration 6465: Training loss = 0.0084, 0.0510 s ---\n",
            "--- Iteration 6466: Training loss = 0.0091, 0.0505 s ---\n",
            "--- Iteration 6467: Training loss = 0.0161, 0.0837 s ---\n",
            "--- Iteration 6468: Training loss = 0.0120, 0.0574 s ---\n",
            "--- Iteration 6469: Training loss = 0.0109, 0.0501 s ---\n",
            "--- Iteration 6470: Training loss = 0.0135, 0.0527 s ---\n",
            "--- Iteration 6470: Test loss = 0.0302 ---\n",
            "\n",
            "--- Iteration 6471: Training loss = 0.0127, 0.0421 s ---\n",
            "--- Iteration 6472: Training loss = 0.0109, 0.0473 s ---\n",
            "--- Iteration 6473: Training loss = 0.0085, 0.0438 s ---\n",
            "--- Iteration 6474: Training loss = 0.0108, 0.0426 s ---\n",
            "--- Iteration 6475: Training loss = 0.0102, 0.0429 s ---\n",
            "--- Iteration 6476: Training loss = 0.0104, 0.0449 s ---\n",
            "--- Iteration 6477: Training loss = 0.0105, 0.0440 s ---\n",
            "--- Iteration 6478: Training loss = 0.0114, 0.0426 s ---\n",
            "--- Iteration 6479: Training loss = 0.0165, 0.0435 s ---\n",
            "--- Iteration 6480: Training loss = 0.0139, 0.0430 s ---\n",
            "--- Iteration 6480: Test loss = 0.0295 ---\n",
            "\n",
            "--- Iteration 6481: Training loss = 0.0102, 0.0417 s ---\n",
            "--- Iteration 6482: Training loss = 0.0129, 0.0427 s ---\n",
            "--- Iteration 6483: Training loss = 0.0104, 0.0442 s ---\n",
            "--- Iteration 6484: Training loss = 0.0158, 0.0488 s ---\n",
            "--- Iteration 6485: Training loss = 0.0118, 0.0472 s ---\n",
            "--- Iteration 6486: Training loss = 0.0060, 0.0505 s ---\n",
            "--- Iteration 6487: Training loss = 0.0162, 0.0471 s ---\n",
            "--- Iteration 6488: Training loss = 0.0144, 0.0476 s ---\n",
            "--- Iteration 6489: Training loss = 0.0095, 0.0494 s ---\n",
            "--- Iteration 6490: Training loss = 0.0088, 0.0472 s ---\n",
            "--- Iteration 6490: Test loss = 0.0268 ---\n",
            "\n",
            "--- Iteration 6491: Training loss = 0.0118, 0.0457 s ---\n",
            "--- Iteration 6492: Training loss = 0.0163, 0.0496 s ---\n",
            "--- Iteration 6493: Training loss = 0.0095, 0.1454 s ---\n",
            "--- Iteration 6494: Training loss = 0.0101, 0.0926 s ---\n",
            "--- Iteration 6495: Training loss = 0.0150, 0.0901 s ---\n",
            "--- Iteration 6496: Training loss = 0.0115, 0.0833 s ---\n",
            "--- Iteration 6497: Training loss = 0.0163, 0.0525 s ---\n",
            "--- Iteration 6498: Training loss = 0.0114, 0.0487 s ---\n",
            "--- Iteration 6499: Training loss = 0.0077, 0.0478 s ---\n",
            "--- Iteration 6500: Training loss = 0.0124, 0.0571 s ---\n",
            "--- Iteration 6500: Test loss = 0.0573 ---\n",
            "\n",
            "--- Iteration 6501: Training loss = 0.0110, 0.0468 s ---\n",
            "--- Iteration 6502: Training loss = 0.0141, 0.0479 s ---\n",
            "--- Iteration 6503: Training loss = 0.0146, 0.0479 s ---\n",
            "--- Iteration 6504: Training loss = 0.0139, 0.0749 s ---\n",
            "--- Iteration 6505: Training loss = 0.0124, 0.0715 s ---\n",
            "--- Iteration 6506: Training loss = 0.0095, 0.0612 s ---\n",
            "--- Iteration 6507: Training loss = 0.0097, 0.0481 s ---\n",
            "--- Iteration 6508: Training loss = 0.0114, 0.0438 s ---\n",
            "--- Iteration 6509: Training loss = 0.0130, 0.0441 s ---\n",
            "--- Iteration 6510: Training loss = 0.0104, 0.0428 s ---\n",
            "--- Iteration 6510: Test loss = 0.0512 ---\n",
            "\n",
            "--- Iteration 6511: Training loss = 0.0148, 0.0408 s ---\n",
            "--- Iteration 6512: Training loss = 0.0136, 0.0428 s ---\n",
            "--- Iteration 6513: Training loss = 0.0134, 0.0436 s ---\n",
            "--- Iteration 6514: Training loss = 0.0151, 0.0427 s ---\n",
            "--- Iteration 6515: Training loss = 0.0108, 0.0433 s ---\n",
            "--- Iteration 6516: Training loss = 0.0124, 0.0449 s ---\n",
            "--- Iteration 6517: Training loss = 0.0134, 0.0450 s ---\n",
            "--- Iteration 6518: Training loss = 0.0102, 0.0467 s ---\n",
            "--- Iteration 6519: Training loss = 0.0159, 0.0436 s ---\n",
            "--- Iteration 6520: Training loss = 0.0154, 0.0445 s ---\n",
            "--- Iteration 6520: Test loss = 0.0391 ---\n",
            "\n",
            "--- Iteration 6521: Training loss = 0.0153, 0.0438 s ---\n",
            "--- Iteration 6522: Training loss = 0.0199, 0.0441 s ---\n",
            "--- Iteration 6523: Training loss = 0.0139, 0.0459 s ---\n",
            "--- Iteration 6524: Training loss = 0.0095, 0.0427 s ---\n",
            "--- Iteration 6525: Training loss = 0.0117, 0.0429 s ---\n",
            "--- Iteration 6526: Training loss = 0.0127, 0.0435 s ---\n",
            "--- Iteration 6527: Training loss = 0.0122, 0.0450 s ---\n",
            "--- Iteration 6528: Training loss = 0.0079, 0.0438 s ---\n",
            "--- Iteration 6529: Training loss = 0.0133, 0.0429 s ---\n",
            "--- Iteration 6530: Training loss = 0.0106, 0.0425 s ---\n",
            "--- Iteration 6530: Test loss = 0.0387 ---\n",
            "\n",
            "--- Iteration 6531: Training loss = 0.0204, 0.0406 s ---\n",
            "--- Iteration 6532: Training loss = 0.0110, 0.0429 s ---\n",
            "--- Iteration 6533: Training loss = 0.0102, 0.0446 s ---\n",
            "--- Iteration 6534: Training loss = 0.0182, 0.0428 s ---\n",
            "--- Iteration 6535: Training loss = 0.0141, 0.0447 s ---\n",
            "--- Iteration 6536: Training loss = 0.0142, 0.0454 s ---\n",
            "--- Iteration 6537: Training loss = 0.0147, 0.0425 s ---\n",
            "--- Iteration 6538: Training loss = 0.0114, 0.0430 s ---\n",
            "--- Iteration 6539: Training loss = 0.0122, 0.0437 s ---\n",
            "--- Iteration 6540: Training loss = 0.0088, 0.0438 s ---\n",
            "--- Iteration 6540: Test loss = 0.0359 ---\n",
            "\n",
            "--- Iteration 6541: Training loss = 0.0121, 0.0438 s ---\n",
            "--- Iteration 6542: Training loss = 0.0105, 0.0454 s ---\n",
            "--- Iteration 6543: Training loss = 0.0111, 0.0502 s ---\n",
            "--- Iteration 6544: Training loss = 0.0107, 0.0432 s ---\n",
            "--- Iteration 6545: Training loss = 0.0106, 0.0436 s ---\n",
            "--- Iteration 6546: Training loss = 0.0116, 0.0466 s ---\n",
            "--- Iteration 6547: Training loss = 0.0112, 0.0434 s ---\n",
            "--- Iteration 6548: Training loss = 0.0122, 0.0423 s ---\n",
            "--- Iteration 6549: Training loss = 0.0056, 0.0438 s ---\n",
            "--- Iteration 6550: Training loss = 0.0110, 0.0430 s ---\n",
            "--- Iteration 6550: Test loss = 0.0392 ---\n",
            "\n",
            "--- Iteration 6551: Training loss = 0.0108, 0.0427 s ---\n",
            "--- Iteration 6552: Training loss = 0.0151, 0.0442 s ---\n",
            "--- Iteration 6553: Training loss = 0.0103, 0.0450 s ---\n",
            "--- Iteration 6554: Training loss = 0.0076, 0.0434 s ---\n",
            "--- Iteration 6555: Training loss = 0.0106, 0.0420 s ---\n",
            "--- Iteration 6556: Training loss = 0.0122, 0.0425 s ---\n",
            "--- Iteration 6557: Training loss = 0.0165, 0.0425 s ---\n",
            "--- Iteration 6558: Training loss = 0.0127, 0.0432 s ---\n",
            "--- Iteration 6559: Training loss = 0.0078, 0.0442 s ---\n",
            "--- Iteration 6560: Training loss = 0.0124, 0.0448 s ---\n",
            "--- Iteration 6560: Test loss = 0.0314 ---\n",
            "\n",
            "--- Iteration 6561: Training loss = 0.0104, 0.0430 s ---\n",
            "--- Iteration 6562: Training loss = 0.0157, 0.0430 s ---\n",
            "--- Iteration 6563: Training loss = 0.0091, 0.0456 s ---\n",
            "--- Iteration 6564: Training loss = 0.0106, 0.0432 s ---\n",
            "--- Iteration 6565: Training loss = 0.0136, 0.0429 s ---\n",
            "--- Iteration 6566: Training loss = 0.0104, 0.0438 s ---\n",
            "--- Iteration 6567: Training loss = 0.0094, 0.0446 s ---\n",
            "--- Iteration 6568: Training loss = 0.0086, 0.0429 s ---\n",
            "--- Iteration 6569: Training loss = 0.0071, 0.0425 s ---\n",
            "--- Iteration 6570: Training loss = 0.0110, 0.0429 s ---\n",
            "--- Iteration 6570: Test loss = 0.0616 ---\n",
            "\n",
            "--- Iteration 6571: Training loss = 0.0120, 0.0413 s ---\n",
            "--- Iteration 6572: Training loss = 0.0102, 0.0434 s ---\n",
            "--- Iteration 6573: Training loss = 0.0103, 0.0430 s ---\n",
            "--- Iteration 6574: Training loss = 0.0137, 0.0488 s ---\n",
            "--- Iteration 6575: Training loss = 0.0096, 0.0425 s ---\n",
            "--- Iteration 6576: Training loss = 0.0122, 0.0432 s ---\n",
            "--- Iteration 6577: Training loss = 0.0119, 0.0432 s ---\n",
            "--- Iteration 6578: Training loss = 0.0112, 0.0431 s ---\n",
            "--- Iteration 6579: Training loss = 0.0114, 0.0426 s ---\n",
            "--- Iteration 6580: Training loss = 0.0098, 0.0432 s ---\n",
            "--- Iteration 6580: Test loss = 0.0323 ---\n",
            "\n",
            "--- Iteration 6581: Training loss = 0.0075, 0.0434 s ---\n",
            "--- Iteration 6582: Training loss = 0.0125, 0.0438 s ---\n",
            "--- Iteration 6583: Training loss = 0.0115, 0.0440 s ---\n",
            "--- Iteration 6584: Training loss = 0.0162, 0.0430 s ---\n",
            "--- Iteration 6585: Training loss = 0.0164, 0.0435 s ---\n",
            "--- Iteration 6586: Training loss = 0.0099, 0.0433 s ---\n",
            "--- Iteration 6587: Training loss = 0.0069, 0.0440 s ---\n",
            "--- Iteration 6588: Training loss = 0.0142, 0.0448 s ---\n",
            "--- Iteration 6589: Training loss = 0.0166, 0.0445 s ---\n",
            "--- Iteration 6590: Training loss = 0.0085, 0.0428 s ---\n",
            "--- Iteration 6590: Test loss = 0.0391 ---\n",
            "\n",
            "--- Iteration 6591: Training loss = 0.0130, 0.0408 s ---\n",
            "--- Iteration 6592: Training loss = 0.0142, 0.0426 s ---\n",
            "--- Iteration 6593: Training loss = 0.0143, 0.0427 s ---\n",
            "--- Iteration 6594: Training loss = 0.0091, 0.0472 s ---\n",
            "--- Iteration 6595: Training loss = 0.0113, 0.0441 s ---\n",
            "--- Iteration 6596: Training loss = 0.0096, 0.0434 s ---\n",
            "--- Iteration 6597: Training loss = 0.0089, 0.0423 s ---\n",
            "--- Iteration 6598: Training loss = 0.0096, 0.0419 s ---\n",
            "--- Iteration 6599: Training loss = 0.0147, 0.0424 s ---\n",
            "--- Iteration 6600: Training loss = 0.0080, 0.0432 s ---\n",
            "--- Iteration 6600: Test loss = 0.0403 ---\n",
            "\n",
            "--- Iteration 6601: Training loss = 0.0086, 0.0423 s ---\n",
            "--- Iteration 6602: Training loss = 0.0086, 0.0431 s ---\n",
            "--- Iteration 6603: Training loss = 0.0134, 0.0446 s ---\n",
            "--- Iteration 6604: Training loss = 0.0114, 0.0440 s ---\n",
            "--- Iteration 6605: Training loss = 0.0127, 0.0436 s ---\n",
            "--- Iteration 6606: Training loss = 0.0147, 0.0421 s ---\n",
            "--- Iteration 6607: Training loss = 0.0085, 0.0422 s ---\n",
            "--- Iteration 6608: Training loss = 0.0092, 0.0427 s ---\n",
            "--- Iteration 6609: Training loss = 0.0059, 0.0439 s ---\n",
            "--- Iteration 6610: Training loss = 0.0146, 0.0434 s ---\n",
            "--- Iteration 6610: Test loss = 0.0329 ---\n",
            "\n",
            "--- Iteration 6611: Training loss = 0.0146, 0.0442 s ---\n",
            "--- Iteration 6612: Training loss = 0.0096, 0.0443 s ---\n",
            "--- Iteration 6613: Training loss = 0.0105, 0.0452 s ---\n",
            "--- Iteration 6614: Training loss = 0.0131, 0.0427 s ---\n",
            "--- Iteration 6615: Training loss = 0.0148, 0.0424 s ---\n",
            "--- Iteration 6616: Training loss = 0.0135, 0.0427 s ---\n",
            "--- Iteration 6617: Training loss = 0.0130, 0.0432 s ---\n",
            "--- Iteration 6618: Training loss = 0.0130, 0.0447 s ---\n",
            "--- Iteration 6619: Training loss = 0.0154, 0.0485 s ---\n",
            "--- Iteration 6620: Training loss = 0.0090, 0.0421 s ---\n",
            "--- Iteration 6620: Test loss = 0.0189 ---\n",
            "\n",
            "--- Iteration 6621: Training loss = 0.0126, 0.0418 s ---\n",
            "--- Iteration 6622: Training loss = 0.0121, 0.0431 s ---\n",
            "--- Iteration 6623: Training loss = 0.0094, 0.0450 s ---\n",
            "--- Iteration 6624: Training loss = 0.0136, 0.0452 s ---\n",
            "--- Iteration 6625: Training loss = 0.0132, 0.0441 s ---\n",
            "--- Iteration 6626: Training loss = 0.0095, 0.0430 s ---\n",
            "--- Iteration 6627: Training loss = 0.0087, 0.0427 s ---\n",
            "--- Iteration 6628: Training loss = 0.0158, 0.0426 s ---\n",
            "--- Iteration 6629: Training loss = 0.0169, 0.0466 s ---\n",
            "--- Iteration 6630: Training loss = 0.0091, 0.0446 s ---\n",
            "--- Iteration 6630: Test loss = 0.0233 ---\n",
            "\n",
            "--- Iteration 6631: Training loss = 0.0117, 0.0421 s ---\n",
            "--- Iteration 6632: Training loss = 0.0091, 0.0437 s ---\n",
            "--- Iteration 6633: Training loss = 0.0119, 0.0467 s ---\n",
            "--- Iteration 6634: Training loss = 0.0106, 0.0429 s ---\n",
            "--- Iteration 6635: Training loss = 0.0115, 0.0435 s ---\n",
            "--- Iteration 6636: Training loss = 0.0145, 0.0449 s ---\n",
            "--- Iteration 6637: Training loss = 0.0090, 0.0441 s ---\n",
            "--- Iteration 6638: Training loss = 0.0149, 0.0427 s ---\n",
            "--- Iteration 6639: Training loss = 0.0127, 0.0432 s ---\n",
            "--- Iteration 6640: Training loss = 0.0098, 0.0426 s ---\n",
            "--- Iteration 6640: Test loss = 0.0366 ---\n",
            "\n",
            "--- Iteration 6641: Training loss = 0.0089, 0.0475 s ---\n",
            "--- Iteration 6642: Training loss = 0.0124, 0.0444 s ---\n",
            "--- Iteration 6643: Training loss = 0.0071, 0.0476 s ---\n",
            "--- Iteration 6644: Training loss = 0.0139, 0.0427 s ---\n",
            "--- Iteration 6645: Training loss = 0.0079, 0.0424 s ---\n",
            "--- Iteration 6646: Training loss = 0.0092, 0.0440 s ---\n",
            "--- Iteration 6647: Training loss = 0.0111, 0.0442 s ---\n",
            "--- Iteration 6648: Training loss = 0.0107, 0.0444 s ---\n",
            "--- Iteration 6649: Training loss = 0.0099, 0.0426 s ---\n",
            "--- Iteration 6650: Training loss = 0.0099, 0.0427 s ---\n",
            "--- Iteration 6650: Test loss = 0.0565 ---\n",
            "\n",
            "--- Iteration 6651: Training loss = 0.0136, 0.0403 s ---\n",
            "--- Iteration 6652: Training loss = 0.0141, 0.0426 s ---\n",
            "--- Iteration 6653: Training loss = 0.0079, 0.0422 s ---\n",
            "--- Iteration 6654: Training loss = 0.0099, 0.0426 s ---\n",
            "--- Iteration 6655: Training loss = 0.0116, 0.0433 s ---\n",
            "--- Iteration 6656: Training loss = 0.0118, 0.0424 s ---\n",
            "--- Iteration 6657: Training loss = 0.0120, 0.0426 s ---\n",
            "--- Iteration 6658: Training loss = 0.0104, 0.0439 s ---\n",
            "--- Iteration 6659: Training loss = 0.0136, 0.0452 s ---\n",
            "--- Iteration 6660: Training loss = 0.0136, 0.0425 s ---\n",
            "--- Iteration 6660: Test loss = 0.0508 ---\n",
            "\n",
            "--- Iteration 6661: Training loss = 0.0113, 0.0404 s ---\n",
            "--- Iteration 6662: Training loss = 0.0113, 0.0423 s ---\n",
            "--- Iteration 6663: Training loss = 0.0136, 0.0430 s ---\n",
            "--- Iteration 6664: Training loss = 0.0095, 0.0431 s ---\n",
            "--- Iteration 6665: Training loss = 0.0106, 0.0429 s ---\n",
            "--- Iteration 6666: Training loss = 0.0125, 0.0445 s ---\n",
            "--- Iteration 6667: Training loss = 0.0081, 0.0451 s ---\n",
            "--- Iteration 6668: Training loss = 0.0097, 0.0436 s ---\n",
            "--- Iteration 6669: Training loss = 0.0093, 0.0429 s ---\n",
            "--- Iteration 6670: Training loss = 0.0117, 0.0432 s ---\n",
            "--- Iteration 6670: Test loss = 0.0501 ---\n",
            "\n",
            "--- Iteration 6671: Training loss = 0.0072, 0.0413 s ---\n",
            "--- Iteration 6672: Training loss = 0.0117, 0.0431 s ---\n",
            "--- Iteration 6673: Training loss = 0.0111, 0.0429 s ---\n",
            "--- Iteration 6674: Training loss = 0.0096, 0.0446 s ---\n",
            "--- Iteration 6675: Training loss = 0.0128, 0.0452 s ---\n",
            "--- Iteration 6676: Training loss = 0.0140, 0.0434 s ---\n",
            "--- Iteration 6677: Training loss = 0.0103, 0.0434 s ---\n",
            "--- Iteration 6678: Training loss = 0.0098, 0.0431 s ---\n",
            "--- Iteration 6679: Training loss = 0.0090, 0.0427 s ---\n",
            "--- Iteration 6680: Training loss = 0.0075, 0.0441 s ---\n",
            "--- Iteration 6680: Test loss = 0.0501 ---\n",
            "\n",
            "--- Iteration 6681: Training loss = 0.0145, 0.0438 s ---\n",
            "--- Iteration 6682: Training loss = 0.0066, 0.0445 s ---\n",
            "--- Iteration 6683: Training loss = 0.0180, 0.0469 s ---\n",
            "--- Iteration 6684: Training loss = 0.0066, 0.0427 s ---\n",
            "--- Iteration 6685: Training loss = 0.0110, 0.0433 s ---\n",
            "--- Iteration 6686: Training loss = 0.0075, 0.0490 s ---\n",
            "--- Iteration 6687: Training loss = 0.0113, 0.0433 s ---\n",
            "--- Iteration 6688: Training loss = 0.0118, 0.0434 s ---\n",
            "--- Iteration 6689: Training loss = 0.0107, 0.0429 s ---\n",
            "--- Iteration 6690: Training loss = 0.0122, 0.0436 s ---\n",
            "--- Iteration 6690: Test loss = 0.0456 ---\n",
            "\n",
            "--- Iteration 6691: Training loss = 0.0072, 0.0446 s ---\n",
            "--- Iteration 6692: Training loss = 0.0119, 0.0454 s ---\n",
            "--- Iteration 6693: Training loss = 0.0103, 0.0479 s ---\n",
            "--- Iteration 6694: Training loss = 0.0126, 0.0442 s ---\n",
            "--- Iteration 6695: Training loss = 0.0108, 0.0455 s ---\n",
            "--- Iteration 6696: Training loss = 0.0152, 0.0451 s ---\n",
            "--- Iteration 6697: Training loss = 0.0097, 0.0437 s ---\n",
            "--- Iteration 6698: Training loss = 0.0088, 0.0434 s ---\n",
            "--- Iteration 6699: Training loss = 0.0123, 0.0450 s ---\n",
            "--- Iteration 6700: Training loss = 0.0073, 0.0454 s ---\n",
            "--- Iteration 6700: Test loss = 0.0359 ---\n",
            "\n",
            "--- Iteration 6701: Training loss = 0.0125, 0.0428 s ---\n",
            "--- Iteration 6702: Training loss = 0.0134, 0.0435 s ---\n",
            "--- Iteration 6703: Training loss = 0.0134, 0.0472 s ---\n",
            "--- Iteration 6704: Training loss = 0.0105, 0.0436 s ---\n",
            "--- Iteration 6705: Training loss = 0.0180, 0.0442 s ---\n",
            "--- Iteration 6706: Training loss = 0.0051, 0.0443 s ---\n",
            "--- Iteration 6707: Training loss = 0.0120, 0.0434 s ---\n",
            "--- Iteration 6708: Training loss = 0.0100, 0.0489 s ---\n",
            "--- Iteration 6709: Training loss = 0.0136, 0.0445 s ---\n",
            "--- Iteration 6710: Training loss = 0.0195, 0.0449 s ---\n",
            "--- Iteration 6710: Test loss = 0.0314 ---\n",
            "\n",
            "--- Iteration 6711: Training loss = 0.0095, 0.0415 s ---\n",
            "--- Iteration 6712: Training loss = 0.0104, 0.0438 s ---\n",
            "--- Iteration 6713: Training loss = 0.0074, 0.0483 s ---\n",
            "--- Iteration 6714: Training loss = 0.0166, 0.0465 s ---\n",
            "--- Iteration 6715: Training loss = 0.0110, 0.0448 s ---\n",
            "--- Iteration 6716: Training loss = 0.0113, 0.0437 s ---\n",
            "--- Iteration 6717: Training loss = 0.0137, 0.0426 s ---\n",
            "--- Iteration 6718: Training loss = 0.0082, 0.0427 s ---\n",
            "--- Iteration 6719: Training loss = 0.0114, 0.0427 s ---\n",
            "--- Iteration 6720: Training loss = 0.0050, 0.0445 s ---\n",
            "--- Iteration 6720: Test loss = 0.0391 ---\n",
            "\n",
            "--- Iteration 6721: Training loss = 0.0126, 0.0429 s ---\n",
            "--- Iteration 6722: Training loss = 0.0108, 0.0465 s ---\n",
            "--- Iteration 6723: Training loss = 0.0127, 0.0458 s ---\n",
            "--- Iteration 6724: Training loss = 0.0139, 0.0422 s ---\n",
            "--- Iteration 6725: Training loss = 0.0153, 0.0438 s ---\n",
            "--- Iteration 6726: Training loss = 0.0082, 0.0444 s ---\n",
            "--- Iteration 6727: Training loss = 0.0095, 0.0438 s ---\n",
            "--- Iteration 6728: Training loss = 0.0140, 0.0423 s ---\n",
            "--- Iteration 6729: Training loss = 0.0088, 0.0415 s ---\n",
            "--- Iteration 6730: Training loss = 0.0116, 0.0464 s ---\n",
            "--- Iteration 6730: Test loss = 0.0458 ---\n",
            "\n",
            "--- Iteration 6731: Training loss = 0.0128, 0.0429 s ---\n",
            "--- Iteration 6732: Training loss = 0.0132, 0.0440 s ---\n",
            "--- Iteration 6733: Training loss = 0.0123, 0.0525 s ---\n",
            "--- Iteration 6734: Training loss = 0.0164, 0.0429 s ---\n",
            "--- Iteration 6735: Training loss = 0.0090, 0.0441 s ---\n",
            "--- Iteration 6736: Training loss = 0.0205, 0.0447 s ---\n",
            "--- Iteration 6737: Training loss = 0.0094, 0.0436 s ---\n",
            "--- Iteration 6738: Training loss = 0.0113, 0.0435 s ---\n",
            "--- Iteration 6739: Training loss = 0.0149, 0.0430 s ---\n",
            "--- Iteration 6740: Training loss = 0.0130, 0.0434 s ---\n",
            "--- Iteration 6740: Test loss = 0.0327 ---\n",
            "\n",
            "--- Iteration 6741: Training loss = 0.0093, 0.0432 s ---\n",
            "--- Iteration 6742: Training loss = 0.0125, 0.0443 s ---\n",
            "--- Iteration 6743: Training loss = 0.0100, 0.0471 s ---\n",
            "--- Iteration 6744: Training loss = 0.0106, 0.0421 s ---\n",
            "--- Iteration 6745: Training loss = 0.0103, 0.0425 s ---\n",
            "--- Iteration 6746: Training loss = 0.0189, 0.0424 s ---\n",
            "--- Iteration 6747: Training loss = 0.0127, 0.0430 s ---\n",
            "--- Iteration 6748: Training loss = 0.0094, 0.0445 s ---\n",
            "--- Iteration 6749: Training loss = 0.0091, 0.0456 s ---\n",
            "--- Iteration 6750: Training loss = 0.0165, 0.0433 s ---\n",
            "--- Iteration 6750: Test loss = 0.0512 ---\n",
            "\n",
            "--- Iteration 6751: Training loss = 0.0166, 0.0418 s ---\n",
            "--- Iteration 6752: Training loss = 0.0095, 0.0484 s ---\n",
            "--- Iteration 6753: Training loss = 0.0052, 0.0489 s ---\n",
            "--- Iteration 6754: Training loss = 0.0106, 0.0428 s ---\n",
            "--- Iteration 6755: Training loss = 0.0125, 0.0433 s ---\n",
            "--- Iteration 6756: Training loss = 0.0155, 0.0433 s ---\n",
            "--- Iteration 6757: Training loss = 0.0055, 0.0430 s ---\n",
            "--- Iteration 6758: Training loss = 0.0114, 0.0435 s ---\n",
            "--- Iteration 6759: Training loss = 0.0116, 0.0479 s ---\n",
            "--- Iteration 6760: Training loss = 0.0123, 0.0428 s ---\n",
            "--- Iteration 6760: Test loss = 0.0322 ---\n",
            "\n",
            "--- Iteration 6761: Training loss = 0.0106, 0.0408 s ---\n",
            "--- Iteration 6762: Training loss = 0.0127, 0.0424 s ---\n",
            "--- Iteration 6763: Training loss = 0.0121, 0.0440 s ---\n",
            "--- Iteration 6764: Training loss = 0.0110, 0.0426 s ---\n",
            "--- Iteration 6765: Training loss = 0.0087, 0.0435 s ---\n",
            "--- Iteration 6766: Training loss = 0.0137, 0.0444 s ---\n",
            "--- Iteration 6767: Training loss = 0.0151, 0.0442 s ---\n",
            "--- Iteration 6768: Training loss = 0.0102, 0.0426 s ---\n",
            "--- Iteration 6769: Training loss = 0.0112, 0.0457 s ---\n",
            "--- Iteration 6770: Training loss = 0.0084, 0.0427 s ---\n",
            "--- Iteration 6770: Test loss = 0.0275 ---\n",
            "\n",
            "--- Iteration 6771: Training loss = 0.0132, 0.0423 s ---\n",
            "--- Iteration 6772: Training loss = 0.0134, 0.0428 s ---\n",
            "--- Iteration 6773: Training loss = 0.0163, 0.0444 s ---\n",
            "--- Iteration 6774: Training loss = 0.0118, 0.0455 s ---\n",
            "--- Iteration 6775: Training loss = 0.0090, 0.0430 s ---\n",
            "--- Iteration 6776: Training loss = 0.0128, 0.0434 s ---\n",
            "--- Iteration 6777: Training loss = 0.0108, 0.0429 s ---\n",
            "--- Iteration 6778: Training loss = 0.0131, 0.0427 s ---\n",
            "--- Iteration 6779: Training loss = 0.0118, 0.0433 s ---\n",
            "--- Iteration 6780: Training loss = 0.0105, 0.0460 s ---\n",
            "--- Iteration 6780: Test loss = 0.0471 ---\n",
            "\n",
            "--- Iteration 6781: Training loss = 0.0093, 0.0423 s ---\n",
            "--- Iteration 6782: Training loss = 0.0091, 0.0432 s ---\n",
            "--- Iteration 6783: Training loss = 0.0107, 0.0459 s ---\n",
            "--- Iteration 6784: Training loss = 0.0083, 0.0431 s ---\n",
            "--- Iteration 6785: Training loss = 0.0074, 0.0426 s ---\n",
            "--- Iteration 6786: Training loss = 0.0087, 0.0428 s ---\n",
            "--- Iteration 6787: Training loss = 0.0119, 0.0454 s ---\n",
            "--- Iteration 6788: Training loss = 0.0102, 0.0436 s ---\n",
            "--- Iteration 6789: Training loss = 0.0136, 0.0418 s ---\n",
            "--- Iteration 6790: Training loss = 0.0187, 0.0416 s ---\n",
            "--- Iteration 6790: Test loss = 0.0469 ---\n",
            "\n",
            "--- Iteration 6791: Training loss = 0.0129, 0.0406 s ---\n",
            "--- Iteration 6792: Training loss = 0.0125, 0.0422 s ---\n",
            "--- Iteration 6793: Training loss = 0.0098, 0.0429 s ---\n",
            "--- Iteration 6794: Training loss = 0.0113, 0.0430 s ---\n",
            "--- Iteration 6795: Training loss = 0.0130, 0.0433 s ---\n",
            "--- Iteration 6796: Training loss = 0.0109, 0.0450 s ---\n",
            "--- Iteration 6797: Training loss = 0.0120, 0.0478 s ---\n",
            "--- Iteration 6798: Training loss = 0.0094, 0.0431 s ---\n",
            "--- Iteration 6799: Training loss = 0.0130, 0.0427 s ---\n",
            "--- Iteration 6800: Training loss = 0.0115, 0.0433 s ---\n",
            "--- Iteration 6800: Test loss = 0.0245 ---\n",
            "\n",
            "--- Iteration 6801: Training loss = 0.0062, 0.0429 s ---\n",
            "--- Iteration 6802: Training loss = 0.0119, 0.0446 s ---\n",
            "--- Iteration 6803: Training loss = 0.0118, 0.0479 s ---\n",
            "--- Iteration 6804: Training loss = 0.0096, 0.0432 s ---\n",
            "--- Iteration 6805: Training loss = 0.0203, 0.0438 s ---\n",
            "--- Iteration 6806: Training loss = 0.0133, 0.0431 s ---\n",
            "--- Iteration 6807: Training loss = 0.0151, 0.0445 s ---\n",
            "--- Iteration 6808: Training loss = 0.0051, 0.0446 s ---\n",
            "--- Iteration 6809: Training loss = 0.0132, 0.0432 s ---\n",
            "--- Iteration 6810: Training loss = 0.0101, 0.0467 s ---\n",
            "--- Iteration 6810: Test loss = 0.0631 ---\n",
            "\n",
            "--- Iteration 6811: Training loss = 0.0070, 0.0425 s ---\n",
            "--- Iteration 6812: Training loss = 0.0149, 0.0436 s ---\n",
            "--- Iteration 6813: Training loss = 0.0071, 0.0473 s ---\n",
            "--- Iteration 6814: Training loss = 0.0089, 0.0432 s ---\n",
            "--- Iteration 6815: Training loss = 0.0096, 0.0416 s ---\n",
            "--- Iteration 6816: Training loss = 0.0124, 0.0424 s ---\n",
            "--- Iteration 6817: Training loss = 0.0130, 0.0425 s ---\n",
            "--- Iteration 6818: Training loss = 0.0127, 0.0427 s ---\n",
            "--- Iteration 6819: Training loss = 0.0098, 0.0529 s ---\n",
            "--- Iteration 6820: Training loss = 0.0124, 0.0424 s ---\n",
            "--- Iteration 6820: Test loss = 0.0394 ---\n",
            "\n",
            "--- Iteration 6821: Training loss = 0.0114, 0.0412 s ---\n",
            "--- Iteration 6822: Training loss = 0.0158, 0.0431 s ---\n",
            "--- Iteration 6823: Training loss = 0.0123, 0.0490 s ---\n",
            "--- Iteration 6824: Training loss = 0.0133, 0.0444 s ---\n",
            "--- Iteration 6825: Training loss = 0.0097, 0.0444 s ---\n",
            "--- Iteration 6826: Training loss = 0.0083, 0.0422 s ---\n",
            "--- Iteration 6827: Training loss = 0.0141, 0.0426 s ---\n",
            "--- Iteration 6828: Training loss = 0.0173, 0.0421 s ---\n",
            "--- Iteration 6829: Training loss = 0.0080, 0.0432 s ---\n",
            "--- Iteration 6830: Training loss = 0.0129, 0.0436 s ---\n",
            "--- Iteration 6830: Test loss = 0.0460 ---\n",
            "\n",
            "--- Iteration 6831: Training loss = 0.0125, 0.0434 s ---\n",
            "--- Iteration 6832: Training loss = 0.0166, 0.0446 s ---\n",
            "--- Iteration 6833: Training loss = 0.0105, 0.0446 s ---\n",
            "--- Iteration 6834: Training loss = 0.0111, 0.0427 s ---\n",
            "--- Iteration 6835: Training loss = 0.0103, 0.0426 s ---\n",
            "--- Iteration 6836: Training loss = 0.0170, 0.0434 s ---\n",
            "--- Iteration 6837: Training loss = 0.0152, 0.0438 s ---\n",
            "--- Iteration 6838: Training loss = 0.0097, 0.0444 s ---\n",
            "--- Iteration 6839: Training loss = 0.0085, 0.0436 s ---\n",
            "--- Iteration 6840: Training loss = 0.0145, 0.0433 s ---\n",
            "--- Iteration 6840: Test loss = 0.0478 ---\n",
            "\n",
            "--- Iteration 6841: Training loss = 0.0161, 0.0468 s ---\n",
            "--- Iteration 6842: Training loss = 0.0084, 0.0425 s ---\n",
            "--- Iteration 6843: Training loss = 0.0140, 0.0467 s ---\n",
            "--- Iteration 6844: Training loss = 0.0112, 0.0447 s ---\n",
            "--- Iteration 6845: Training loss = 0.0092, 0.0467 s ---\n",
            "--- Iteration 6846: Training loss = 0.0118, 0.0431 s ---\n",
            "--- Iteration 6847: Training loss = 0.0108, 0.0439 s ---\n",
            "--- Iteration 6848: Training loss = 0.0111, 0.0455 s ---\n",
            "--- Iteration 6849: Training loss = 0.0095, 0.0445 s ---\n",
            "--- Iteration 6850: Training loss = 0.0142, 0.0434 s ---\n",
            "--- Iteration 6850: Test loss = 0.0354 ---\n",
            "\n",
            "--- Iteration 6851: Training loss = 0.0114, 0.0415 s ---\n",
            "--- Iteration 6852: Training loss = 0.0089, 0.0433 s ---\n",
            "--- Iteration 6853: Training loss = 0.0065, 0.0479 s ---\n",
            "--- Iteration 6854: Training loss = 0.0112, 0.0442 s ---\n",
            "--- Iteration 6855: Training loss = 0.0094, 0.0427 s ---\n",
            "--- Iteration 6856: Training loss = 0.0152, 0.0424 s ---\n",
            "--- Iteration 6857: Training loss = 0.0157, 0.0436 s ---\n",
            "--- Iteration 6858: Training loss = 0.0113, 0.0426 s ---\n",
            "--- Iteration 6859: Training loss = 0.0115, 0.0441 s ---\n",
            "--- Iteration 6860: Training loss = 0.0130, 0.0454 s ---\n",
            "--- Iteration 6860: Test loss = 0.0339 ---\n",
            "\n",
            "--- Iteration 6861: Training loss = 0.0093, 0.0433 s ---\n",
            "--- Iteration 6862: Training loss = 0.0121, 0.0434 s ---\n",
            "--- Iteration 6863: Training loss = 0.0113, 0.0506 s ---\n",
            "--- Iteration 6864: Training loss = 0.0109, 0.0452 s ---\n",
            "--- Iteration 6865: Training loss = 0.0097, 0.0451 s ---\n",
            "--- Iteration 6866: Training loss = 0.0114, 0.0432 s ---\n",
            "--- Iteration 6867: Training loss = 0.0117, 0.0426 s ---\n",
            "--- Iteration 6868: Training loss = 0.0165, 0.0425 s ---\n",
            "--- Iteration 6869: Training loss = 0.0102, 0.0435 s ---\n",
            "--- Iteration 6870: Training loss = 0.0120, 0.0440 s ---\n",
            "--- Iteration 6870: Test loss = 0.0446 ---\n",
            "\n",
            "--- Iteration 6871: Training loss = 0.0129, 0.0435 s ---\n",
            "--- Iteration 6872: Training loss = 0.0088, 0.0434 s ---\n",
            "--- Iteration 6873: Training loss = 0.0119, 0.0442 s ---\n",
            "--- Iteration 6874: Training loss = 0.0088, 0.0432 s ---\n",
            "--- Iteration 6875: Training loss = 0.0077, 0.0436 s ---\n",
            "--- Iteration 6876: Training loss = 0.0068, 0.0433 s ---\n",
            "--- Iteration 6877: Training loss = 0.0106, 0.0443 s ---\n",
            "--- Iteration 6878: Training loss = 0.0066, 0.0444 s ---\n",
            "--- Iteration 6879: Training loss = 0.0223, 0.0426 s ---\n",
            "--- Iteration 6880: Training loss = 0.0092, 0.0421 s ---\n",
            "--- Iteration 6880: Test loss = 0.0487 ---\n",
            "\n",
            "--- Iteration 6881: Training loss = 0.0100, 0.0412 s ---\n",
            "--- Iteration 6882: Training loss = 0.0110, 0.0428 s ---\n",
            "--- Iteration 6883: Training loss = 0.0119, 0.0450 s ---\n",
            "--- Iteration 6884: Training loss = 0.0103, 0.0427 s ---\n",
            "--- Iteration 6885: Training loss = 0.0115, 0.0438 s ---\n",
            "--- Iteration 6886: Training loss = 0.0130, 0.0448 s ---\n",
            "--- Iteration 6887: Training loss = 0.0119, 0.0428 s ---\n",
            "--- Iteration 6888: Training loss = 0.0081, 0.0425 s ---\n",
            "--- Iteration 6889: Training loss = 0.0090, 0.0426 s ---\n",
            "--- Iteration 6890: Training loss = 0.0103, 0.0422 s ---\n",
            "--- Iteration 6890: Test loss = 0.0452 ---\n",
            "\n",
            "--- Iteration 6891: Training loss = 0.0057, 0.0410 s ---\n",
            "--- Iteration 6892: Training loss = 0.0147, 0.0465 s ---\n",
            "--- Iteration 6893: Training loss = 0.0128, 0.0449 s ---\n",
            "--- Iteration 6894: Training loss = 0.0075, 0.0437 s ---\n",
            "--- Iteration 6895: Training loss = 0.0102, 0.0430 s ---\n",
            "--- Iteration 6896: Training loss = 0.0110, 0.0423 s ---\n",
            "--- Iteration 6897: Training loss = 0.0110, 0.0426 s ---\n",
            "--- Iteration 6898: Training loss = 0.0137, 0.0426 s ---\n",
            "--- Iteration 6899: Training loss = 0.0122, 0.0418 s ---\n",
            "--- Iteration 6900: Training loss = 0.0113, 0.0427 s ---\n",
            "--- Iteration 6900: Test loss = 0.0362 ---\n",
            "\n",
            "--- Iteration 6901: Training loss = 0.0119, 0.0431 s ---\n",
            "--- Iteration 6902: Training loss = 0.0143, 0.0449 s ---\n",
            "--- Iteration 6903: Training loss = 0.0094, 0.0442 s ---\n",
            "--- Iteration 6904: Training loss = 0.0099, 0.0419 s ---\n",
            "--- Iteration 6905: Training loss = 0.0159, 0.0428 s ---\n",
            "--- Iteration 6906: Training loss = 0.0096, 0.0439 s ---\n",
            "--- Iteration 6907: Training loss = 0.0099, 0.0440 s ---\n",
            "--- Iteration 6908: Training loss = 0.0120, 0.0451 s ---\n",
            "--- Iteration 6909: Training loss = 0.0087, 0.0453 s ---\n",
            "--- Iteration 6910: Training loss = 0.0125, 0.0436 s ---\n",
            "--- Iteration 6910: Test loss = 0.0357 ---\n",
            "\n",
            "--- Iteration 6911: Training loss = 0.0123, 0.0431 s ---\n",
            "--- Iteration 6912: Training loss = 0.0094, 0.0449 s ---\n",
            "--- Iteration 6913: Training loss = 0.0142, 0.0479 s ---\n",
            "--- Iteration 6914: Training loss = 0.0122, 0.0424 s ---\n",
            "--- Iteration 6915: Training loss = 0.0103, 0.0429 s ---\n",
            "--- Iteration 6916: Training loss = 0.0092, 0.0426 s ---\n",
            "--- Iteration 6917: Training loss = 0.0099, 0.0437 s ---\n",
            "--- Iteration 6918: Training loss = 0.0106, 0.0457 s ---\n",
            "--- Iteration 6919: Training loss = 0.0093, 0.0425 s ---\n",
            "--- Iteration 6920: Training loss = 0.0130, 0.0427 s ---\n",
            "--- Iteration 6920: Test loss = 0.0383 ---\n",
            "\n",
            "--- Iteration 6921: Training loss = 0.0159, 0.0411 s ---\n",
            "--- Iteration 6922: Training loss = 0.0056, 0.0441 s ---\n",
            "--- Iteration 6923: Training loss = 0.0161, 0.0430 s ---\n",
            "--- Iteration 6924: Training loss = 0.0122, 0.0426 s ---\n",
            "--- Iteration 6925: Training loss = 0.0122, 0.0432 s ---\n",
            "--- Iteration 6926: Training loss = 0.0079, 0.0446 s ---\n",
            "--- Iteration 6927: Training loss = 0.0092, 0.0437 s ---\n",
            "--- Iteration 6928: Training loss = 0.0069, 0.0436 s ---\n",
            "--- Iteration 6929: Training loss = 0.0055, 0.0426 s ---\n",
            "--- Iteration 6930: Training loss = 0.0117, 0.0427 s ---\n",
            "--- Iteration 6930: Test loss = 0.0324 ---\n",
            "\n",
            "--- Iteration 6931: Training loss = 0.0095, 0.0427 s ---\n",
            "--- Iteration 6932: Training loss = 0.0073, 0.0501 s ---\n",
            "--- Iteration 6933: Training loss = 0.0091, 0.0426 s ---\n",
            "--- Iteration 6934: Training loss = 0.0121, 0.0425 s ---\n",
            "--- Iteration 6935: Training loss = 0.0108, 0.0422 s ---\n",
            "--- Iteration 6936: Training loss = 0.0110, 0.0436 s ---\n",
            "--- Iteration 6937: Training loss = 0.0113, 0.0446 s ---\n",
            "--- Iteration 6938: Training loss = 0.0141, 0.0432 s ---\n",
            "--- Iteration 6939: Training loss = 0.0155, 0.0429 s ---\n",
            "--- Iteration 6940: Training loss = 0.0103, 0.0422 s ---\n",
            "--- Iteration 6940: Test loss = 0.0612 ---\n",
            "\n",
            "--- Iteration 6941: Training loss = 0.0124, 0.0409 s ---\n",
            "--- Iteration 6942: Training loss = 0.0122, 0.0428 s ---\n",
            "--- Iteration 6943: Training loss = 0.0081, 0.0419 s ---\n",
            "--- Iteration 6944: Training loss = 0.0098, 0.0428 s ---\n",
            "--- Iteration 6945: Training loss = 0.0068, 0.0433 s ---\n",
            "--- Iteration 6946: Training loss = 0.0149, 0.0444 s ---\n",
            "--- Iteration 6947: Training loss = 0.0121, 0.0445 s ---\n",
            "--- Iteration 6948: Training loss = 0.0147, 0.0428 s ---\n",
            "--- Iteration 6949: Training loss = 0.0126, 0.0427 s ---\n",
            "--- Iteration 6950: Training loss = 0.0088, 0.0431 s ---\n",
            "--- Iteration 6950: Test loss = 0.0600 ---\n",
            "\n",
            "--- Iteration 6951: Training loss = 0.0081, 0.0410 s ---\n",
            "--- Iteration 6952: Training loss = 0.0096, 0.0427 s ---\n",
            "--- Iteration 6953: Training loss = 0.0118, 0.0489 s ---\n",
            "--- Iteration 6954: Training loss = 0.0157, 0.0445 s ---\n",
            "--- Iteration 6955: Training loss = 0.0123, 0.0431 s ---\n",
            "--- Iteration 6956: Training loss = 0.0092, 0.0427 s ---\n",
            "--- Iteration 6957: Training loss = 0.0100, 0.0433 s ---\n",
            "--- Iteration 6958: Training loss = 0.0153, 0.0446 s ---\n",
            "--- Iteration 6959: Training loss = 0.0108, 0.0450 s ---\n",
            "--- Iteration 6960: Training loss = 0.0133, 0.0444 s ---\n",
            "--- Iteration 6960: Test loss = 0.0376 ---\n",
            "\n",
            "--- Iteration 6961: Training loss = 0.0193, 0.0412 s ---\n",
            "--- Iteration 6962: Training loss = 0.0074, 0.0460 s ---\n",
            "--- Iteration 6963: Training loss = 0.0082, 0.0431 s ---\n",
            "--- Iteration 6964: Training loss = 0.0128, 0.0424 s ---\n",
            "--- Iteration 6965: Training loss = 0.0139, 0.0430 s ---\n",
            "--- Iteration 6966: Training loss = 0.0147, 0.0446 s ---\n",
            "--- Iteration 6967: Training loss = 0.0100, 0.0445 s ---\n",
            "--- Iteration 6968: Training loss = 0.0138, 0.0420 s ---\n",
            "--- Iteration 6969: Training loss = 0.0117, 0.0425 s ---\n",
            "--- Iteration 6970: Training loss = 0.0098, 0.0434 s ---\n",
            "--- Iteration 6970: Test loss = 0.0466 ---\n",
            "\n",
            "--- Iteration 6971: Training loss = 0.0103, 0.0421 s ---\n",
            "--- Iteration 6972: Training loss = 0.0062, 0.0432 s ---\n",
            "--- Iteration 6973: Training loss = 0.0120, 0.0426 s ---\n",
            "--- Iteration 6974: Training loss = 0.0111, 0.0444 s ---\n",
            "--- Iteration 6975: Training loss = 0.0156, 0.0440 s ---\n",
            "--- Iteration 6976: Training loss = 0.0060, 0.0426 s ---\n",
            "--- Iteration 6977: Training loss = 0.0090, 0.0425 s ---\n",
            "--- Iteration 6978: Training loss = 0.0144, 0.0416 s ---\n",
            "--- Iteration 6979: Training loss = 0.0100, 0.0425 s ---\n",
            "--- Iteration 6980: Training loss = 0.0125, 0.0439 s ---\n",
            "--- Iteration 6980: Test loss = 0.0308 ---\n",
            "\n",
            "--- Iteration 6981: Training loss = 0.0104, 0.0436 s ---\n",
            "--- Iteration 6982: Training loss = 0.0110, 0.0454 s ---\n",
            "--- Iteration 6983: Training loss = 0.0119, 0.0439 s ---\n",
            "--- Iteration 6984: Training loss = 0.0101, 0.0425 s ---\n",
            "--- Iteration 6985: Training loss = 0.0093, 0.0429 s ---\n",
            "--- Iteration 6986: Training loss = 0.0090, 0.0432 s ---\n",
            "--- Iteration 6987: Training loss = 0.0130, 0.0437 s ---\n",
            "--- Iteration 6988: Training loss = 0.0102, 0.0451 s ---\n",
            "--- Iteration 6989: Training loss = 0.0112, 0.0435 s ---\n",
            "--- Iteration 6990: Training loss = 0.0077, 0.0432 s ---\n",
            "--- Iteration 6990: Test loss = 0.0754 ---\n",
            "\n",
            "--- Iteration 6991: Training loss = 0.0109, 0.0419 s ---\n",
            "--- Iteration 6992: Training loss = 0.0109, 0.0458 s ---\n",
            "--- Iteration 6993: Training loss = 0.0098, 0.0420 s ---\n",
            "--- Iteration 6994: Training loss = 0.0094, 0.0433 s ---\n",
            "--- Iteration 6995: Training loss = 0.0097, 0.0449 s ---\n",
            "--- Iteration 6996: Training loss = 0.0082, 0.0430 s ---\n",
            "--- Iteration 6997: Training loss = 0.0110, 0.0435 s ---\n",
            "--- Iteration 6998: Training loss = 0.0113, 0.0467 s ---\n",
            "--- Iteration 6999: Training loss = 0.0116, 0.0432 s ---\n",
            "--- Iteration 7000: Training loss = 0.0154, 0.0449 s ---\n",
            "--- Iteration 7000: Test loss = 0.0423 ---\n",
            "\n",
            "--- Iteration 7001: Training loss = 0.0113, 0.0427 s ---\n",
            "--- Iteration 7002: Training loss = 0.0107, 0.0532 s ---\n",
            "--- Iteration 7003: Training loss = 0.0119, 0.0451 s ---\n",
            "--- Iteration 7004: Training loss = 0.0074, 0.0458 s ---\n",
            "--- Iteration 7005: Training loss = 0.0111, 0.0426 s ---\n",
            "--- Iteration 7006: Training loss = 0.0093, 0.0423 s ---\n",
            "--- Iteration 7007: Training loss = 0.0114, 0.0429 s ---\n",
            "--- Iteration 7008: Training loss = 0.0147, 0.0422 s ---\n",
            "--- Iteration 7009: Training loss = 0.0084, 0.0434 s ---\n",
            "--- Iteration 7010: Training loss = 0.0145, 0.0448 s ---\n",
            "--- Iteration 7010: Test loss = 0.0513 ---\n",
            "\n",
            "--- Iteration 7011: Training loss = 0.0126, 0.0423 s ---\n",
            "--- Iteration 7012: Training loss = 0.0127, 0.0430 s ---\n",
            "--- Iteration 7013: Training loss = 0.0153, 0.0421 s ---\n",
            "--- Iteration 7014: Training loss = 0.0100, 0.0428 s ---\n",
            "--- Iteration 7015: Training loss = 0.0083, 0.0424 s ---\n",
            "--- Iteration 7016: Training loss = 0.0072, 0.0423 s ---\n",
            "--- Iteration 7017: Training loss = 0.0091, 0.0441 s ---\n",
            "--- Iteration 7018: Training loss = 0.0091, 0.0446 s ---\n",
            "--- Iteration 7019: Training loss = 0.0158, 0.0438 s ---\n",
            "--- Iteration 7020: Training loss = 0.0108, 0.0435 s ---\n",
            "--- Iteration 7020: Test loss = 0.0496 ---\n",
            "\n",
            "--- Iteration 7021: Training loss = 0.0132, 0.0448 s ---\n",
            "--- Iteration 7022: Training loss = 0.0057, 0.0452 s ---\n",
            "--- Iteration 7023: Training loss = 0.0122, 0.0426 s ---\n",
            "--- Iteration 7024: Training loss = 0.0134, 0.0450 s ---\n",
            "--- Iteration 7025: Training loss = 0.0116, 0.0441 s ---\n",
            "--- Iteration 7026: Training loss = 0.0116, 0.0454 s ---\n",
            "--- Iteration 7027: Training loss = 0.0131, 0.0423 s ---\n",
            "--- Iteration 7028: Training loss = 0.0093, 0.0440 s ---\n",
            "--- Iteration 7029: Training loss = 0.0133, 0.0435 s ---\n",
            "--- Iteration 7030: Training loss = 0.0138, 0.0451 s ---\n",
            "--- Iteration 7030: Test loss = 0.0561 ---\n",
            "\n",
            "--- Iteration 7031: Training loss = 0.0126, 0.0425 s ---\n",
            "--- Iteration 7032: Training loss = 0.0114, 0.0460 s ---\n",
            "--- Iteration 7033: Training loss = 0.0136, 0.0466 s ---\n",
            "--- Iteration 7034: Training loss = 0.0110, 0.0449 s ---\n",
            "--- Iteration 7035: Training loss = 0.0142, 0.0446 s ---\n",
            "--- Iteration 7036: Training loss = 0.0095, 0.0434 s ---\n",
            "--- Iteration 7037: Training loss = 0.0097, 0.0420 s ---\n",
            "--- Iteration 7038: Training loss = 0.0121, 0.0437 s ---\n",
            "--- Iteration 7039: Training loss = 0.0120, 0.0438 s ---\n",
            "--- Iteration 7040: Training loss = 0.0136, 0.0440 s ---\n",
            "--- Iteration 7040: Test loss = 0.0502 ---\n",
            "\n",
            "--- Iteration 7041: Training loss = 0.0159, 0.0429 s ---\n",
            "--- Iteration 7042: Training loss = 0.0068, 0.0502 s ---\n",
            "--- Iteration 7043: Training loss = 0.0130, 0.0434 s ---\n",
            "--- Iteration 7044: Training loss = 0.0075, 0.0426 s ---\n",
            "--- Iteration 7045: Training loss = 0.0115, 0.0445 s ---\n",
            "--- Iteration 7046: Training loss = 0.0118, 0.0440 s ---\n",
            "--- Iteration 7047: Training loss = 0.0141, 0.0437 s ---\n",
            "--- Iteration 7048: Training loss = 0.0126, 0.0445 s ---\n",
            "--- Iteration 7049: Training loss = 0.0108, 0.0436 s ---\n",
            "--- Iteration 7050: Training loss = 0.0097, 0.0442 s ---\n",
            "--- Iteration 7050: Test loss = 0.0556 ---\n",
            "\n",
            "--- Iteration 7051: Training loss = 0.0137, 0.0438 s ---\n",
            "--- Iteration 7052: Training loss = 0.0113, 0.0489 s ---\n",
            "--- Iteration 7053: Training loss = 0.0073, 0.0421 s ---\n",
            "--- Iteration 7054: Training loss = 0.0134, 0.0430 s ---\n",
            "--- Iteration 7055: Training loss = 0.0089, 0.0432 s ---\n",
            "--- Iteration 7056: Training loss = 0.0145, 0.0447 s ---\n",
            "--- Iteration 7057: Training loss = 0.0100, 0.0446 s ---\n",
            "--- Iteration 7058: Training loss = 0.0114, 0.0431 s ---\n",
            "--- Iteration 7059: Training loss = 0.0103, 0.0427 s ---\n",
            "--- Iteration 7060: Training loss = 0.0086, 0.0425 s ---\n",
            "--- Iteration 7060: Test loss = 0.0615 ---\n",
            "\n",
            "--- Iteration 7061: Training loss = 0.0079, 0.0412 s ---\n",
            "--- Iteration 7062: Training loss = 0.0109, 0.0429 s ---\n",
            "--- Iteration 7063: Training loss = 0.0075, 0.0423 s ---\n",
            "--- Iteration 7064: Training loss = 0.0103, 0.0428 s ---\n",
            "--- Iteration 7065: Training loss = 0.0095, 0.0449 s ---\n",
            "--- Iteration 7066: Training loss = 0.0148, 0.0431 s ---\n",
            "--- Iteration 7067: Training loss = 0.0146, 0.0426 s ---\n",
            "--- Iteration 7068: Training loss = 0.0117, 0.0428 s ---\n",
            "--- Iteration 7069: Training loss = 0.0097, 0.0425 s ---\n",
            "--- Iteration 7070: Training loss = 0.0127, 0.0438 s ---\n",
            "--- Iteration 7070: Test loss = 0.0306 ---\n",
            "\n",
            "--- Iteration 7071: Training loss = 0.0109, 0.0420 s ---\n",
            "--- Iteration 7072: Training loss = 0.0065, 0.0440 s ---\n",
            "--- Iteration 7073: Training loss = 0.0077, 0.0446 s ---\n",
            "--- Iteration 7074: Training loss = 0.0102, 0.0437 s ---\n",
            "--- Iteration 7075: Training loss = 0.0141, 0.0426 s ---\n",
            "--- Iteration 7076: Training loss = 0.0154, 0.0424 s ---\n",
            "--- Iteration 7077: Training loss = 0.0076, 0.0421 s ---\n",
            "--- Iteration 7078: Training loss = 0.0157, 0.0439 s ---\n",
            "--- Iteration 7079: Training loss = 0.0125, 0.0437 s ---\n",
            "--- Iteration 7080: Training loss = 0.0128, 0.0451 s ---\n",
            "--- Iteration 7080: Test loss = 0.0366 ---\n",
            "\n",
            "--- Iteration 7081: Training loss = 0.0114, 0.0422 s ---\n",
            "--- Iteration 7082: Training loss = 0.0153, 0.0440 s ---\n",
            "--- Iteration 7083: Training loss = 0.0136, 0.0424 s ---\n",
            "--- Iteration 7084: Training loss = 0.0134, 0.0434 s ---\n",
            "--- Iteration 7085: Training loss = 0.0133, 0.0425 s ---\n",
            "--- Iteration 7086: Training loss = 0.0161, 0.0424 s ---\n",
            "--- Iteration 7087: Training loss = 0.0100, 0.0482 s ---\n",
            "--- Iteration 7088: Training loss = 0.0179, 0.0430 s ---\n",
            "--- Iteration 7089: Training loss = 0.0105, 0.0430 s ---\n",
            "--- Iteration 7090: Training loss = 0.0105, 0.0427 s ---\n",
            "--- Iteration 7090: Test loss = 0.0613 ---\n",
            "\n",
            "--- Iteration 7091: Training loss = 0.0100, 0.0415 s ---\n",
            "--- Iteration 7092: Training loss = 0.0079, 0.0430 s ---\n",
            "--- Iteration 7093: Training loss = 0.0137, 0.0431 s ---\n",
            "--- Iteration 7094: Training loss = 0.0128, 0.0432 s ---\n",
            "--- Iteration 7095: Training loss = 0.0108, 0.0447 s ---\n",
            "--- Iteration 7096: Training loss = 0.0097, 0.0443 s ---\n",
            "--- Iteration 7097: Training loss = 0.0159, 0.0435 s ---\n",
            "--- Iteration 7098: Training loss = 0.0138, 0.0423 s ---\n",
            "--- Iteration 7099: Training loss = 0.0083, 0.0419 s ---\n",
            "--- Iteration 7100: Training loss = 0.0068, 0.0426 s ---\n",
            "--- Iteration 7100: Test loss = 0.0317 ---\n",
            "\n",
            "--- Iteration 7101: Training loss = 0.0115, 0.0421 s ---\n",
            "--- Iteration 7102: Training loss = 0.0104, 0.0433 s ---\n",
            "--- Iteration 7103: Training loss = 0.0175, 0.0452 s ---\n",
            "--- Iteration 7104: Training loss = 0.0123, 0.0448 s ---\n",
            "--- Iteration 7105: Training loss = 0.0078, 0.0433 s ---\n",
            "--- Iteration 7106: Training loss = 0.0120, 0.0422 s ---\n",
            "--- Iteration 7107: Training loss = 0.0141, 0.0425 s ---\n",
            "--- Iteration 7108: Training loss = 0.0065, 0.0432 s ---\n",
            "--- Iteration 7109: Training loss = 0.0110, 0.0431 s ---\n",
            "--- Iteration 7110: Training loss = 0.0115, 0.0445 s ---\n",
            "--- Iteration 7110: Test loss = 0.0554 ---\n",
            "\n",
            "--- Iteration 7111: Training loss = 0.0061, 0.0430 s ---\n",
            "--- Iteration 7112: Training loss = 0.0166, 0.0450 s ---\n",
            "--- Iteration 7113: Training loss = 0.0075, 0.0429 s ---\n",
            "--- Iteration 7114: Training loss = 0.0103, 0.0438 s ---\n",
            "--- Iteration 7115: Training loss = 0.0088, 0.0435 s ---\n",
            "--- Iteration 7116: Training loss = 0.0038, 0.0432 s ---\n",
            "--- Iteration 7117: Training loss = 0.0131, 0.0442 s ---\n",
            "--- Iteration 7118: Training loss = 0.0080, 0.0427 s ---\n",
            "--- Iteration 7119: Training loss = 0.0162, 0.0428 s ---\n",
            "--- Iteration 7120: Training loss = 0.0112, 0.0432 s ---\n",
            "--- Iteration 7120: Test loss = 0.0561 ---\n",
            "\n",
            "--- Iteration 7121: Training loss = 0.0088, 0.0411 s ---\n",
            "--- Iteration 7122: Training loss = 0.0070, 0.0424 s ---\n",
            "--- Iteration 7123: Training loss = 0.0130, 0.0426 s ---\n",
            "--- Iteration 7124: Training loss = 0.0103, 0.0428 s ---\n",
            "--- Iteration 7125: Training loss = 0.0111, 0.0437 s ---\n",
            "--- Iteration 7126: Training loss = 0.0071, 0.0448 s ---\n",
            "--- Iteration 7127: Training loss = 0.0160, 0.0453 s ---\n",
            "--- Iteration 7128: Training loss = 0.0111, 0.0420 s ---\n",
            "--- Iteration 7129: Training loss = 0.0089, 0.0425 s ---\n",
            "--- Iteration 7130: Training loss = 0.0077, 0.0426 s ---\n",
            "--- Iteration 7130: Test loss = 0.0335 ---\n",
            "\n",
            "--- Iteration 7131: Training loss = 0.0084, 0.0420 s ---\n",
            "--- Iteration 7132: Training loss = 0.0155, 0.0495 s ---\n",
            "--- Iteration 7133: Training loss = 0.0141, 0.0431 s ---\n",
            "--- Iteration 7134: Training loss = 0.0112, 0.0433 s ---\n",
            "--- Iteration 7135: Training loss = 0.0133, 0.0426 s ---\n",
            "--- Iteration 7136: Training loss = 0.0102, 0.0426 s ---\n",
            "--- Iteration 7137: Training loss = 0.0135, 0.0429 s ---\n",
            "--- Iteration 7138: Training loss = 0.0120, 0.0441 s ---\n",
            "--- Iteration 7139: Training loss = 0.0126, 0.0443 s ---\n",
            "--- Iteration 7140: Training loss = 0.0080, 0.0427 s ---\n",
            "--- Iteration 7140: Test loss = 0.0233 ---\n",
            "\n",
            "--- Iteration 7141: Training loss = 0.0122, 0.0422 s ---\n",
            "--- Iteration 7142: Training loss = 0.0143, 0.0452 s ---\n",
            "--- Iteration 7143: Training loss = 0.0137, 0.0430 s ---\n",
            "--- Iteration 7144: Training loss = 0.0092, 0.0428 s ---\n",
            "--- Iteration 7145: Training loss = 0.0197, 0.0439 s ---\n",
            "--- Iteration 7146: Training loss = 0.0098, 0.0449 s ---\n",
            "--- Iteration 7147: Training loss = 0.0106, 0.0437 s ---\n",
            "--- Iteration 7148: Training loss = 0.0088, 0.0488 s ---\n",
            "--- Iteration 7149: Training loss = 0.0120, 0.0432 s ---\n",
            "--- Iteration 7150: Training loss = 0.0096, 0.0437 s ---\n",
            "--- Iteration 7150: Test loss = 0.0692 ---\n",
            "\n",
            "--- Iteration 7151: Training loss = 0.0148, 0.0425 s ---\n",
            "--- Iteration 7152: Training loss = 0.0139, 0.0476 s ---\n",
            "--- Iteration 7153: Training loss = 0.0113, 0.0424 s ---\n",
            "--- Iteration 7154: Training loss = 0.0108, 0.0471 s ---\n",
            "--- Iteration 7155: Training loss = 0.0083, 0.0446 s ---\n",
            "--- Iteration 7156: Training loss = 0.0086, 0.0441 s ---\n",
            "--- Iteration 7157: Training loss = 0.0081, 0.0430 s ---\n",
            "--- Iteration 7158: Training loss = 0.0099, 0.0438 s ---\n",
            "--- Iteration 7159: Training loss = 0.0086, 0.0433 s ---\n",
            "--- Iteration 7160: Training loss = 0.0117, 0.0461 s ---\n",
            "--- Iteration 7160: Test loss = 0.0550 ---\n",
            "\n",
            "--- Iteration 7161: Training loss = 0.0136, 0.0429 s ---\n",
            "--- Iteration 7162: Training loss = 0.0130, 0.0490 s ---\n",
            "--- Iteration 7163: Training loss = 0.0106, 0.0416 s ---\n",
            "--- Iteration 7164: Training loss = 0.0099, 0.0433 s ---\n",
            "--- Iteration 7165: Training loss = 0.0119, 0.0445 s ---\n",
            "--- Iteration 7166: Training loss = 0.0116, 0.0448 s ---\n",
            "--- Iteration 7167: Training loss = 0.0165, 0.0428 s ---\n",
            "--- Iteration 7168: Training loss = 0.0100, 0.0428 s ---\n",
            "--- Iteration 7169: Training loss = 0.0126, 0.0423 s ---\n",
            "--- Iteration 7170: Training loss = 0.0147, 0.0434 s ---\n",
            "--- Iteration 7170: Test loss = 0.0382 ---\n",
            "\n",
            "--- Iteration 7171: Training loss = 0.0158, 0.0449 s ---\n",
            "--- Iteration 7172: Training loss = 0.0112, 0.0462 s ---\n",
            "--- Iteration 7173: Training loss = 0.0136, 0.0434 s ---\n",
            "--- Iteration 7174: Training loss = 0.0142, 0.0423 s ---\n",
            "--- Iteration 7175: Training loss = 0.0140, 0.0427 s ---\n",
            "--- Iteration 7176: Training loss = 0.0102, 0.0432 s ---\n",
            "--- Iteration 7177: Training loss = 0.0090, 0.0429 s ---\n",
            "--- Iteration 7178: Training loss = 0.0109, 0.0442 s ---\n",
            "--- Iteration 7179: Training loss = 0.0138, 0.0449 s ---\n",
            "--- Iteration 7180: Training loss = 0.0134, 0.0431 s ---\n",
            "--- Iteration 7180: Test loss = 0.0498 ---\n",
            "\n",
            "--- Iteration 7181: Training loss = 0.0072, 0.0421 s ---\n",
            "--- Iteration 7182: Training loss = 0.0090, 0.0435 s ---\n",
            "--- Iteration 7183: Training loss = 0.0117, 0.0430 s ---\n",
            "--- Iteration 7184: Training loss = 0.0084, 0.0429 s ---\n",
            "--- Iteration 7185: Training loss = 0.0175, 0.0428 s ---\n",
            "--- Iteration 7186: Training loss = 0.0083, 0.0466 s ---\n",
            "--- Iteration 7187: Training loss = 0.0131, 0.0438 s ---\n",
            "--- Iteration 7188: Training loss = 0.0131, 0.0423 s ---\n",
            "--- Iteration 7189: Training loss = 0.0083, 0.0424 s ---\n",
            "--- Iteration 7190: Training loss = 0.0107, 0.0478 s ---\n",
            "--- Iteration 7190: Test loss = 0.0294 ---\n",
            "\n",
            "--- Iteration 7191: Training loss = 0.0109, 0.0449 s ---\n",
            "--- Iteration 7192: Training loss = 0.0128, 0.0492 s ---\n",
            "--- Iteration 7193: Training loss = 0.0076, 0.0516 s ---\n",
            "--- Iteration 7194: Training loss = 0.0080, 0.0456 s ---\n",
            "--- Iteration 7195: Training loss = 0.0125, 0.0454 s ---\n",
            "--- Iteration 7196: Training loss = 0.0115, 0.0526 s ---\n",
            "--- Iteration 7197: Training loss = 0.0090, 0.0458 s ---\n",
            "--- Iteration 7198: Training loss = 0.0116, 0.0479 s ---\n",
            "--- Iteration 7199: Training loss = 0.0151, 0.0457 s ---\n",
            "--- Iteration 7200: Training loss = 0.0097, 0.0483 s ---\n",
            "--- Iteration 7200: Test loss = 0.0433 ---\n",
            "\n",
            "--- Iteration 7201: Training loss = 0.0101, 0.0495 s ---\n",
            "--- Iteration 7202: Training loss = 0.0099, 0.0460 s ---\n",
            "--- Iteration 7203: Training loss = 0.0083, 0.0458 s ---\n",
            "--- Iteration 7204: Training loss = 0.0084, 0.0498 s ---\n",
            "--- Iteration 7205: Training loss = 0.0135, 0.0540 s ---\n",
            "--- Iteration 7206: Training loss = 0.0094, 0.0457 s ---\n",
            "--- Iteration 7207: Training loss = 0.0126, 0.0466 s ---\n",
            "--- Iteration 7208: Training loss = 0.0123, 0.0463 s ---\n",
            "--- Iteration 7209: Training loss = 0.0092, 0.0466 s ---\n",
            "--- Iteration 7210: Training loss = 0.0081, 0.0540 s ---\n",
            "--- Iteration 7210: Test loss = 0.0358 ---\n",
            "\n",
            "--- Iteration 7211: Training loss = 0.0133, 0.0444 s ---\n",
            "--- Iteration 7212: Training loss = 0.0093, 0.0466 s ---\n",
            "--- Iteration 7213: Training loss = 0.0126, 0.0460 s ---\n",
            "--- Iteration 7214: Training loss = 0.0157, 0.0464 s ---\n",
            "--- Iteration 7215: Training loss = 0.0095, 0.0481 s ---\n",
            "--- Iteration 7216: Training loss = 0.0141, 0.0451 s ---\n",
            "--- Iteration 7217: Training loss = 0.0132, 0.0472 s ---\n",
            "--- Iteration 7218: Training loss = 0.0103, 0.0469 s ---\n",
            "--- Iteration 7219: Training loss = 0.0081, 0.0504 s ---\n",
            "--- Iteration 7220: Training loss = 0.0122, 0.0460 s ---\n",
            "--- Iteration 7220: Test loss = 0.0646 ---\n",
            "\n",
            "--- Iteration 7221: Training loss = 0.0107, 0.0453 s ---\n",
            "--- Iteration 7222: Training loss = 0.0066, 0.0461 s ---\n",
            "--- Iteration 7223: Training loss = 0.0075, 0.0484 s ---\n",
            "--- Iteration 7224: Training loss = 0.0068, 0.0457 s ---\n",
            "--- Iteration 7225: Training loss = 0.0154, 0.0478 s ---\n",
            "--- Iteration 7226: Training loss = 0.0140, 0.0471 s ---\n",
            "--- Iteration 7227: Training loss = 0.0125, 0.0450 s ---\n",
            "--- Iteration 7228: Training loss = 0.0074, 0.0510 s ---\n",
            "--- Iteration 7229: Training loss = 0.0098, 0.0455 s ---\n",
            "--- Iteration 7230: Training loss = 0.0071, 0.0472 s ---\n",
            "--- Iteration 7230: Test loss = 0.0517 ---\n",
            "\n",
            "--- Iteration 7231: Training loss = 0.0134, 0.0459 s ---\n",
            "--- Iteration 7232: Training loss = 0.0080, 0.0455 s ---\n",
            "--- Iteration 7233: Training loss = 0.0111, 0.0480 s ---\n",
            "--- Iteration 7234: Training loss = 0.0136, 0.0443 s ---\n",
            "--- Iteration 7235: Training loss = 0.0095, 0.0428 s ---\n",
            "--- Iteration 7236: Training loss = 0.0129, 0.0429 s ---\n",
            "--- Iteration 7237: Training loss = 0.0128, 0.0445 s ---\n",
            "--- Iteration 7238: Training loss = 0.0106, 0.0454 s ---\n",
            "--- Iteration 7239: Training loss = 0.0077, 0.0442 s ---\n",
            "--- Iteration 7240: Training loss = 0.0103, 0.0445 s ---\n",
            "--- Iteration 7240: Test loss = 0.0323 ---\n",
            "\n",
            "--- Iteration 7241: Training loss = 0.0126, 0.0421 s ---\n",
            "--- Iteration 7242: Training loss = 0.0071, 0.0497 s ---\n",
            "--- Iteration 7243: Training loss = 0.0092, 0.0449 s ---\n",
            "--- Iteration 7244: Training loss = 0.0089, 0.0430 s ---\n",
            "--- Iteration 7245: Training loss = 0.0100, 0.0426 s ---\n",
            "--- Iteration 7246: Training loss = 0.0093, 0.0460 s ---\n",
            "--- Iteration 7247: Training loss = 0.0095, 0.0427 s ---\n",
            "--- Iteration 7248: Training loss = 0.0092, 0.0461 s ---\n",
            "--- Iteration 7249: Training loss = 0.0160, 0.0447 s ---\n",
            "--- Iteration 7250: Training loss = 0.0139, 0.0437 s ---\n",
            "--- Iteration 7250: Test loss = 0.0348 ---\n",
            "\n",
            "--- Iteration 7251: Training loss = 0.0071, 0.0419 s ---\n",
            "--- Iteration 7252: Training loss = 0.0076, 0.0466 s ---\n",
            "--- Iteration 7253: Training loss = 0.0138, 0.0450 s ---\n",
            "--- Iteration 7254: Training loss = 0.0102, 0.0452 s ---\n",
            "--- Iteration 7255: Training loss = 0.0112, 0.0437 s ---\n",
            "--- Iteration 7256: Training loss = 0.0072, 0.0431 s ---\n",
            "--- Iteration 7257: Training loss = 0.0104, 0.0428 s ---\n",
            "--- Iteration 7258: Training loss = 0.0120, 0.0439 s ---\n",
            "--- Iteration 7259: Training loss = 0.0140, 0.0441 s ---\n",
            "--- Iteration 7260: Training loss = 0.0109, 0.0450 s ---\n",
            "--- Iteration 7260: Test loss = 0.0430 ---\n",
            "\n",
            "--- Iteration 7261: Training loss = 0.0118, 0.0426 s ---\n",
            "--- Iteration 7262: Training loss = 0.0118, 0.0469 s ---\n",
            "--- Iteration 7263: Training loss = 0.0087, 0.0425 s ---\n",
            "--- Iteration 7264: Training loss = 0.0093, 0.0434 s ---\n",
            "--- Iteration 7265: Training loss = 0.0109, 0.0443 s ---\n",
            "--- Iteration 7266: Training loss = 0.0089, 0.0434 s ---\n",
            "--- Iteration 7267: Training loss = 0.0139, 0.0429 s ---\n",
            "--- Iteration 7268: Training loss = 0.0123, 0.0427 s ---\n",
            "--- Iteration 7269: Training loss = 0.0109, 0.0431 s ---\n",
            "--- Iteration 7270: Training loss = 0.0094, 0.0431 s ---\n",
            "--- Iteration 7270: Test loss = 0.0346 ---\n",
            "\n",
            "--- Iteration 7271: Training loss = 0.0091, 0.0426 s ---\n",
            "--- Iteration 7272: Training loss = 0.0115, 0.0449 s ---\n",
            "--- Iteration 7273: Training loss = 0.0101, 0.0450 s ---\n",
            "--- Iteration 7274: Training loss = 0.0102, 0.0481 s ---\n",
            "--- Iteration 7275: Training loss = 0.0065, 0.0479 s ---\n",
            "--- Iteration 7276: Training loss = 0.0148, 0.0731 s ---\n",
            "--- Iteration 7277: Training loss = 0.0126, 0.0506 s ---\n",
            "--- Iteration 7278: Training loss = 0.0115, 0.0495 s ---\n",
            "--- Iteration 7279: Training loss = 0.0147, 0.0550 s ---\n",
            "--- Iteration 7280: Training loss = 0.0141, 0.0907 s ---\n",
            "--- Iteration 7280: Test loss = 0.0561 ---\n",
            "\n",
            "--- Iteration 7281: Training loss = 0.0118, 0.0514 s ---\n",
            "--- Iteration 7282: Training loss = 0.0116, 0.0487 s ---\n",
            "--- Iteration 7283: Training loss = 0.0108, 0.0477 s ---\n",
            "--- Iteration 7284: Training loss = 0.0114, 0.0653 s ---\n",
            "--- Iteration 7285: Training loss = 0.0111, 0.0656 s ---\n",
            "--- Iteration 7286: Training loss = 0.0107, 0.0529 s ---\n",
            "--- Iteration 7287: Training loss = 0.0099, 0.0473 s ---\n",
            "--- Iteration 7288: Training loss = 0.0076, 0.0793 s ---\n",
            "--- Iteration 7289: Training loss = 0.0097, 0.0477 s ---\n",
            "--- Iteration 7290: Training loss = 0.0088, 0.0549 s ---\n",
            "--- Iteration 7290: Test loss = 0.0155 ---\n",
            "\n",
            "--- Iteration 7291: Training loss = 0.0153, 0.0788 s ---\n",
            "--- Iteration 7292: Training loss = 0.0089, 0.0732 s ---\n",
            "--- Iteration 7293: Training loss = 0.0109, 0.0481 s ---\n",
            "--- Iteration 7294: Training loss = 0.0098, 0.0478 s ---\n",
            "--- Iteration 7295: Training loss = 0.0117, 0.0735 s ---\n",
            "--- Iteration 7296: Training loss = 0.0128, 0.0549 s ---\n",
            "--- Iteration 7297: Training loss = 0.0134, 0.0506 s ---\n",
            "--- Iteration 7298: Training loss = 0.0116, 0.0486 s ---\n",
            "--- Iteration 7299: Training loss = 0.0099, 0.0506 s ---\n",
            "--- Iteration 7300: Training loss = 0.0097, 0.0774 s ---\n",
            "--- Iteration 7300: Test loss = 0.0290 ---\n",
            "\n",
            "--- Iteration 7301: Training loss = 0.0134, 0.0488 s ---\n",
            "--- Iteration 7302: Training loss = 0.0064, 0.0847 s ---\n",
            "--- Iteration 7303: Training loss = 0.0154, 0.0856 s ---\n",
            "--- Iteration 7304: Training loss = 0.0081, 0.1116 s ---\n",
            "--- Iteration 7305: Training loss = 0.0117, 0.1019 s ---\n",
            "--- Iteration 7306: Training loss = 0.0079, 0.0996 s ---\n",
            "--- Iteration 7307: Training loss = 0.0116, 0.0912 s ---\n",
            "--- Iteration 7308: Training loss = 0.0126, 0.0811 s ---\n",
            "--- Iteration 7309: Training loss = 0.0097, 0.0568 s ---\n",
            "--- Iteration 7310: Training loss = 0.0057, 0.0514 s ---\n",
            "--- Iteration 7310: Test loss = 0.0529 ---\n",
            "\n",
            "--- Iteration 7311: Training loss = 0.0156, 0.0490 s ---\n",
            "--- Iteration 7312: Training loss = 0.0114, 0.0458 s ---\n",
            "--- Iteration 7313: Training loss = 0.0099, 0.0465 s ---\n",
            "--- Iteration 7314: Training loss = 0.0113, 0.0761 s ---\n",
            "--- Iteration 7315: Training loss = 0.0088, 0.0778 s ---\n",
            "--- Iteration 7316: Training loss = 0.0093, 0.0779 s ---\n",
            "--- Iteration 7317: Training loss = 0.0088, 0.0545 s ---\n",
            "--- Iteration 7318: Training loss = 0.0174, 0.0509 s ---\n",
            "--- Iteration 7319: Training loss = 0.0073, 0.0726 s ---\n",
            "--- Iteration 7320: Training loss = 0.0091, 0.0663 s ---\n",
            "--- Iteration 7320: Test loss = 0.0353 ---\n",
            "\n",
            "--- Iteration 7321: Training loss = 0.0125, 0.0443 s ---\n",
            "--- Iteration 7322: Training loss = 0.0080, 0.0611 s ---\n",
            "--- Iteration 7323: Training loss = 0.0080, 0.0477 s ---\n",
            "--- Iteration 7324: Training loss = 0.0112, 0.0634 s ---\n",
            "--- Iteration 7325: Training loss = 0.0101, 0.0847 s ---\n",
            "--- Iteration 7326: Training loss = 0.0109, 0.0667 s ---\n",
            "--- Iteration 7327: Training loss = 0.0165, 0.0551 s ---\n",
            "--- Iteration 7328: Training loss = 0.0083, 0.0810 s ---\n",
            "--- Iteration 7329: Training loss = 0.0116, 0.0948 s ---\n",
            "--- Iteration 7330: Training loss = 0.0106, 0.0738 s ---\n",
            "--- Iteration 7330: Test loss = 0.0590 ---\n",
            "\n",
            "--- Iteration 7331: Training loss = 0.0130, 0.0478 s ---\n",
            "--- Iteration 7332: Training loss = 0.0103, 0.0450 s ---\n",
            "--- Iteration 7333: Training loss = 0.0105, 0.0441 s ---\n",
            "--- Iteration 7334: Training loss = 0.0099, 0.0423 s ---\n",
            "--- Iteration 7335: Training loss = 0.0132, 0.0444 s ---\n",
            "--- Iteration 7336: Training loss = 0.0101, 0.0590 s ---\n",
            "--- Iteration 7337: Training loss = 0.0082, 0.0495 s ---\n",
            "--- Iteration 7338: Training loss = 0.0171, 0.0529 s ---\n",
            "--- Iteration 7339: Training loss = 0.0151, 0.0577 s ---\n",
            "--- Iteration 7340: Training loss = 0.0123, 0.0897 s ---\n",
            "--- Iteration 7340: Test loss = 0.0315 ---\n",
            "\n",
            "--- Iteration 7341: Training loss = 0.0069, 0.0615 s ---\n",
            "--- Iteration 7342: Training loss = 0.0119, 0.0498 s ---\n",
            "--- Iteration 7343: Training loss = 0.0064, 0.0670 s ---\n",
            "--- Iteration 7344: Training loss = 0.0125, 0.0550 s ---\n",
            "--- Iteration 7345: Training loss = 0.0105, 0.0532 s ---\n",
            "--- Iteration 7346: Training loss = 0.0118, 0.0439 s ---\n",
            "--- Iteration 7347: Training loss = 0.0169, 0.0474 s ---\n",
            "--- Iteration 7348: Training loss = 0.0118, 0.0488 s ---\n",
            "--- Iteration 7349: Training loss = 0.0112, 0.0446 s ---\n",
            "--- Iteration 7350: Training loss = 0.0102, 0.0433 s ---\n",
            "--- Iteration 7350: Test loss = 0.0627 ---\n",
            "\n",
            "--- Iteration 7351: Training loss = 0.0151, 0.0424 s ---\n",
            "--- Iteration 7352: Training loss = 0.0121, 0.0428 s ---\n",
            "--- Iteration 7353: Training loss = 0.0108, 0.0458 s ---\n",
            "--- Iteration 7354: Training loss = 0.0127, 0.0486 s ---\n",
            "--- Iteration 7355: Training loss = 0.0110, 0.0434 s ---\n",
            "--- Iteration 7356: Training loss = 0.0117, 0.0430 s ---\n",
            "--- Iteration 7357: Training loss = 0.0143, 0.0494 s ---\n",
            "--- Iteration 7358: Training loss = 0.0140, 0.0557 s ---\n",
            "--- Iteration 7359: Training loss = 0.0088, 0.0588 s ---\n",
            "--- Iteration 7360: Training loss = 0.0093, 0.0489 s ---\n",
            "--- Iteration 7360: Test loss = 0.0406 ---\n",
            "\n",
            "--- Iteration 7361: Training loss = 0.0143, 0.0472 s ---\n",
            "--- Iteration 7362: Training loss = 0.0068, 0.0500 s ---\n",
            "--- Iteration 7363: Training loss = 0.0141, 0.0471 s ---\n",
            "--- Iteration 7364: Training loss = 0.0107, 0.0873 s ---\n",
            "--- Iteration 7365: Training loss = 0.0121, 0.0950 s ---\n",
            "--- Iteration 7366: Training loss = 0.0134, 0.0737 s ---\n",
            "--- Iteration 7367: Training loss = 0.0090, 0.0819 s ---\n",
            "--- Iteration 7368: Training loss = 0.0127, 0.0509 s ---\n",
            "--- Iteration 7369: Training loss = 0.0109, 0.0500 s ---\n",
            "--- Iteration 7370: Training loss = 0.0105, 0.0428 s ---\n",
            "--- Iteration 7370: Test loss = 0.0478 ---\n",
            "\n",
            "--- Iteration 7371: Training loss = 0.0085, 0.0434 s ---\n",
            "--- Iteration 7372: Training loss = 0.0114, 0.0450 s ---\n",
            "--- Iteration 7373: Training loss = 0.0091, 0.0491 s ---\n",
            "--- Iteration 7374: Training loss = 0.0107, 0.0446 s ---\n",
            "--- Iteration 7375: Training loss = 0.0070, 0.0479 s ---\n",
            "--- Iteration 7376: Training loss = 0.0071, 0.0481 s ---\n",
            "--- Iteration 7377: Training loss = 0.0124, 0.0532 s ---\n",
            "--- Iteration 7378: Training loss = 0.0067, 0.0645 s ---\n",
            "--- Iteration 7379: Training loss = 0.0088, 0.0441 s ---\n",
            "--- Iteration 7380: Training loss = 0.0169, 0.0438 s ---\n",
            "--- Iteration 7380: Test loss = 0.0459 ---\n",
            "\n",
            "--- Iteration 7381: Training loss = 0.0127, 0.0430 s ---\n",
            "--- Iteration 7382: Training loss = 0.0064, 0.0473 s ---\n",
            "--- Iteration 7383: Training loss = 0.0126, 0.0444 s ---\n",
            "--- Iteration 7384: Training loss = 0.0113, 0.0503 s ---\n",
            "--- Iteration 7385: Training loss = 0.0097, 0.0826 s ---\n",
            "--- Iteration 7386: Training loss = 0.0102, 0.0490 s ---\n",
            "--- Iteration 7387: Training loss = 0.0121, 0.0488 s ---\n",
            "--- Iteration 7388: Training loss = 0.0113, 0.0488 s ---\n",
            "--- Iteration 7389: Training loss = 0.0104, 0.0576 s ---\n",
            "--- Iteration 7390: Training loss = 0.0064, 0.1174 s ---\n",
            "--- Iteration 7390: Test loss = 0.0539 ---\n",
            "\n",
            "--- Iteration 7391: Training loss = 0.0168, 0.1045 s ---\n",
            "--- Iteration 7392: Training loss = 0.0062, 0.0812 s ---\n",
            "--- Iteration 7393: Training loss = 0.0088, 0.0859 s ---\n",
            "--- Iteration 7394: Training loss = 0.0102, 0.0635 s ---\n",
            "--- Iteration 7395: Training loss = 0.0119, 0.0643 s ---\n",
            "--- Iteration 7396: Training loss = 0.0112, 0.0488 s ---\n",
            "--- Iteration 7397: Training loss = 0.0138, 0.0498 s ---\n",
            "--- Iteration 7398: Training loss = 0.0135, 0.0489 s ---\n",
            "--- Iteration 7399: Training loss = 0.0124, 0.0543 s ---\n",
            "--- Iteration 7400: Training loss = 0.0172, 0.0442 s ---\n",
            "--- Iteration 7400: Test loss = 0.0331 ---\n",
            "\n",
            "--- Iteration 7401: Training loss = 0.0074, 0.0409 s ---\n",
            "--- Iteration 7402: Training loss = 0.0103, 0.0431 s ---\n",
            "--- Iteration 7403: Training loss = 0.0062, 0.0572 s ---\n",
            "--- Iteration 7404: Training loss = 0.0070, 0.0659 s ---\n",
            "--- Iteration 7405: Training loss = 0.0146, 0.0795 s ---\n",
            "--- Iteration 7406: Training loss = 0.0099, 0.0501 s ---\n",
            "--- Iteration 7407: Training loss = 0.0106, 0.0642 s ---\n",
            "--- Iteration 7408: Training loss = 0.0114, 0.0703 s ---\n",
            "--- Iteration 7409: Training loss = 0.0089, 0.1017 s ---\n",
            "--- Iteration 7410: Training loss = 0.0092, 0.0602 s ---\n",
            "--- Iteration 7410: Test loss = 0.0692 ---\n",
            "\n",
            "--- Iteration 7411: Training loss = 0.0156, 0.0511 s ---\n",
            "--- Iteration 7412: Training loss = 0.0108, 0.0513 s ---\n",
            "--- Iteration 7413: Training loss = 0.0074, 0.0492 s ---\n",
            "--- Iteration 7414: Training loss = 0.0188, 0.0660 s ---\n",
            "--- Iteration 7415: Training loss = 0.0093, 0.0488 s ---\n",
            "--- Iteration 7416: Training loss = 0.0128, 0.0628 s ---\n",
            "--- Iteration 7417: Training loss = 0.0154, 0.0529 s ---\n",
            "--- Iteration 7418: Training loss = 0.0146, 0.0585 s ---\n",
            "--- Iteration 7419: Training loss = 0.0107, 0.0548 s ---\n",
            "--- Iteration 7420: Training loss = 0.0095, 0.0472 s ---\n",
            "--- Iteration 7420: Test loss = 0.0669 ---\n",
            "\n",
            "--- Iteration 7421: Training loss = 0.0072, 0.0410 s ---\n",
            "--- Iteration 7422: Training loss = 0.0087, 0.0443 s ---\n",
            "--- Iteration 7423: Training loss = 0.0068, 0.0456 s ---\n",
            "--- Iteration 7424: Training loss = 0.0084, 0.0432 s ---\n",
            "--- Iteration 7425: Training loss = 0.0100, 0.0427 s ---\n",
            "--- Iteration 7426: Training loss = 0.0119, 0.0450 s ---\n",
            "--- Iteration 7427: Training loss = 0.0068, 0.0439 s ---\n",
            "--- Iteration 7428: Training loss = 0.0096, 0.0465 s ---\n",
            "--- Iteration 7429: Training loss = 0.0182, 0.0422 s ---\n",
            "--- Iteration 7430: Training loss = 0.0073, 0.0431 s ---\n",
            "--- Iteration 7430: Test loss = 0.0652 ---\n",
            "\n",
            "--- Iteration 7431: Training loss = 0.0103, 0.0428 s ---\n",
            "--- Iteration 7432: Training loss = 0.0112, 0.0440 s ---\n",
            "--- Iteration 7433: Training loss = 0.0069, 0.0442 s ---\n",
            "--- Iteration 7434: Training loss = 0.0113, 0.0452 s ---\n",
            "--- Iteration 7435: Training loss = 0.0114, 0.0424 s ---\n",
            "--- Iteration 7436: Training loss = 0.0105, 0.0439 s ---\n",
            "--- Iteration 7437: Training loss = 0.0099, 0.0434 s ---\n",
            "--- Iteration 7438: Training loss = 0.0084, 0.0454 s ---\n",
            "--- Iteration 7439: Training loss = 0.0095, 0.0438 s ---\n",
            "--- Iteration 7440: Training loss = 0.0109, 0.0420 s ---\n",
            "--- Iteration 7440: Test loss = 0.0705 ---\n",
            "\n",
            "--- Iteration 7441: Training loss = 0.0103, 0.0417 s ---\n",
            "--- Iteration 7442: Training loss = 0.0103, 0.0467 s ---\n",
            "--- Iteration 7443: Training loss = 0.0094, 0.0431 s ---\n",
            "--- Iteration 7444: Training loss = 0.0124, 0.0455 s ---\n",
            "--- Iteration 7445: Training loss = 0.0118, 0.0427 s ---\n",
            "--- Iteration 7446: Training loss = 0.0123, 0.0426 s ---\n",
            "--- Iteration 7447: Training loss = 0.0087, 0.0425 s ---\n",
            "--- Iteration 7448: Training loss = 0.0075, 0.0423 s ---\n",
            "--- Iteration 7449: Training loss = 0.0055, 0.0428 s ---\n",
            "--- Iteration 7450: Training loss = 0.0135, 0.0439 s ---\n",
            "--- Iteration 7450: Test loss = 0.0411 ---\n",
            "\n",
            "--- Iteration 7451: Training loss = 0.0158, 0.0497 s ---\n",
            "--- Iteration 7452: Training loss = 0.0087, 0.0466 s ---\n",
            "--- Iteration 7453: Training loss = 0.0062, 0.0541 s ---\n",
            "--- Iteration 7454: Training loss = 0.0110, 0.0486 s ---\n",
            "--- Iteration 7455: Training loss = 0.0113, 0.0439 s ---\n",
            "--- Iteration 7456: Training loss = 0.0147, 0.0451 s ---\n",
            "--- Iteration 7457: Training loss = 0.0127, 0.0445 s ---\n",
            "--- Iteration 7458: Training loss = 0.0084, 0.0424 s ---\n",
            "--- Iteration 7459: Training loss = 0.0129, 0.0428 s ---\n",
            "--- Iteration 7460: Training loss = 0.0101, 0.0433 s ---\n",
            "--- Iteration 7460: Test loss = 0.0402 ---\n",
            "\n",
            "--- Iteration 7461: Training loss = 0.0098, 0.0411 s ---\n",
            "--- Iteration 7462: Training loss = 0.0097, 0.0425 s ---\n",
            "--- Iteration 7463: Training loss = 0.0127, 0.0426 s ---\n",
            "--- Iteration 7464: Training loss = 0.0106, 0.0429 s ---\n",
            "--- Iteration 7465: Training loss = 0.0161, 0.0440 s ---\n",
            "--- Iteration 7466: Training loss = 0.0123, 0.0431 s ---\n",
            "--- Iteration 7467: Training loss = 0.0101, 0.0427 s ---\n",
            "--- Iteration 7468: Training loss = 0.0118, 0.0423 s ---\n",
            "--- Iteration 7469: Training loss = 0.0045, 0.0437 s ---\n",
            "--- Iteration 7470: Training loss = 0.0149, 0.0440 s ---\n",
            "--- Iteration 7470: Test loss = 0.0362 ---\n",
            "\n",
            "--- Iteration 7471: Training loss = 0.0092, 0.0438 s ---\n",
            "--- Iteration 7472: Training loss = 0.0093, 0.0471 s ---\n",
            "--- Iteration 7473: Training loss = 0.0083, 0.0468 s ---\n",
            "--- Iteration 7474: Training loss = 0.0141, 0.0430 s ---\n",
            "--- Iteration 7475: Training loss = 0.0097, 0.0433 s ---\n",
            "--- Iteration 7476: Training loss = 0.0089, 0.0447 s ---\n",
            "--- Iteration 7477: Training loss = 0.0087, 0.0445 s ---\n",
            "--- Iteration 7478: Training loss = 0.0133, 0.0449 s ---\n",
            "--- Iteration 7479: Training loss = 0.0108, 0.0425 s ---\n",
            "--- Iteration 7480: Training loss = 0.0104, 0.0431 s ---\n",
            "--- Iteration 7480: Test loss = 0.0713 ---\n",
            "\n",
            "--- Iteration 7481: Training loss = 0.0084, 0.0426 s ---\n",
            "--- Iteration 7482: Training loss = 0.0128, 0.0441 s ---\n",
            "--- Iteration 7483: Training loss = 0.0114, 0.0457 s ---\n",
            "--- Iteration 7484: Training loss = 0.0160, 0.0431 s ---\n",
            "--- Iteration 7485: Training loss = 0.0079, 0.0422 s ---\n",
            "--- Iteration 7486: Training loss = 0.0134, 0.0466 s ---\n",
            "--- Iteration 7487: Training loss = 0.0078, 0.0434 s ---\n",
            "--- Iteration 7488: Training loss = 0.0084, 0.0448 s ---\n",
            "--- Iteration 7489: Training loss = 0.0141, 0.0434 s ---\n",
            "--- Iteration 7490: Training loss = 0.0109, 0.0430 s ---\n",
            "--- Iteration 7490: Test loss = 0.0358 ---\n",
            "\n",
            "--- Iteration 7491: Training loss = 0.0103, 0.0427 s ---\n",
            "--- Iteration 7492: Training loss = 0.0064, 0.0429 s ---\n",
            "--- Iteration 7493: Training loss = 0.0079, 0.0436 s ---\n",
            "--- Iteration 7494: Training loss = 0.0124, 0.0473 s ---\n",
            "--- Iteration 7495: Training loss = 0.0077, 0.0436 s ---\n",
            "--- Iteration 7496: Training loss = 0.0100, 0.0433 s ---\n",
            "--- Iteration 7497: Training loss = 0.0100, 0.0426 s ---\n",
            "--- Iteration 7498: Training loss = 0.0145, 0.0437 s ---\n",
            "--- Iteration 7499: Training loss = 0.0082, 0.0429 s ---\n",
            "--- Iteration 7500: Training loss = 0.0118, 0.0447 s ---\n",
            "--- Iteration 7500: Test loss = 0.0300 ---\n",
            "\n",
            "--- Iteration 7501: Training loss = 0.0129, 0.0428 s ---\n",
            "--- Iteration 7502: Training loss = 0.0168, 0.0435 s ---\n",
            "--- Iteration 7503: Training loss = 0.0141, 0.0444 s ---\n",
            "--- Iteration 7504: Training loss = 0.0156, 0.0430 s ---\n",
            "--- Iteration 7505: Training loss = 0.0068, 0.0428 s ---\n",
            "--- Iteration 7506: Training loss = 0.0087, 0.0425 s ---\n",
            "--- Iteration 7507: Training loss = 0.0111, 0.0455 s ---\n",
            "--- Iteration 7508: Training loss = 0.0116, 0.0450 s ---\n",
            "--- Iteration 7509: Training loss = 0.0084, 0.0430 s ---\n",
            "--- Iteration 7510: Training loss = 0.0076, 0.0428 s ---\n",
            "--- Iteration 7510: Test loss = 0.0587 ---\n",
            "\n",
            "--- Iteration 7511: Training loss = 0.0088, 0.0415 s ---\n",
            "--- Iteration 7512: Training loss = 0.0072, 0.0434 s ---\n",
            "--- Iteration 7513: Training loss = 0.0085, 0.0427 s ---\n",
            "--- Iteration 7514: Training loss = 0.0153, 0.0435 s ---\n",
            "--- Iteration 7515: Training loss = 0.0069, 0.0445 s ---\n",
            "--- Iteration 7516: Training loss = 0.0175, 0.0438 s ---\n",
            "--- Iteration 7517: Training loss = 0.0120, 0.0432 s ---\n",
            "--- Iteration 7518: Training loss = 0.0108, 0.0429 s ---\n",
            "--- Iteration 7519: Training loss = 0.0143, 0.0432 s ---\n",
            "--- Iteration 7520: Training loss = 0.0086, 0.0429 s ---\n",
            "--- Iteration 7520: Test loss = 0.0343 ---\n",
            "\n",
            "--- Iteration 7521: Training loss = 0.0107, 0.0428 s ---\n",
            "--- Iteration 7522: Training loss = 0.0080, 0.0438 s ---\n",
            "--- Iteration 7523: Training loss = 0.0081, 0.0448 s ---\n",
            "--- Iteration 7524: Training loss = 0.0118, 0.0486 s ---\n",
            "--- Iteration 7525: Training loss = 0.0168, 0.0433 s ---\n",
            "--- Iteration 7526: Training loss = 0.0169, 0.0430 s ---\n",
            "--- Iteration 7527: Training loss = 0.0093, 0.0447 s ---\n",
            "--- Iteration 7528: Training loss = 0.0080, 0.0440 s ---\n",
            "--- Iteration 7529: Training loss = 0.0095, 0.0427 s ---\n",
            "--- Iteration 7530: Training loss = 0.0129, 0.0437 s ---\n",
            "--- Iteration 7530: Test loss = 0.0338 ---\n",
            "\n",
            "--- Iteration 7531: Training loss = 0.0111, 0.0420 s ---\n",
            "--- Iteration 7532: Training loss = 0.0152, 0.0437 s ---\n",
            "--- Iteration 7533: Training loss = 0.0138, 0.0450 s ---\n",
            "--- Iteration 7534: Training loss = 0.0110, 0.0449 s ---\n",
            "--- Iteration 7535: Training loss = 0.0067, 0.0442 s ---\n",
            "--- Iteration 7536: Training loss = 0.0101, 0.0435 s ---\n",
            "--- Iteration 7537: Training loss = 0.0110, 0.0420 s ---\n",
            "--- Iteration 7538: Training loss = 0.0060, 0.0426 s ---\n",
            "--- Iteration 7539: Training loss = 0.0109, 0.0467 s ---\n",
            "--- Iteration 7540: Training loss = 0.0128, 0.0488 s ---\n",
            "--- Iteration 7540: Test loss = 0.0495 ---\n",
            "\n",
            "--- Iteration 7541: Training loss = 0.0126, 0.0404 s ---\n",
            "--- Iteration 7542: Training loss = 0.0143, 0.0426 s ---\n",
            "--- Iteration 7543: Training loss = 0.0110, 0.0439 s ---\n",
            "--- Iteration 7544: Training loss = 0.0165, 0.0429 s ---\n",
            "--- Iteration 7545: Training loss = 0.0137, 0.0432 s ---\n",
            "--- Iteration 7546: Training loss = 0.0078, 0.0442 s ---\n",
            "--- Iteration 7547: Training loss = 0.0143, 0.0448 s ---\n",
            "--- Iteration 7548: Training loss = 0.0104, 0.0517 s ---\n",
            "--- Iteration 7549: Training loss = 0.0136, 0.0472 s ---\n",
            "--- Iteration 7550: Training loss = 0.0104, 0.0428 s ---\n",
            "--- Iteration 7550: Test loss = 0.0499 ---\n",
            "\n",
            "--- Iteration 7551: Training loss = 0.0119, 0.0429 s ---\n",
            "--- Iteration 7552: Training loss = 0.0097, 0.0418 s ---\n",
            "--- Iteration 7553: Training loss = 0.0074, 0.0482 s ---\n",
            "--- Iteration 7554: Training loss = 0.0107, 0.0437 s ---\n",
            "--- Iteration 7555: Training loss = 0.0173, 0.0436 s ---\n",
            "--- Iteration 7556: Training loss = 0.0156, 0.0424 s ---\n",
            "--- Iteration 7557: Training loss = 0.0126, 0.0423 s ---\n",
            "--- Iteration 7558: Training loss = 0.0080, 0.0431 s ---\n",
            "--- Iteration 7559: Training loss = 0.0129, 0.0484 s ---\n",
            "--- Iteration 7560: Training loss = 0.0089, 0.0448 s ---\n",
            "--- Iteration 7560: Test loss = 0.0525 ---\n",
            "\n",
            "--- Iteration 7561: Training loss = 0.0109, 0.0419 s ---\n",
            "--- Iteration 7562: Training loss = 0.0094, 0.0434 s ---\n",
            "--- Iteration 7563: Training loss = 0.0135, 0.0497 s ---\n",
            "--- Iteration 7564: Training loss = 0.0153, 0.0437 s ---\n",
            "--- Iteration 7565: Training loss = 0.0130, 0.0451 s ---\n",
            "--- Iteration 7566: Training loss = 0.0080, 0.0439 s ---\n",
            "--- Iteration 7567: Training loss = 0.0107, 0.0427 s ---\n",
            "--- Iteration 7568: Training loss = 0.0109, 0.0453 s ---\n",
            "--- Iteration 7569: Training loss = 0.0124, 0.0440 s ---\n",
            "--- Iteration 7570: Training loss = 0.0069, 0.0448 s ---\n",
            "--- Iteration 7570: Test loss = 0.0417 ---\n",
            "\n",
            "--- Iteration 7571: Training loss = 0.0144, 0.0421 s ---\n",
            "--- Iteration 7572: Training loss = 0.0130, 0.0436 s ---\n",
            "--- Iteration 7573: Training loss = 0.0130, 0.0450 s ---\n",
            "--- Iteration 7574: Training loss = 0.0133, 0.0422 s ---\n",
            "--- Iteration 7575: Training loss = 0.0070, 0.0427 s ---\n",
            "--- Iteration 7576: Training loss = 0.0126, 0.0441 s ---\n",
            "--- Iteration 7577: Training loss = 0.0163, 0.0445 s ---\n",
            "--- Iteration 7578: Training loss = 0.0122, 0.0429 s ---\n",
            "--- Iteration 7579: Training loss = 0.0149, 0.0426 s ---\n",
            "--- Iteration 7580: Training loss = 0.0114, 0.0421 s ---\n",
            "--- Iteration 7580: Test loss = 0.0370 ---\n",
            "\n",
            "--- Iteration 7581: Training loss = 0.0127, 0.0434 s ---\n",
            "--- Iteration 7582: Training loss = 0.0066, 0.0424 s ---\n",
            "--- Iteration 7583: Training loss = 0.0129, 0.0426 s ---\n",
            "--- Iteration 7584: Training loss = 0.0086, 0.0420 s ---\n",
            "--- Iteration 7585: Training loss = 0.0090, 0.0442 s ---\n",
            "--- Iteration 7586: Training loss = 0.0074, 0.0436 s ---\n",
            "--- Iteration 7587: Training loss = 0.0058, 0.0428 s ---\n",
            "--- Iteration 7588: Training loss = 0.0111, 0.0429 s ---\n",
            "--- Iteration 7589: Training loss = 0.0072, 0.0421 s ---\n",
            "--- Iteration 7590: Training loss = 0.0139, 0.0426 s ---\n",
            "--- Iteration 7590: Test loss = 0.0230 ---\n",
            "\n",
            "--- Iteration 7591: Training loss = 0.0105, 0.0415 s ---\n",
            "--- Iteration 7592: Training loss = 0.0141, 0.0438 s ---\n",
            "--- Iteration 7593: Training loss = 0.0141, 0.0452 s ---\n",
            "--- Iteration 7594: Training loss = 0.0124, 0.0449 s ---\n",
            "--- Iteration 7595: Training loss = 0.0134, 0.0443 s ---\n",
            "--- Iteration 7596: Training loss = 0.0106, 0.0427 s ---\n",
            "--- Iteration 7597: Training loss = 0.0117, 0.0425 s ---\n",
            "--- Iteration 7598: Training loss = 0.0154, 0.0429 s ---\n",
            "--- Iteration 7599: Training loss = 0.0104, 0.0426 s ---\n",
            "--- Iteration 7600: Training loss = 0.0104, 0.0440 s ---\n",
            "--- Iteration 7600: Test loss = 0.0519 ---\n",
            "\n",
            "--- Iteration 7601: Training loss = 0.0068, 0.0427 s ---\n",
            "--- Iteration 7602: Training loss = 0.0129, 0.0436 s ---\n",
            "--- Iteration 7603: Training loss = 0.0076, 0.0448 s ---\n",
            "--- Iteration 7604: Training loss = 0.0093, 0.0451 s ---\n",
            "--- Iteration 7605: Training loss = 0.0068, 0.0486 s ---\n",
            "--- Iteration 7606: Training loss = 0.0103, 0.0427 s ---\n",
            "--- Iteration 7607: Training loss = 0.0126, 0.0424 s ---\n",
            "--- Iteration 7608: Training loss = 0.0115, 0.0422 s ---\n",
            "--- Iteration 7609: Training loss = 0.0126, 0.0419 s ---\n",
            "--- Iteration 7610: Training loss = 0.0134, 0.0435 s ---\n",
            "--- Iteration 7610: Test loss = 0.0414 ---\n",
            "\n",
            "--- Iteration 7611: Training loss = 0.0166, 0.0424 s ---\n",
            "--- Iteration 7612: Training loss = 0.0138, 0.0429 s ---\n",
            "--- Iteration 7613: Training loss = 0.0152, 0.0438 s ---\n",
            "--- Iteration 7614: Training loss = 0.0100, 0.0438 s ---\n",
            "--- Iteration 7615: Training loss = 0.0123, 0.0438 s ---\n",
            "--- Iteration 7616: Training loss = 0.0118, 0.0425 s ---\n",
            "--- Iteration 7617: Training loss = 0.0119, 0.0445 s ---\n",
            "--- Iteration 7618: Training loss = 0.0118, 0.0431 s ---\n",
            "--- Iteration 7619: Training loss = 0.0121, 0.0437 s ---\n",
            "--- Iteration 7620: Training loss = 0.0132, 0.0447 s ---\n",
            "--- Iteration 7620: Test loss = 0.0557 ---\n",
            "\n",
            "--- Iteration 7621: Training loss = 0.0136, 0.0419 s ---\n",
            "--- Iteration 7622: Training loss = 0.0141, 0.0428 s ---\n",
            "--- Iteration 7623: Training loss = 0.0146, 0.0442 s ---\n",
            "--- Iteration 7624: Training loss = 0.0118, 0.0429 s ---\n",
            "--- Iteration 7625: Training loss = 0.0127, 0.0428 s ---\n",
            "--- Iteration 7626: Training loss = 0.0097, 0.0435 s ---\n",
            "--- Iteration 7627: Training loss = 0.0095, 0.0447 s ---\n",
            "--- Iteration 7628: Training loss = 0.0125, 0.0446 s ---\n",
            "--- Iteration 7629: Training loss = 0.0076, 0.0437 s ---\n",
            "--- Iteration 7630: Training loss = 0.0099, 0.0468 s ---\n",
            "--- Iteration 7630: Test loss = 0.0403 ---\n",
            "\n",
            "--- Iteration 7631: Training loss = 0.0072, 0.0436 s ---\n",
            "--- Iteration 7632: Training loss = 0.0088, 0.0444 s ---\n",
            "--- Iteration 7633: Training loss = 0.0124, 0.0506 s ---\n",
            "--- Iteration 7634: Training loss = 0.0099, 0.0426 s ---\n",
            "--- Iteration 7635: Training loss = 0.0088, 0.0463 s ---\n",
            "--- Iteration 7636: Training loss = 0.0102, 0.0440 s ---\n",
            "--- Iteration 7637: Training loss = 0.0129, 0.0436 s ---\n",
            "--- Iteration 7638: Training loss = 0.0119, 0.0429 s ---\n",
            "--- Iteration 7639: Training loss = 0.0077, 0.0436 s ---\n",
            "--- Iteration 7640: Training loss = 0.0117, 0.0424 s ---\n",
            "--- Iteration 7640: Test loss = 0.0486 ---\n",
            "\n",
            "--- Iteration 7641: Training loss = 0.0122, 0.0423 s ---\n",
            "--- Iteration 7642: Training loss = 0.0141, 0.0433 s ---\n",
            "--- Iteration 7643: Training loss = 0.0141, 0.0522 s ---\n",
            "--- Iteration 7644: Training loss = 0.0087, 0.0477 s ---\n",
            "--- Iteration 7645: Training loss = 0.0084, 0.0486 s ---\n",
            "--- Iteration 7646: Training loss = 0.0107, 0.0462 s ---\n",
            "--- Iteration 7647: Training loss = 0.0111, 0.0507 s ---\n",
            "--- Iteration 7648: Training loss = 0.0117, 0.0573 s ---\n",
            "--- Iteration 7649: Training loss = 0.0115, 0.0544 s ---\n",
            "--- Iteration 7650: Training loss = 0.0128, 0.0432 s ---\n",
            "--- Iteration 7650: Test loss = 0.0295 ---\n",
            "\n",
            "--- Iteration 7651: Training loss = 0.0162, 0.0437 s ---\n",
            "--- Iteration 7652: Training loss = 0.0122, 0.0451 s ---\n",
            "--- Iteration 7653: Training loss = 0.0074, 0.0443 s ---\n",
            "--- Iteration 7654: Training loss = 0.0118, 0.0426 s ---\n",
            "--- Iteration 7655: Training loss = 0.0132, 0.0428 s ---\n",
            "--- Iteration 7656: Training loss = 0.0081, 0.0429 s ---\n",
            "--- Iteration 7657: Training loss = 0.0083, 0.0431 s ---\n",
            "--- Iteration 7658: Training loss = 0.0139, 0.0435 s ---\n",
            "--- Iteration 7659: Training loss = 0.0085, 0.0439 s ---\n",
            "--- Iteration 7660: Training loss = 0.0101, 0.0427 s ---\n",
            "--- Iteration 7660: Test loss = 0.0358 ---\n",
            "\n",
            "--- Iteration 7661: Training loss = 0.0066, 0.0407 s ---\n",
            "--- Iteration 7662: Training loss = 0.0092, 0.0428 s ---\n",
            "--- Iteration 7663: Training loss = 0.0135, 0.0431 s ---\n",
            "--- Iteration 7664: Training loss = 0.0096, 0.0428 s ---\n",
            "--- Iteration 7665: Training loss = 0.0086, 0.0427 s ---\n",
            "--- Iteration 7666: Training loss = 0.0080, 0.0432 s ---\n",
            "--- Iteration 7667: Training loss = 0.0062, 0.0439 s ---\n",
            "--- Iteration 7668: Training loss = 0.0072, 0.0450 s ---\n",
            "--- Iteration 7669: Training loss = 0.0064, 0.0427 s ---\n",
            "--- Iteration 7670: Training loss = 0.0093, 0.0425 s ---\n",
            "--- Iteration 7670: Test loss = 0.0442 ---\n",
            "\n",
            "--- Iteration 7671: Training loss = 0.0094, 0.0467 s ---\n",
            "--- Iteration 7672: Training loss = 0.0159, 0.0475 s ---\n",
            "--- Iteration 7673: Training loss = 0.0170, 0.0444 s ---\n",
            "--- Iteration 7674: Training loss = 0.0123, 0.0434 s ---\n",
            "--- Iteration 7675: Training loss = 0.0129, 0.0419 s ---\n",
            "--- Iteration 7676: Training loss = 0.0140, 0.0427 s ---\n",
            "--- Iteration 7677: Training loss = 0.0086, 0.0428 s ---\n",
            "--- Iteration 7678: Training loss = 0.0094, 0.0425 s ---\n",
            "--- Iteration 7679: Training loss = 0.0091, 0.0436 s ---\n",
            "--- Iteration 7680: Training loss = 0.0159, 0.0452 s ---\n",
            "--- Iteration 7680: Test loss = 0.0339 ---\n",
            "\n",
            "--- Iteration 7681: Training loss = 0.0103, 0.0439 s ---\n",
            "--- Iteration 7682: Training loss = 0.0138, 0.0464 s ---\n",
            "--- Iteration 7683: Training loss = 0.0103, 0.0415 s ---\n",
            "--- Iteration 7684: Training loss = 0.0100, 0.0426 s ---\n",
            "--- Iteration 7685: Training loss = 0.0085, 0.0433 s ---\n",
            "--- Iteration 7686: Training loss = 0.0082, 0.0443 s ---\n",
            "--- Iteration 7687: Training loss = 0.0099, 0.0439 s ---\n",
            "--- Iteration 7688: Training loss = 0.0122, 0.0421 s ---\n",
            "--- Iteration 7689: Training loss = 0.0101, 0.0421 s ---\n",
            "--- Iteration 7690: Training loss = 0.0103, 0.0424 s ---\n",
            "--- Iteration 7690: Test loss = 0.0369 ---\n",
            "\n",
            "--- Iteration 7691: Training loss = 0.0086, 0.0414 s ---\n",
            "--- Iteration 7692: Training loss = 0.0110, 0.0424 s ---\n",
            "--- Iteration 7693: Training loss = 0.0101, 0.0435 s ---\n",
            "--- Iteration 7694: Training loss = 0.0094, 0.0451 s ---\n",
            "--- Iteration 7695: Training loss = 0.0091, 0.0440 s ---\n",
            "--- Iteration 7696: Training loss = 0.0078, 0.0432 s ---\n",
            "--- Iteration 7697: Training loss = 0.0116, 0.0431 s ---\n",
            "--- Iteration 7698: Training loss = 0.0119, 0.0429 s ---\n",
            "--- Iteration 7699: Training loss = 0.0138, 0.0425 s ---\n",
            "--- Iteration 7700: Training loss = 0.0065, 0.0425 s ---\n",
            "--- Iteration 7700: Test loss = 0.0346 ---\n",
            "\n",
            "--- Iteration 7701: Training loss = 0.0112, 0.0426 s ---\n",
            "--- Iteration 7702: Training loss = 0.0086, 0.0449 s ---\n",
            "--- Iteration 7703: Training loss = 0.0117, 0.0437 s ---\n",
            "--- Iteration 7704: Training loss = 0.0087, 0.0423 s ---\n",
            "--- Iteration 7705: Training loss = 0.0139, 0.0435 s ---\n",
            "--- Iteration 7706: Training loss = 0.0102, 0.0422 s ---\n",
            "--- Iteration 7707: Training loss = 0.0105, 0.0441 s ---\n",
            "--- Iteration 7708: Training loss = 0.0119, 0.0437 s ---\n",
            "--- Iteration 7709: Training loss = 0.0048, 0.0441 s ---\n",
            "--- Iteration 7710: Training loss = 0.0088, 0.0438 s ---\n",
            "--- Iteration 7710: Test loss = 0.0317 ---\n",
            "\n",
            "--- Iteration 7711: Training loss = 0.0076, 0.0418 s ---\n",
            "--- Iteration 7712: Training loss = 0.0091, 0.0433 s ---\n",
            "--- Iteration 7713: Training loss = 0.0180, 0.0427 s ---\n",
            "--- Iteration 7714: Training loss = 0.0094, 0.0434 s ---\n",
            "--- Iteration 7715: Training loss = 0.0117, 0.0430 s ---\n",
            "--- Iteration 7716: Training loss = 0.0094, 0.0483 s ---\n",
            "--- Iteration 7717: Training loss = 0.0123, 0.0432 s ---\n",
            "--- Iteration 7718: Training loss = 0.0098, 0.0431 s ---\n",
            "--- Iteration 7719: Training loss = 0.0076, 0.0423 s ---\n",
            "--- Iteration 7720: Training loss = 0.0130, 0.0426 s ---\n",
            "--- Iteration 7720: Test loss = 0.0783 ---\n",
            "\n",
            "--- Iteration 7721: Training loss = 0.0158, 0.0411 s ---\n",
            "--- Iteration 7722: Training loss = 0.0125, 0.0422 s ---\n",
            "--- Iteration 7723: Training loss = 0.0139, 0.0416 s ---\n",
            "--- Iteration 7724: Training loss = 0.0080, 0.0431 s ---\n",
            "--- Iteration 7725: Training loss = 0.0097, 0.0430 s ---\n",
            "--- Iteration 7726: Training loss = 0.0061, 0.0447 s ---\n",
            "--- Iteration 7727: Training loss = 0.0106, 0.0436 s ---\n",
            "--- Iteration 7728: Training loss = 0.0090, 0.0424 s ---\n",
            "--- Iteration 7729: Training loss = 0.0122, 0.0424 s ---\n",
            "--- Iteration 7730: Training loss = 0.0132, 0.0431 s ---\n",
            "--- Iteration 7730: Test loss = 0.0443 ---\n",
            "\n",
            "--- Iteration 7731: Training loss = 0.0070, 0.0442 s ---\n",
            "--- Iteration 7732: Training loss = 0.0129, 0.0440 s ---\n",
            "--- Iteration 7733: Training loss = 0.0109, 0.0443 s ---\n",
            "--- Iteration 7734: Training loss = 0.0096, 0.0446 s ---\n",
            "--- Iteration 7735: Training loss = 0.0057, 0.0433 s ---\n",
            "--- Iteration 7736: Training loss = 0.0112, 0.0434 s ---\n",
            "--- Iteration 7737: Training loss = 0.0088, 0.0428 s ---\n",
            "--- Iteration 7738: Training loss = 0.0132, 0.0441 s ---\n",
            "--- Iteration 7739: Training loss = 0.0064, 0.0454 s ---\n",
            "--- Iteration 7740: Training loss = 0.0144, 0.0457 s ---\n",
            "--- Iteration 7740: Test loss = 0.0210 ---\n",
            "\n",
            "--- Iteration 7741: Training loss = 0.0145, 0.0419 s ---\n",
            "--- Iteration 7742: Training loss = 0.0085, 0.0469 s ---\n",
            "--- Iteration 7743: Training loss = 0.0113, 0.0415 s ---\n",
            "--- Iteration 7744: Training loss = 0.0123, 0.0426 s ---\n",
            "--- Iteration 7745: Training loss = 0.0098, 0.0434 s ---\n",
            "--- Iteration 7746: Training loss = 0.0122, 0.0444 s ---\n",
            "--- Iteration 7747: Training loss = 0.0091, 0.0441 s ---\n",
            "--- Iteration 7748: Training loss = 0.0052, 0.0437 s ---\n",
            "--- Iteration 7749: Training loss = 0.0112, 0.0427 s ---\n",
            "--- Iteration 7750: Training loss = 0.0150, 0.0424 s ---\n",
            "--- Iteration 7750: Test loss = 0.0656 ---\n",
            "\n",
            "--- Iteration 7751: Training loss = 0.0127, 0.0407 s ---\n",
            "--- Iteration 7752: Training loss = 0.0117, 0.0433 s ---\n",
            "--- Iteration 7753: Training loss = 0.0118, 0.0433 s ---\n",
            "--- Iteration 7754: Training loss = 0.0092, 0.0443 s ---\n",
            "--- Iteration 7755: Training loss = 0.0128, 0.0448 s ---\n",
            "--- Iteration 7756: Training loss = 0.0151, 0.0443 s ---\n",
            "--- Iteration 7757: Training loss = 0.0145, 0.0428 s ---\n",
            "--- Iteration 7758: Training loss = 0.0056, 0.0436 s ---\n",
            "--- Iteration 7759: Training loss = 0.0105, 0.0421 s ---\n",
            "--- Iteration 7760: Training loss = 0.0113, 0.0465 s ---\n",
            "--- Iteration 7760: Test loss = 0.0450 ---\n",
            "\n",
            "--- Iteration 7761: Training loss = 0.0055, 0.0426 s ---\n",
            "--- Iteration 7762: Training loss = 0.0074, 0.0505 s ---\n",
            "--- Iteration 7763: Training loss = 0.0100, 0.0430 s ---\n",
            "--- Iteration 7764: Training loss = 0.0060, 0.0426 s ---\n",
            "--- Iteration 7765: Training loss = 0.0094, 0.0439 s ---\n",
            "--- Iteration 7766: Training loss = 0.0099, 0.0486 s ---\n",
            "--- Iteration 7767: Training loss = 0.0093, 0.0428 s ---\n",
            "--- Iteration 7768: Training loss = 0.0104, 0.0417 s ---\n",
            "--- Iteration 7769: Training loss = 0.0127, 0.0428 s ---\n",
            "--- Iteration 7770: Training loss = 0.0112, 0.0425 s ---\n",
            "--- Iteration 7770: Test loss = 0.0338 ---\n",
            "\n",
            "--- Iteration 7771: Training loss = 0.0111, 0.0422 s ---\n",
            "--- Iteration 7772: Training loss = 0.0099, 0.0434 s ---\n",
            "--- Iteration 7773: Training loss = 0.0069, 0.0451 s ---\n",
            "--- Iteration 7774: Training loss = 0.0107, 0.0441 s ---\n",
            "--- Iteration 7775: Training loss = 0.0089, 0.0430 s ---\n",
            "--- Iteration 7776: Training loss = 0.0055, 0.0426 s ---\n",
            "--- Iteration 7777: Training loss = 0.0105, 0.0432 s ---\n",
            "--- Iteration 7778: Training loss = 0.0134, 0.0439 s ---\n",
            "--- Iteration 7779: Training loss = 0.0056, 0.0434 s ---\n",
            "--- Iteration 7780: Training loss = 0.0150, 0.0447 s ---\n",
            "--- Iteration 7780: Test loss = 0.0212 ---\n",
            "\n",
            "--- Iteration 7781: Training loss = 0.0067, 0.0429 s ---\n",
            "--- Iteration 7782: Training loss = 0.0066, 0.0469 s ---\n",
            "--- Iteration 7783: Training loss = 0.0081, 0.0464 s ---\n",
            "--- Iteration 7784: Training loss = 0.0098, 0.0444 s ---\n",
            "--- Iteration 7785: Training loss = 0.0083, 0.0445 s ---\n",
            "--- Iteration 7786: Training loss = 0.0095, 0.0463 s ---\n",
            "--- Iteration 7787: Training loss = 0.0102, 0.0436 s ---\n",
            "--- Iteration 7788: Training loss = 0.0088, 0.0426 s ---\n",
            "--- Iteration 7789: Training loss = 0.0068, 0.0431 s ---\n",
            "--- Iteration 7790: Training loss = 0.0095, 0.0444 s ---\n",
            "--- Iteration 7790: Test loss = 0.0538 ---\n",
            "\n",
            "--- Iteration 7791: Training loss = 0.0103, 0.0415 s ---\n",
            "--- Iteration 7792: Training loss = 0.0101, 0.0441 s ---\n",
            "--- Iteration 7793: Training loss = 0.0100, 0.0431 s ---\n",
            "--- Iteration 7794: Training loss = 0.0130, 0.0426 s ---\n",
            "--- Iteration 7795: Training loss = 0.0088, 0.0425 s ---\n",
            "--- Iteration 7796: Training loss = 0.0093, 0.0432 s ---\n",
            "--- Iteration 7797: Training loss = 0.0103, 0.0438 s ---\n",
            "--- Iteration 7798: Training loss = 0.0092, 0.0447 s ---\n",
            "--- Iteration 7799: Training loss = 0.0098, 0.0436 s ---\n",
            "--- Iteration 7800: Training loss = 0.0113, 0.0425 s ---\n",
            "--- Iteration 7800: Test loss = 0.0430 ---\n",
            "\n",
            "--- Iteration 7801: Training loss = 0.0126, 0.0410 s ---\n",
            "--- Iteration 7802: Training loss = 0.0118, 0.0434 s ---\n",
            "--- Iteration 7803: Training loss = 0.0065, 0.0429 s ---\n",
            "--- Iteration 7804: Training loss = 0.0092, 0.0429 s ---\n",
            "--- Iteration 7805: Training loss = 0.0083, 0.0431 s ---\n",
            "--- Iteration 7806: Training loss = 0.0155, 0.0474 s ---\n",
            "--- Iteration 7807: Training loss = 0.0079, 0.0447 s ---\n",
            "--- Iteration 7808: Training loss = 0.0120, 0.0417 s ---\n",
            "--- Iteration 7809: Training loss = 0.0150, 0.0423 s ---\n",
            "--- Iteration 7810: Training loss = 0.0092, 0.0425 s ---\n",
            "--- Iteration 7810: Test loss = 0.0434 ---\n",
            "\n",
            "--- Iteration 7811: Training loss = 0.0072, 0.0412 s ---\n",
            "--- Iteration 7812: Training loss = 0.0097, 0.0418 s ---\n",
            "--- Iteration 7813: Training loss = 0.0109, 0.0425 s ---\n",
            "--- Iteration 7814: Training loss = 0.0160, 0.0431 s ---\n",
            "--- Iteration 7815: Training loss = 0.0093, 0.0440 s ---\n",
            "--- Iteration 7816: Training loss = 0.0114, 0.0441 s ---\n",
            "--- Iteration 7817: Training loss = 0.0078, 0.0429 s ---\n",
            "--- Iteration 7818: Training loss = 0.0087, 0.0422 s ---\n",
            "--- Iteration 7819: Training loss = 0.0107, 0.0424 s ---\n",
            "--- Iteration 7820: Training loss = 0.0113, 0.0427 s ---\n",
            "--- Iteration 7820: Test loss = 0.0454 ---\n",
            "\n",
            "--- Iteration 7821: Training loss = 0.0056, 0.0414 s ---\n",
            "--- Iteration 7822: Training loss = 0.0097, 0.0432 s ---\n",
            "--- Iteration 7823: Training loss = 0.0088, 0.0434 s ---\n",
            "--- Iteration 7824: Training loss = 0.0056, 0.0448 s ---\n",
            "--- Iteration 7825: Training loss = 0.0098, 0.0445 s ---\n",
            "--- Iteration 7826: Training loss = 0.0072, 0.0475 s ---\n",
            "--- Iteration 7827: Training loss = 0.0172, 0.0422 s ---\n",
            "--- Iteration 7828: Training loss = 0.0096, 0.0477 s ---\n",
            "--- Iteration 7829: Training loss = 0.0102, 0.0441 s ---\n",
            "--- Iteration 7830: Training loss = 0.0095, 0.0428 s ---\n",
            "--- Iteration 7830: Test loss = 0.0580 ---\n",
            "\n",
            "--- Iteration 7831: Training loss = 0.0088, 0.0412 s ---\n",
            "--- Iteration 7832: Training loss = 0.0153, 0.0472 s ---\n",
            "--- Iteration 7833: Training loss = 0.0125, 0.0424 s ---\n",
            "--- Iteration 7834: Training loss = 0.0082, 0.0443 s ---\n",
            "--- Iteration 7835: Training loss = 0.0106, 0.0447 s ---\n",
            "--- Iteration 7836: Training loss = 0.0120, 0.0438 s ---\n",
            "--- Iteration 7837: Training loss = 0.0120, 0.0428 s ---\n",
            "--- Iteration 7838: Training loss = 0.0051, 0.0424 s ---\n",
            "--- Iteration 7839: Training loss = 0.0076, 0.0423 s ---\n",
            "--- Iteration 7840: Training loss = 0.0099, 0.0423 s ---\n",
            "--- Iteration 7840: Test loss = 0.0565 ---\n",
            "\n",
            "--- Iteration 7841: Training loss = 0.0100, 0.0413 s ---\n",
            "--- Iteration 7842: Training loss = 0.0094, 0.0435 s ---\n",
            "--- Iteration 7843: Training loss = 0.0105, 0.0428 s ---\n",
            "--- Iteration 7844: Training loss = 0.0089, 0.0439 s ---\n",
            "--- Iteration 7845: Training loss = 0.0135, 0.0447 s ---\n",
            "--- Iteration 7846: Training loss = 0.0079, 0.0439 s ---\n",
            "--- Iteration 7847: Training loss = 0.0107, 0.0435 s ---\n",
            "--- Iteration 7848: Training loss = 0.0096, 0.0445 s ---\n",
            "--- Iteration 7849: Training loss = 0.0118, 0.0443 s ---\n",
            "--- Iteration 7850: Training loss = 0.0090, 0.0455 s ---\n",
            "--- Iteration 7850: Test loss = 0.0251 ---\n",
            "\n",
            "--- Iteration 7851: Training loss = 0.0090, 0.0416 s ---\n",
            "--- Iteration 7852: Training loss = 0.0104, 0.0518 s ---\n",
            "--- Iteration 7853: Training loss = 0.0118, 0.0452 s ---\n",
            "--- Iteration 7854: Training loss = 0.0164, 0.0441 s ---\n",
            "--- Iteration 7855: Training loss = 0.0071, 0.0432 s ---\n",
            "--- Iteration 7856: Training loss = 0.0078, 0.0436 s ---\n",
            "--- Iteration 7857: Training loss = 0.0118, 0.0436 s ---\n",
            "--- Iteration 7858: Training loss = 0.0151, 0.0432 s ---\n",
            "--- Iteration 7859: Training loss = 0.0141, 0.0435 s ---\n",
            "--- Iteration 7860: Training loss = 0.0086, 0.0451 s ---\n",
            "--- Iteration 7860: Test loss = 0.0701 ---\n",
            "\n",
            "--- Iteration 7861: Training loss = 0.0098, 0.0424 s ---\n",
            "--- Iteration 7862: Training loss = 0.0130, 0.0461 s ---\n",
            "--- Iteration 7863: Training loss = 0.0133, 0.0425 s ---\n",
            "--- Iteration 7864: Training loss = 0.0146, 0.0432 s ---\n",
            "--- Iteration 7865: Training loss = 0.0101, 0.0434 s ---\n",
            "--- Iteration 7866: Training loss = 0.0076, 0.0461 s ---\n",
            "--- Iteration 7867: Training loss = 0.0111, 0.0441 s ---\n",
            "--- Iteration 7868: Training loss = 0.0097, 0.0417 s ---\n",
            "--- Iteration 7869: Training loss = 0.0109, 0.0440 s ---\n",
            "--- Iteration 7870: Training loss = 0.0068, 0.0430 s ---\n",
            "--- Iteration 7870: Test loss = 0.0453 ---\n",
            "\n",
            "--- Iteration 7871: Training loss = 0.0128, 0.0426 s ---\n",
            "--- Iteration 7872: Training loss = 0.0096, 0.0549 s ---\n",
            "--- Iteration 7873: Training loss = 0.0123, 0.0420 s ---\n",
            "--- Iteration 7874: Training loss = 0.0078, 0.0431 s ---\n",
            "--- Iteration 7875: Training loss = 0.0126, 0.0434 s ---\n",
            "--- Iteration 7876: Training loss = 0.0123, 0.0442 s ---\n",
            "--- Iteration 7877: Training loss = 0.0113, 0.0440 s ---\n",
            "--- Iteration 7878: Training loss = 0.0087, 0.0427 s ---\n",
            "--- Iteration 7879: Training loss = 0.0084, 0.0419 s ---\n",
            "--- Iteration 7880: Training loss = 0.0091, 0.0429 s ---\n",
            "--- Iteration 7880: Test loss = 0.0618 ---\n",
            "\n",
            "--- Iteration 7881: Training loss = 0.0118, 0.0408 s ---\n",
            "--- Iteration 7882: Training loss = 0.0120, 0.0423 s ---\n",
            "--- Iteration 7883: Training loss = 0.0132, 0.0424 s ---\n",
            "--- Iteration 7884: Training loss = 0.0068, 0.0428 s ---\n",
            "--- Iteration 7885: Training loss = 0.0126, 0.0426 s ---\n",
            "--- Iteration 7886: Training loss = 0.0131, 0.0444 s ---\n",
            "--- Iteration 7887: Training loss = 0.0146, 0.0436 s ---\n",
            "--- Iteration 7888: Training loss = 0.0129, 0.0425 s ---\n",
            "--- Iteration 7889: Training loss = 0.0126, 0.0445 s ---\n",
            "--- Iteration 7890: Training loss = 0.0103, 0.0421 s ---\n",
            "--- Iteration 7890: Test loss = 0.0237 ---\n",
            "\n",
            "--- Iteration 7891: Training loss = 0.0103, 0.0419 s ---\n",
            "--- Iteration 7892: Training loss = 0.0088, 0.0430 s ---\n",
            "--- Iteration 7893: Training loss = 0.0085, 0.0438 s ---\n",
            "--- Iteration 7894: Training loss = 0.0099, 0.0438 s ---\n",
            "--- Iteration 7895: Training loss = 0.0121, 0.0440 s ---\n",
            "--- Iteration 7896: Training loss = 0.0118, 0.0433 s ---\n",
            "--- Iteration 7897: Training loss = 0.0074, 0.0418 s ---\n",
            "--- Iteration 7898: Training loss = 0.0122, 0.0426 s ---\n",
            "--- Iteration 7899: Training loss = 0.0095, 0.0426 s ---\n",
            "--- Iteration 7900: Training loss = 0.0092, 0.0424 s ---\n",
            "--- Iteration 7900: Test loss = 0.0338 ---\n",
            "\n",
            "--- Iteration 7901: Training loss = 0.0081, 0.0411 s ---\n",
            "--- Iteration 7902: Training loss = 0.0139, 0.0431 s ---\n",
            "--- Iteration 7903: Training loss = 0.0134, 0.0430 s ---\n",
            "--- Iteration 7904: Training loss = 0.0088, 0.0447 s ---\n",
            "--- Iteration 7905: Training loss = 0.0097, 0.0442 s ---\n",
            "--- Iteration 7906: Training loss = 0.0123, 0.0426 s ---\n",
            "--- Iteration 7907: Training loss = 0.0091, 0.0423 s ---\n",
            "--- Iteration 7908: Training loss = 0.0104, 0.0427 s ---\n",
            "--- Iteration 7909: Training loss = 0.0079, 0.0423 s ---\n",
            "--- Iteration 7910: Training loss = 0.0096, 0.0431 s ---\n",
            "--- Iteration 7910: Test loss = 0.0422 ---\n",
            "\n",
            "--- Iteration 7911: Training loss = 0.0066, 0.0421 s ---\n",
            "--- Iteration 7912: Training loss = 0.0094, 0.0428 s ---\n",
            "--- Iteration 7913: Training loss = 0.0076, 0.0453 s ---\n",
            "--- Iteration 7914: Training loss = 0.0110, 0.0434 s ---\n",
            "--- Iteration 7915: Training loss = 0.0095, 0.0427 s ---\n",
            "--- Iteration 7916: Training loss = 0.0099, 0.0428 s ---\n",
            "--- Iteration 7917: Training loss = 0.0092, 0.0424 s ---\n",
            "--- Iteration 7918: Training loss = 0.0129, 0.0416 s ---\n",
            "--- Iteration 7919: Training loss = 0.0095, 0.0429 s ---\n",
            "--- Iteration 7920: Training loss = 0.0092, 0.0430 s ---\n",
            "--- Iteration 7920: Test loss = 0.0283 ---\n",
            "\n",
            "--- Iteration 7921: Training loss = 0.0084, 0.0432 s ---\n",
            "--- Iteration 7922: Training loss = 0.0071, 0.0446 s ---\n",
            "--- Iteration 7923: Training loss = 0.0133, 0.0432 s ---\n",
            "--- Iteration 7924: Training loss = 0.0084, 0.0422 s ---\n",
            "--- Iteration 7925: Training loss = 0.0085, 0.0425 s ---\n",
            "--- Iteration 7926: Training loss = 0.0066, 0.0420 s ---\n",
            "--- Iteration 7927: Training loss = 0.0066, 0.0424 s ---\n",
            "--- Iteration 7928: Training loss = 0.0084, 0.0425 s ---\n",
            "--- Iteration 7929: Training loss = 0.0135, 0.0430 s ---\n",
            "--- Iteration 7930: Training loss = 0.0126, 0.0452 s ---\n",
            "--- Iteration 7930: Test loss = 0.0491 ---\n",
            "\n",
            "--- Iteration 7931: Training loss = 0.0080, 0.0424 s ---\n",
            "--- Iteration 7932: Training loss = 0.0082, 0.0460 s ---\n",
            "--- Iteration 7933: Training loss = 0.0102, 0.0420 s ---\n",
            "--- Iteration 7934: Training loss = 0.0086, 0.0425 s ---\n",
            "--- Iteration 7935: Training loss = 0.0157, 0.0423 s ---\n",
            "--- Iteration 7936: Training loss = 0.0151, 0.0426 s ---\n",
            "--- Iteration 7937: Training loss = 0.0075, 0.0430 s ---\n",
            "--- Iteration 7938: Training loss = 0.0116, 0.0456 s ---\n",
            "--- Iteration 7939: Training loss = 0.0065, 0.0433 s ---\n",
            "--- Iteration 7940: Training loss = 0.0120, 0.0475 s ---\n",
            "--- Iteration 7940: Test loss = 0.0563 ---\n",
            "\n",
            "--- Iteration 7941: Training loss = 0.0116, 0.0444 s ---\n",
            "--- Iteration 7942: Training loss = 0.0086, 0.0478 s ---\n",
            "--- Iteration 7943: Training loss = 0.0121, 0.0434 s ---\n",
            "--- Iteration 7944: Training loss = 0.0125, 0.0426 s ---\n",
            "--- Iteration 7945: Training loss = 0.0098, 0.0418 s ---\n",
            "--- Iteration 7946: Training loss = 0.0075, 0.0418 s ---\n",
            "--- Iteration 7947: Training loss = 0.0122, 0.0429 s ---\n",
            "--- Iteration 7948: Training loss = 0.0112, 0.0434 s ---\n",
            "--- Iteration 7949: Training loss = 0.0131, 0.0422 s ---\n",
            "--- Iteration 7950: Training loss = 0.0119, 0.0433 s ---\n",
            "--- Iteration 7950: Test loss = 0.0245 ---\n",
            "\n",
            "--- Iteration 7951: Training loss = 0.0105, 0.0431 s ---\n",
            "--- Iteration 7952: Training loss = 0.0133, 0.0467 s ---\n",
            "--- Iteration 7953: Training loss = 0.0069, 0.0427 s ---\n",
            "--- Iteration 7954: Training loss = 0.0129, 0.0443 s ---\n",
            "--- Iteration 7955: Training loss = 0.0137, 0.0489 s ---\n",
            "--- Iteration 7956: Training loss = 0.0068, 0.0453 s ---\n",
            "--- Iteration 7957: Training loss = 0.0113, 0.0442 s ---\n",
            "--- Iteration 7958: Training loss = 0.0104, 0.0486 s ---\n",
            "--- Iteration 7959: Training loss = 0.0066, 0.0450 s ---\n",
            "--- Iteration 7960: Training loss = 0.0079, 0.0446 s ---\n",
            "--- Iteration 7960: Test loss = 0.0245 ---\n",
            "\n",
            "--- Iteration 7961: Training loss = 0.0095, 0.0437 s ---\n",
            "--- Iteration 7962: Training loss = 0.0089, 0.0492 s ---\n",
            "--- Iteration 7963: Training loss = 0.0067, 0.0456 s ---\n",
            "--- Iteration 7964: Training loss = 0.0089, 0.0446 s ---\n",
            "--- Iteration 7965: Training loss = 0.0067, 0.0425 s ---\n",
            "--- Iteration 7966: Training loss = 0.0107, 0.0441 s ---\n",
            "--- Iteration 7967: Training loss = 0.0070, 0.0425 s ---\n",
            "--- Iteration 7968: Training loss = 0.0114, 0.0427 s ---\n",
            "--- Iteration 7969: Training loss = 0.0110, 0.0438 s ---\n",
            "--- Iteration 7970: Training loss = 0.0152, 0.0440 s ---\n",
            "--- Iteration 7970: Test loss = 0.0538 ---\n",
            "\n",
            "--- Iteration 7971: Training loss = 0.0109, 0.0435 s ---\n",
            "--- Iteration 7972: Training loss = 0.0135, 0.0432 s ---\n",
            "--- Iteration 7973: Training loss = 0.0094, 0.0430 s ---\n",
            "--- Iteration 7974: Training loss = 0.0119, 0.0435 s ---\n",
            "--- Iteration 7975: Training loss = 0.0086, 0.0437 s ---\n",
            "--- Iteration 7976: Training loss = 0.0137, 0.0438 s ---\n",
            "--- Iteration 7977: Training loss = 0.0102, 0.0457 s ---\n",
            "--- Iteration 7978: Training loss = 0.0078, 0.0434 s ---\n",
            "--- Iteration 7979: Training loss = 0.0066, 0.0428 s ---\n",
            "--- Iteration 7980: Training loss = 0.0140, 0.0434 s ---\n",
            "--- Iteration 7980: Test loss = 0.0483 ---\n",
            "\n",
            "--- Iteration 7981: Training loss = 0.0049, 0.0444 s ---\n",
            "--- Iteration 7982: Training loss = 0.0115, 0.0437 s ---\n",
            "--- Iteration 7983: Training loss = 0.0085, 0.0451 s ---\n",
            "--- Iteration 7984: Training loss = 0.0111, 0.0450 s ---\n",
            "--- Iteration 7985: Training loss = 0.0146, 0.0421 s ---\n",
            "--- Iteration 7986: Training loss = 0.0110, 0.0430 s ---\n",
            "--- Iteration 7987: Training loss = 0.0056, 0.0447 s ---\n",
            "--- Iteration 7988: Training loss = 0.0098, 0.0454 s ---\n",
            "--- Iteration 7989: Training loss = 0.0113, 0.0442 s ---\n",
            "--- Iteration 7990: Training loss = 0.0075, 0.0433 s ---\n",
            "--- Iteration 7990: Test loss = 0.0496 ---\n",
            "\n",
            "--- Iteration 7991: Training loss = 0.0084, 0.0477 s ---\n",
            "--- Iteration 7992: Training loss = 0.0085, 0.0437 s ---\n",
            "--- Iteration 7993: Training loss = 0.0049, 0.0463 s ---\n",
            "--- Iteration 7994: Training loss = 0.0115, 0.0429 s ---\n",
            "--- Iteration 7995: Training loss = 0.0125, 0.0423 s ---\n",
            "--- Iteration 7996: Training loss = 0.0100, 0.0504 s ---\n",
            "--- Iteration 7997: Training loss = 0.0100, 0.0432 s ---\n",
            "--- Iteration 7998: Training loss = 0.0098, 0.0448 s ---\n",
            "--- Iteration 7999: Training loss = 0.0111, 0.0433 s ---\n",
            "--- Iteration 8000: Training loss = 0.0118, 0.0428 s ---\n",
            "--- Iteration 8000: Test loss = 0.0388 ---\n",
            "\n",
            "--- Iteration 8001: Training loss = 0.0094, 0.0455 s ---\n",
            "--- Iteration 8002: Training loss = 0.0054, 0.0497 s ---\n",
            "--- Iteration 8003: Training loss = 0.0047, 0.0501 s ---\n",
            "--- Iteration 8004: Training loss = 0.0106, 0.0436 s ---\n",
            "--- Iteration 8005: Training loss = 0.0141, 0.0443 s ---\n",
            "--- Iteration 8006: Training loss = 0.0109, 0.0439 s ---\n",
            "--- Iteration 8007: Training loss = 0.0133, 0.0431 s ---\n",
            "--- Iteration 8008: Training loss = 0.0073, 0.0424 s ---\n",
            "--- Iteration 8009: Training loss = 0.0059, 0.0429 s ---\n",
            "--- Iteration 8010: Training loss = 0.0124, 0.0428 s ---\n",
            "--- Iteration 8010: Test loss = 0.0265 ---\n",
            "\n",
            "--- Iteration 8011: Training loss = 0.0076, 0.0427 s ---\n",
            "--- Iteration 8012: Training loss = 0.0105, 0.0431 s ---\n",
            "--- Iteration 8013: Training loss = 0.0126, 0.0435 s ---\n",
            "--- Iteration 8014: Training loss = 0.0115, 0.0450 s ---\n",
            "--- Iteration 8015: Training loss = 0.0142, 0.0439 s ---\n",
            "--- Iteration 8016: Training loss = 0.0068, 0.0426 s ---\n",
            "--- Iteration 8017: Training loss = 0.0090, 0.0421 s ---\n",
            "--- Iteration 8018: Training loss = 0.0075, 0.0435 s ---\n",
            "--- Iteration 8019: Training loss = 0.0090, 0.0439 s ---\n",
            "--- Iteration 8020: Training loss = 0.0114, 0.0442 s ---\n",
            "--- Iteration 8020: Test loss = 0.0707 ---\n",
            "\n",
            "--- Iteration 8021: Training loss = 0.0082, 0.0473 s ---\n",
            "--- Iteration 8022: Training loss = 0.0105, 0.0433 s ---\n",
            "--- Iteration 8023: Training loss = 0.0068, 0.0436 s ---\n",
            "--- Iteration 8024: Training loss = 0.0109, 0.0448 s ---\n",
            "--- Iteration 8025: Training loss = 0.0115, 0.0446 s ---\n",
            "--- Iteration 8026: Training loss = 0.0117, 0.0433 s ---\n",
            "--- Iteration 8027: Training loss = 0.0118, 0.0440 s ---\n",
            "--- Iteration 8028: Training loss = 0.0101, 0.0450 s ---\n",
            "--- Iteration 8029: Training loss = 0.0106, 0.0440 s ---\n",
            "--- Iteration 8030: Training loss = 0.0101, 0.0441 s ---\n",
            "--- Iteration 8030: Test loss = 0.0334 ---\n",
            "\n",
            "--- Iteration 8031: Training loss = 0.0140, 0.0481 s ---\n",
            "--- Iteration 8032: Training loss = 0.0108, 0.0426 s ---\n",
            "--- Iteration 8033: Training loss = 0.0110, 0.0430 s ---\n",
            "--- Iteration 8034: Training loss = 0.0129, 0.0438 s ---\n",
            "--- Iteration 8035: Training loss = 0.0074, 0.0449 s ---\n",
            "--- Iteration 8036: Training loss = 0.0065, 0.0444 s ---\n",
            "--- Iteration 8037: Training loss = 0.0082, 0.0423 s ---\n",
            "--- Iteration 8038: Training loss = 0.0094, 0.0424 s ---\n",
            "--- Iteration 8039: Training loss = 0.0078, 0.0423 s ---\n",
            "--- Iteration 8040: Training loss = 0.0131, 0.0428 s ---\n",
            "--- Iteration 8040: Test loss = 0.0551 ---\n",
            "\n",
            "--- Iteration 8041: Training loss = 0.0090, 0.0415 s ---\n",
            "--- Iteration 8042: Training loss = 0.0141, 0.0430 s ---\n",
            "--- Iteration 8043: Training loss = 0.0089, 0.0434 s ---\n",
            "--- Iteration 8044: Training loss = 0.0104, 0.0448 s ---\n",
            "--- Iteration 8045: Training loss = 0.0088, 0.0438 s ---\n",
            "--- Iteration 8046: Training loss = 0.0114, 0.0426 s ---\n",
            "--- Iteration 8047: Training loss = 0.0085, 0.0423 s ---\n",
            "--- Iteration 8048: Training loss = 0.0087, 0.0431 s ---\n",
            "--- Iteration 8049: Training loss = 0.0146, 0.0437 s ---\n",
            "--- Iteration 8050: Training loss = 0.0057, 0.0437 s ---\n",
            "--- Iteration 8050: Test loss = 0.0316 ---\n",
            "\n",
            "--- Iteration 8051: Training loss = 0.0113, 0.0445 s ---\n",
            "--- Iteration 8052: Training loss = 0.0064, 0.0433 s ---\n",
            "--- Iteration 8053: Training loss = 0.0102, 0.0433 s ---\n",
            "--- Iteration 8054: Training loss = 0.0097, 0.0422 s ---\n",
            "--- Iteration 8055: Training loss = 0.0199, 0.0436 s ---\n",
            "--- Iteration 8056: Training loss = 0.0119, 0.0429 s ---\n",
            "--- Iteration 8057: Training loss = 0.0117, 0.0432 s ---\n",
            "--- Iteration 8058: Training loss = 0.0071, 0.0451 s ---\n",
            "--- Iteration 8059: Training loss = 0.0094, 0.0438 s ---\n",
            "--- Iteration 8060: Training loss = 0.0089, 0.0435 s ---\n",
            "--- Iteration 8060: Test loss = 0.0189 ---\n",
            "\n",
            "--- Iteration 8061: Training loss = 0.0080, 0.0454 s ---\n",
            "--- Iteration 8062: Training loss = 0.0076, 0.0420 s ---\n",
            "--- Iteration 8063: Training loss = 0.0089, 0.0426 s ---\n",
            "--- Iteration 8064: Training loss = 0.0136, 0.0435 s ---\n",
            "--- Iteration 8065: Training loss = 0.0100, 0.0449 s ---\n",
            "--- Iteration 8066: Training loss = 0.0118, 0.0436 s ---\n",
            "--- Iteration 8067: Training loss = 0.0076, 0.0433 s ---\n",
            "--- Iteration 8068: Training loss = 0.0118, 0.0426 s ---\n",
            "--- Iteration 8069: Training loss = 0.0108, 0.0426 s ---\n",
            "--- Iteration 8070: Training loss = 0.0080, 0.0430 s ---\n",
            "--- Iteration 8070: Test loss = 0.0407 ---\n",
            "\n",
            "--- Iteration 8071: Training loss = 0.0097, 0.0419 s ---\n",
            "--- Iteration 8072: Training loss = 0.0106, 0.0428 s ---\n",
            "--- Iteration 8073: Training loss = 0.0069, 0.0431 s ---\n",
            "--- Iteration 8074: Training loss = 0.0124, 0.0452 s ---\n",
            "--- Iteration 8075: Training loss = 0.0061, 0.0438 s ---\n",
            "--- Iteration 8076: Training loss = 0.0091, 0.0428 s ---\n",
            "--- Iteration 8077: Training loss = 0.0084, 0.0426 s ---\n",
            "--- Iteration 8078: Training loss = 0.0063, 0.0437 s ---\n",
            "--- Iteration 8079: Training loss = 0.0134, 0.0441 s ---\n",
            "--- Iteration 8080: Training loss = 0.0106, 0.0443 s ---\n",
            "--- Iteration 8080: Test loss = 0.0811 ---\n",
            "\n",
            "--- Iteration 8081: Training loss = 0.0093, 0.0464 s ---\n",
            "--- Iteration 8082: Training loss = 0.0105, 0.0414 s ---\n",
            "--- Iteration 8083: Training loss = 0.0103, 0.0424 s ---\n",
            "--- Iteration 8084: Training loss = 0.0084, 0.0424 s ---\n",
            "--- Iteration 8085: Training loss = 0.0138, 0.0429 s ---\n",
            "--- Iteration 8086: Training loss = 0.0096, 0.0439 s ---\n",
            "--- Iteration 8087: Training loss = 0.0106, 0.0452 s ---\n",
            "--- Iteration 8088: Training loss = 0.0104, 0.0437 s ---\n",
            "--- Iteration 8089: Training loss = 0.0061, 0.0430 s ---\n",
            "--- Iteration 8090: Training loss = 0.0072, 0.0432 s ---\n",
            "--- Iteration 8090: Test loss = 0.0386 ---\n",
            "\n",
            "--- Iteration 8091: Training loss = 0.0092, 0.0439 s ---\n",
            "--- Iteration 8092: Training loss = 0.0093, 0.0451 s ---\n",
            "--- Iteration 8093: Training loss = 0.0098, 0.0451 s ---\n",
            "--- Iteration 8094: Training loss = 0.0092, 0.0444 s ---\n",
            "--- Iteration 8095: Training loss = 0.0124, 0.0428 s ---\n",
            "--- Iteration 8096: Training loss = 0.0075, 0.0420 s ---\n",
            "--- Iteration 8097: Training loss = 0.0122, 0.0427 s ---\n",
            "--- Iteration 8098: Training loss = 0.0081, 0.0432 s ---\n",
            "--- Iteration 8099: Training loss = 0.0103, 0.0444 s ---\n",
            "--- Iteration 8100: Training loss = 0.0115, 0.0442 s ---\n",
            "--- Iteration 8100: Test loss = 0.0487 ---\n",
            "\n",
            "--- Iteration 8101: Training loss = 0.0079, 0.0454 s ---\n",
            "--- Iteration 8102: Training loss = 0.0064, 0.0434 s ---\n",
            "--- Iteration 8103: Training loss = 0.0141, 0.0427 s ---\n",
            "--- Iteration 8104: Training loss = 0.0144, 0.0426 s ---\n",
            "--- Iteration 8105: Training loss = 0.0072, 0.0431 s ---\n",
            "--- Iteration 8106: Training loss = 0.0153, 0.0456 s ---\n",
            "--- Iteration 8107: Training loss = 0.0095, 0.0432 s ---\n",
            "--- Iteration 8108: Training loss = 0.0088, 0.0423 s ---\n",
            "--- Iteration 8109: Training loss = 0.0048, 0.0426 s ---\n",
            "--- Iteration 8110: Training loss = 0.0128, 0.0428 s ---\n",
            "--- Iteration 8110: Test loss = 0.0444 ---\n",
            "\n",
            "--- Iteration 8111: Training loss = 0.0103, 0.0480 s ---\n",
            "--- Iteration 8112: Training loss = 0.0114, 0.0438 s ---\n",
            "--- Iteration 8113: Training loss = 0.0122, 0.0445 s ---\n",
            "--- Iteration 8114: Training loss = 0.0053, 0.0429 s ---\n",
            "--- Iteration 8115: Training loss = 0.0084, 0.0434 s ---\n",
            "--- Iteration 8116: Training loss = 0.0082, 0.0425 s ---\n",
            "--- Iteration 8117: Training loss = 0.0080, 0.0491 s ---\n",
            "--- Iteration 8118: Training loss = 0.0080, 0.0442 s ---\n",
            "--- Iteration 8119: Training loss = 0.0078, 0.0427 s ---\n",
            "--- Iteration 8120: Training loss = 0.0089, 0.0432 s ---\n",
            "--- Iteration 8120: Test loss = 0.0374 ---\n",
            "\n",
            "--- Iteration 8121: Training loss = 0.0104, 0.0419 s ---\n",
            "--- Iteration 8122: Training loss = 0.0065, 0.0427 s ---\n",
            "--- Iteration 8123: Training loss = 0.0092, 0.0427 s ---\n",
            "--- Iteration 8124: Training loss = 0.0044, 0.0420 s ---\n",
            "--- Iteration 8125: Training loss = 0.0060, 0.0434 s ---\n",
            "--- Iteration 8126: Training loss = 0.0063, 0.0455 s ---\n",
            "--- Iteration 8127: Training loss = 0.0099, 0.0429 s ---\n",
            "--- Iteration 8128: Training loss = 0.0075, 0.0430 s ---\n",
            "--- Iteration 8129: Training loss = 0.0090, 0.0423 s ---\n",
            "--- Iteration 8130: Training loss = 0.0124, 0.0425 s ---\n",
            "--- Iteration 8130: Test loss = 0.0497 ---\n",
            "\n",
            "--- Iteration 8131: Training loss = 0.0085, 0.0453 s ---\n",
            "--- Iteration 8132: Training loss = 0.0097, 0.0434 s ---\n",
            "--- Iteration 8133: Training loss = 0.0109, 0.0451 s ---\n",
            "--- Iteration 8134: Training loss = 0.0075, 0.0438 s ---\n",
            "--- Iteration 8135: Training loss = 0.0076, 0.0427 s ---\n",
            "--- Iteration 8136: Training loss = 0.0082, 0.0429 s ---\n",
            "--- Iteration 8137: Training loss = 0.0098, 0.0441 s ---\n",
            "--- Iteration 8138: Training loss = 0.0108, 0.0432 s ---\n",
            "--- Iteration 8139: Training loss = 0.0127, 0.0447 s ---\n",
            "--- Iteration 8140: Training loss = 0.0092, 0.0437 s ---\n",
            "--- Iteration 8140: Test loss = 0.0361 ---\n",
            "\n",
            "--- Iteration 8141: Training loss = 0.0062, 0.0431 s ---\n",
            "--- Iteration 8142: Training loss = 0.0095, 0.0424 s ---\n",
            "--- Iteration 8143: Training loss = 0.0093, 0.0426 s ---\n",
            "--- Iteration 8144: Training loss = 0.0114, 0.0423 s ---\n",
            "--- Iteration 8145: Training loss = 0.0089, 0.0431 s ---\n",
            "--- Iteration 8146: Training loss = 0.0067, 0.0446 s ---\n",
            "--- Iteration 8147: Training loss = 0.0084, 0.0447 s ---\n",
            "--- Iteration 8148: Training loss = 0.0086, 0.0454 s ---\n",
            "--- Iteration 8149: Training loss = 0.0067, 0.0425 s ---\n",
            "--- Iteration 8150: Training loss = 0.0109, 0.0431 s ---\n",
            "--- Iteration 8150: Test loss = 0.0457 ---\n",
            "\n",
            "--- Iteration 8151: Training loss = 0.0130, 0.0450 s ---\n",
            "--- Iteration 8152: Training loss = 0.0075, 0.0432 s ---\n",
            "--- Iteration 8153: Training loss = 0.0118, 0.0453 s ---\n",
            "--- Iteration 8154: Training loss = 0.0104, 0.0433 s ---\n",
            "--- Iteration 8155: Training loss = 0.0088, 0.0431 s ---\n",
            "--- Iteration 8156: Training loss = 0.0142, 0.0424 s ---\n",
            "--- Iteration 8157: Training loss = 0.0110, 0.0425 s ---\n",
            "--- Iteration 8158: Training loss = 0.0123, 0.0430 s ---\n",
            "--- Iteration 8159: Training loss = 0.0075, 0.0433 s ---\n",
            "--- Iteration 8160: Training loss = 0.0123, 0.0446 s ---\n",
            "--- Iteration 8160: Test loss = 0.0191 ---\n",
            "\n",
            "--- Iteration 8161: Training loss = 0.0087, 0.0446 s ---\n",
            "--- Iteration 8162: Training loss = 0.0091, 0.0421 s ---\n",
            "--- Iteration 8163: Training loss = 0.0103, 0.0423 s ---\n",
            "--- Iteration 8164: Training loss = 0.0087, 0.0421 s ---\n",
            "--- Iteration 8165: Training loss = 0.0078, 0.0427 s ---\n",
            "--- Iteration 8166: Training loss = 0.0070, 0.0432 s ---\n",
            "--- Iteration 8167: Training loss = 0.0113, 0.0453 s ---\n",
            "--- Iteration 8168: Training loss = 0.0078, 0.0441 s ---\n",
            "--- Iteration 8169: Training loss = 0.0096, 0.0431 s ---\n",
            "--- Iteration 8170: Training loss = 0.0092, 0.0424 s ---\n",
            "--- Iteration 8170: Test loss = 0.0369 ---\n",
            "\n",
            "--- Iteration 8171: Training loss = 0.0121, 0.0432 s ---\n",
            "--- Iteration 8172: Training loss = 0.0074, 0.0432 s ---\n",
            "--- Iteration 8173: Training loss = 0.0067, 0.0432 s ---\n",
            "--- Iteration 8174: Training loss = 0.0090, 0.0437 s ---\n",
            "--- Iteration 8175: Training loss = 0.0117, 0.0449 s ---\n",
            "--- Iteration 8176: Training loss = 0.0067, 0.0438 s ---\n",
            "--- Iteration 8177: Training loss = 0.0127, 0.0437 s ---\n",
            "--- Iteration 8178: Training loss = 0.0075, 0.0427 s ---\n",
            "--- Iteration 8179: Training loss = 0.0077, 0.0433 s ---\n",
            "--- Iteration 8180: Training loss = 0.0078, 0.0431 s ---\n",
            "--- Iteration 8180: Test loss = 0.0340 ---\n",
            "\n",
            "--- Iteration 8181: Training loss = 0.0056, 0.0433 s ---\n",
            "--- Iteration 8182: Training loss = 0.0085, 0.0453 s ---\n",
            "--- Iteration 8183: Training loss = 0.0085, 0.0437 s ---\n",
            "--- Iteration 8184: Training loss = 0.0118, 0.0499 s ---\n",
            "--- Iteration 8185: Training loss = 0.0073, 0.0432 s ---\n",
            "--- Iteration 8186: Training loss = 0.0065, 0.0440 s ---\n",
            "--- Iteration 8187: Training loss = 0.0073, 0.0445 s ---\n",
            "--- Iteration 8188: Training loss = 0.0088, 0.0423 s ---\n",
            "--- Iteration 8189: Training loss = 0.0056, 0.0422 s ---\n",
            "--- Iteration 8190: Training loss = 0.0118, 0.0427 s ---\n",
            "--- Iteration 8190: Test loss = 0.0536 ---\n",
            "\n",
            "--- Iteration 8191: Training loss = 0.0069, 0.0416 s ---\n",
            "--- Iteration 8192: Training loss = 0.0111, 0.0430 s ---\n",
            "--- Iteration 8193: Training loss = 0.0065, 0.0434 s ---\n",
            "--- Iteration 8194: Training loss = 0.0098, 0.0435 s ---\n",
            "--- Iteration 8195: Training loss = 0.0090, 0.0439 s ---\n",
            "--- Iteration 8196: Training loss = 0.0106, 0.0472 s ---\n",
            "--- Iteration 8197: Training loss = 0.0094, 0.0429 s ---\n",
            "--- Iteration 8198: Training loss = 0.0091, 0.0424 s ---\n",
            "--- Iteration 8199: Training loss = 0.0100, 0.0436 s ---\n",
            "--- Iteration 8200: Training loss = 0.0073, 0.0443 s ---\n",
            "--- Iteration 8200: Test loss = 0.0388 ---\n",
            "\n",
            "--- Iteration 8201: Training loss = 0.0076, 0.0439 s ---\n",
            "--- Iteration 8202: Training loss = 0.0097, 0.0434 s ---\n",
            "--- Iteration 8203: Training loss = 0.0084, 0.0429 s ---\n",
            "--- Iteration 8204: Training loss = 0.0094, 0.0428 s ---\n",
            "--- Iteration 8205: Training loss = 0.0112, 0.0427 s ---\n",
            "--- Iteration 8206: Training loss = 0.0058, 0.0429 s ---\n",
            "--- Iteration 8207: Training loss = 0.0103, 0.0430 s ---\n",
            "--- Iteration 8208: Training loss = 0.0098, 0.0439 s ---\n",
            "--- Iteration 8209: Training loss = 0.0116, 0.0451 s ---\n",
            "--- Iteration 8210: Training loss = 0.0103, 0.0430 s ---\n",
            "--- Iteration 8210: Test loss = 0.0444 ---\n",
            "\n",
            "--- Iteration 8211: Training loss = 0.0124, 0.0450 s ---\n",
            "--- Iteration 8212: Training loss = 0.0088, 0.0426 s ---\n",
            "--- Iteration 8213: Training loss = 0.0098, 0.0429 s ---\n",
            "--- Iteration 8214: Training loss = 0.0127, 0.0429 s ---\n",
            "--- Iteration 8215: Training loss = 0.0112, 0.0446 s ---\n",
            "--- Iteration 8216: Training loss = 0.0079, 0.0441 s ---\n",
            "--- Iteration 8217: Training loss = 0.0097, 0.0421 s ---\n",
            "--- Iteration 8218: Training loss = 0.0047, 0.0468 s ---\n",
            "--- Iteration 8219: Training loss = 0.0080, 0.0472 s ---\n",
            "--- Iteration 8220: Training loss = 0.0110, 0.0460 s ---\n",
            "--- Iteration 8220: Test loss = 0.0456 ---\n",
            "\n",
            "--- Iteration 8221: Training loss = 0.0136, 0.0459 s ---\n",
            "--- Iteration 8222: Training loss = 0.0135, 0.0471 s ---\n",
            "--- Iteration 8223: Training loss = 0.0074, 0.0463 s ---\n",
            "--- Iteration 8224: Training loss = 0.0053, 0.0474 s ---\n",
            "--- Iteration 8225: Training loss = 0.0134, 0.0434 s ---\n",
            "--- Iteration 8226: Training loss = 0.0137, 0.0427 s ---\n",
            "--- Iteration 8227: Training loss = 0.0098, 0.0425 s ---\n",
            "--- Iteration 8228: Training loss = 0.0078, 0.0425 s ---\n",
            "--- Iteration 8229: Training loss = 0.0107, 0.0430 s ---\n",
            "--- Iteration 8230: Training loss = 0.0072, 0.0430 s ---\n",
            "--- Iteration 8230: Test loss = 0.0577 ---\n",
            "\n",
            "--- Iteration 8231: Training loss = 0.0115, 0.0432 s ---\n",
            "--- Iteration 8232: Training loss = 0.0076, 0.0444 s ---\n",
            "--- Iteration 8233: Training loss = 0.0079, 0.0435 s ---\n",
            "--- Iteration 8234: Training loss = 0.0111, 0.0427 s ---\n",
            "--- Iteration 8235: Training loss = 0.0075, 0.0437 s ---\n",
            "--- Iteration 8236: Training loss = 0.0060, 0.0417 s ---\n",
            "--- Iteration 8237: Training loss = 0.0147, 0.0432 s ---\n",
            "--- Iteration 8238: Training loss = 0.0122, 0.0440 s ---\n",
            "--- Iteration 8239: Training loss = 0.0082, 0.0436 s ---\n",
            "--- Iteration 8240: Training loss = 0.0088, 0.0437 s ---\n",
            "--- Iteration 8240: Test loss = 0.0234 ---\n",
            "\n",
            "--- Iteration 8241: Training loss = 0.0129, 0.0407 s ---\n",
            "--- Iteration 8242: Training loss = 0.0075, 0.0446 s ---\n",
            "--- Iteration 8243: Training loss = 0.0117, 0.0426 s ---\n",
            "--- Iteration 8244: Training loss = 0.0088, 0.0439 s ---\n",
            "--- Iteration 8245: Training loss = 0.0101, 0.0436 s ---\n",
            "--- Iteration 8246: Training loss = 0.0127, 0.0441 s ---\n",
            "--- Iteration 8247: Training loss = 0.0126, 0.0439 s ---\n",
            "--- Iteration 8248: Training loss = 0.0124, 0.0427 s ---\n",
            "--- Iteration 8249: Training loss = 0.0101, 0.0425 s ---\n",
            "--- Iteration 8250: Training loss = 0.0141, 0.0421 s ---\n",
            "--- Iteration 8250: Test loss = 0.0445 ---\n",
            "\n",
            "--- Iteration 8251: Training loss = 0.0055, 0.0450 s ---\n",
            "--- Iteration 8252: Training loss = 0.0077, 0.0448 s ---\n",
            "--- Iteration 8253: Training loss = 0.0121, 0.0436 s ---\n",
            "--- Iteration 8254: Training loss = 0.0086, 0.0429 s ---\n",
            "--- Iteration 8255: Training loss = 0.0111, 0.0486 s ---\n",
            "--- Iteration 8256: Training loss = 0.0059, 0.0432 s ---\n",
            "--- Iteration 8257: Training loss = 0.0103, 0.0440 s ---\n",
            "--- Iteration 8258: Training loss = 0.0080, 0.0428 s ---\n",
            "--- Iteration 8259: Training loss = 0.0078, 0.0425 s ---\n",
            "--- Iteration 8260: Training loss = 0.0136, 0.0423 s ---\n",
            "--- Iteration 8260: Test loss = 0.0215 ---\n",
            "\n",
            "--- Iteration 8261: Training loss = 0.0127, 0.0404 s ---\n",
            "--- Iteration 8262: Training loss = 0.0079, 0.0429 s ---\n",
            "--- Iteration 8263: Training loss = 0.0090, 0.0418 s ---\n",
            "--- Iteration 8264: Training loss = 0.0086, 0.0426 s ---\n",
            "--- Iteration 8265: Training loss = 0.0154, 0.0424 s ---\n",
            "--- Iteration 8266: Training loss = 0.0123, 0.0440 s ---\n",
            "--- Iteration 8267: Training loss = 0.0097, 0.0445 s ---\n",
            "--- Iteration 8268: Training loss = 0.0095, 0.0435 s ---\n",
            "--- Iteration 8269: Training loss = 0.0061, 0.0426 s ---\n",
            "--- Iteration 8270: Training loss = 0.0108, 0.0419 s ---\n",
            "--- Iteration 8270: Test loss = 0.0523 ---\n",
            "\n",
            "--- Iteration 8271: Training loss = 0.0174, 0.0407 s ---\n",
            "--- Iteration 8272: Training loss = 0.0131, 0.0417 s ---\n",
            "--- Iteration 8273: Training loss = 0.0063, 0.0469 s ---\n",
            "--- Iteration 8274: Training loss = 0.0149, 0.0429 s ---\n",
            "--- Iteration 8275: Training loss = 0.0096, 0.0444 s ---\n",
            "--- Iteration 8276: Training loss = 0.0086, 0.0438 s ---\n",
            "--- Iteration 8277: Training loss = 0.0097, 0.0429 s ---\n",
            "--- Iteration 8278: Training loss = 0.0089, 0.0430 s ---\n",
            "--- Iteration 8279: Training loss = 0.0105, 0.0424 s ---\n",
            "--- Iteration 8280: Training loss = 0.0110, 0.0431 s ---\n",
            "--- Iteration 8280: Test loss = 0.0516 ---\n",
            "\n",
            "--- Iteration 8281: Training loss = 0.0076, 0.0421 s ---\n",
            "--- Iteration 8282: Training loss = 0.0125, 0.0436 s ---\n",
            "--- Iteration 8283: Training loss = 0.0111, 0.0446 s ---\n",
            "--- Iteration 8284: Training loss = 0.0095, 0.0443 s ---\n",
            "--- Iteration 8285: Training loss = 0.0076, 0.0487 s ---\n",
            "--- Iteration 8286: Training loss = 0.0082, 0.0451 s ---\n",
            "--- Iteration 8287: Training loss = 0.0111, 0.0448 s ---\n",
            "--- Iteration 8288: Training loss = 0.0123, 0.0429 s ---\n",
            "--- Iteration 8289: Training loss = 0.0087, 0.0444 s ---\n",
            "--- Iteration 8290: Training loss = 0.0130, 0.0425 s ---\n",
            "--- Iteration 8290: Test loss = 0.0239 ---\n",
            "\n",
            "--- Iteration 8291: Training loss = 0.0097, 0.0418 s ---\n",
            "--- Iteration 8292: Training loss = 0.0062, 0.0430 s ---\n",
            "--- Iteration 8293: Training loss = 0.0102, 0.0431 s ---\n",
            "--- Iteration 8294: Training loss = 0.0131, 0.0469 s ---\n",
            "--- Iteration 8295: Training loss = 0.0093, 0.0460 s ---\n",
            "--- Iteration 8296: Training loss = 0.0104, 0.0437 s ---\n",
            "--- Iteration 8297: Training loss = 0.0087, 0.0429 s ---\n",
            "--- Iteration 8298: Training loss = 0.0111, 0.0443 s ---\n",
            "--- Iteration 8299: Training loss = 0.0100, 0.0443 s ---\n",
            "--- Iteration 8300: Training loss = 0.0127, 0.0432 s ---\n",
            "--- Iteration 8300: Test loss = 0.0368 ---\n",
            "\n",
            "--- Iteration 8301: Training loss = 0.0079, 0.0410 s ---\n",
            "--- Iteration 8302: Training loss = 0.0111, 0.0423 s ---\n",
            "--- Iteration 8303: Training loss = 0.0107, 0.0428 s ---\n",
            "--- Iteration 8304: Training loss = 0.0082, 0.0429 s ---\n",
            "--- Iteration 8305: Training loss = 0.0063, 0.0428 s ---\n",
            "--- Iteration 8306: Training loss = 0.0093, 0.0434 s ---\n",
            "--- Iteration 8307: Training loss = 0.0068, 0.0438 s ---\n",
            "--- Iteration 8308: Training loss = 0.0102, 0.0446 s ---\n",
            "--- Iteration 8309: Training loss = 0.0128, 0.0426 s ---\n",
            "--- Iteration 8310: Training loss = 0.0082, 0.0423 s ---\n",
            "--- Iteration 8310: Test loss = 0.0492 ---\n",
            "\n",
            "--- Iteration 8311: Training loss = 0.0068, 0.0412 s ---\n",
            "--- Iteration 8312: Training loss = 0.0122, 0.0428 s ---\n",
            "--- Iteration 8313: Training loss = 0.0116, 0.0429 s ---\n",
            "--- Iteration 8314: Training loss = 0.0102, 0.0430 s ---\n",
            "--- Iteration 8315: Training loss = 0.0140, 0.0434 s ---\n",
            "--- Iteration 8316: Training loss = 0.0111, 0.0446 s ---\n",
            "--- Iteration 8317: Training loss = 0.0079, 0.0444 s ---\n",
            "--- Iteration 8318: Training loss = 0.0073, 0.0456 s ---\n",
            "--- Iteration 8319: Training loss = 0.0123, 0.0431 s ---\n",
            "--- Iteration 8320: Training loss = 0.0099, 0.0429 s ---\n",
            "--- Iteration 8320: Test loss = 0.0287 ---\n",
            "\n",
            "--- Iteration 8321: Training loss = 0.0042, 0.0434 s ---\n",
            "--- Iteration 8322: Training loss = 0.0147, 0.0442 s ---\n",
            "--- Iteration 8323: Training loss = 0.0072, 0.0433 s ---\n",
            "--- Iteration 8324: Training loss = 0.0076, 0.0425 s ---\n",
            "--- Iteration 8325: Training loss = 0.0118, 0.0451 s ---\n",
            "--- Iteration 8326: Training loss = 0.0062, 0.0430 s ---\n",
            "--- Iteration 8327: Training loss = 0.0108, 0.0429 s ---\n",
            "--- Iteration 8328: Training loss = 0.0109, 0.0438 s ---\n",
            "--- Iteration 8329: Training loss = 0.0097, 0.0445 s ---\n",
            "--- Iteration 8330: Training loss = 0.0147, 0.0442 s ---\n",
            "--- Iteration 8330: Test loss = 0.0407 ---\n",
            "\n",
            "--- Iteration 8331: Training loss = 0.0084, 0.0407 s ---\n",
            "--- Iteration 8332: Training loss = 0.0087, 0.0472 s ---\n",
            "--- Iteration 8333: Training loss = 0.0128, 0.0424 s ---\n",
            "--- Iteration 8334: Training loss = 0.0095, 0.0431 s ---\n",
            "--- Iteration 8335: Training loss = 0.0099, 0.0457 s ---\n",
            "--- Iteration 8336: Training loss = 0.0032, 0.0440 s ---\n",
            "--- Iteration 8337: Training loss = 0.0117, 0.0429 s ---\n",
            "--- Iteration 8338: Training loss = 0.0089, 0.0431 s ---\n",
            "--- Iteration 8339: Training loss = 0.0120, 0.0434 s ---\n",
            "--- Iteration 8340: Training loss = 0.0085, 0.0429 s ---\n",
            "--- Iteration 8340: Test loss = 0.0429 ---\n",
            "\n",
            "--- Iteration 8341: Training loss = 0.0108, 0.0432 s ---\n",
            "--- Iteration 8342: Training loss = 0.0101, 0.0445 s ---\n",
            "--- Iteration 8343: Training loss = 0.0104, 0.0433 s ---\n",
            "--- Iteration 8344: Training loss = 0.0099, 0.0427 s ---\n",
            "--- Iteration 8345: Training loss = 0.0092, 0.0430 s ---\n",
            "--- Iteration 8346: Training loss = 0.0123, 0.0417 s ---\n",
            "--- Iteration 8347: Training loss = 0.0074, 0.0425 s ---\n",
            "--- Iteration 8348: Training loss = 0.0082, 0.0428 s ---\n",
            "--- Iteration 8349: Training loss = 0.0077, 0.0450 s ---\n",
            "--- Iteration 8350: Training loss = 0.0057, 0.0442 s ---\n",
            "--- Iteration 8350: Test loss = 0.0281 ---\n",
            "\n",
            "--- Iteration 8351: Training loss = 0.0101, 0.0414 s ---\n",
            "--- Iteration 8352: Training loss = 0.0100, 0.0434 s ---\n",
            "--- Iteration 8353: Training loss = 0.0109, 0.0425 s ---\n",
            "--- Iteration 8354: Training loss = 0.0074, 0.0427 s ---\n",
            "--- Iteration 8355: Training loss = 0.0087, 0.0427 s ---\n",
            "--- Iteration 8356: Training loss = 0.0086, 0.0445 s ---\n",
            "--- Iteration 8357: Training loss = 0.0087, 0.0450 s ---\n",
            "--- Iteration 8358: Training loss = 0.0130, 0.0434 s ---\n",
            "--- Iteration 8359: Training loss = 0.0073, 0.0426 s ---\n",
            "--- Iteration 8360: Training loss = 0.0069, 0.0424 s ---\n",
            "--- Iteration 8360: Test loss = 0.0777 ---\n",
            "\n",
            "--- Iteration 8361: Training loss = 0.0097, 0.0413 s ---\n",
            "--- Iteration 8362: Training loss = 0.0085, 0.0430 s ---\n",
            "--- Iteration 8363: Training loss = 0.0089, 0.0441 s ---\n",
            "--- Iteration 8364: Training loss = 0.0071, 0.0440 s ---\n",
            "--- Iteration 8365: Training loss = 0.0059, 0.0451 s ---\n",
            "--- Iteration 8366: Training loss = 0.0061, 0.0433 s ---\n",
            "--- Iteration 8367: Training loss = 0.0101, 0.0422 s ---\n",
            "--- Iteration 8368: Training loss = 0.0098, 0.0421 s ---\n",
            "--- Iteration 8369: Training loss = 0.0113, 0.0427 s ---\n",
            "--- Iteration 8370: Training loss = 0.0049, 0.0426 s ---\n",
            "--- Iteration 8370: Test loss = 0.0396 ---\n",
            "\n",
            "--- Iteration 8371: Training loss = 0.0122, 0.0425 s ---\n",
            "--- Iteration 8372: Training loss = 0.0048, 0.0461 s ---\n",
            "--- Iteration 8373: Training loss = 0.0099, 0.0438 s ---\n",
            "--- Iteration 8374: Training loss = 0.0085, 0.0470 s ---\n",
            "--- Iteration 8375: Training loss = 0.0093, 0.0498 s ---\n",
            "--- Iteration 8376: Training loss = 0.0102, 0.0441 s ---\n",
            "--- Iteration 8377: Training loss = 0.0075, 0.0433 s ---\n",
            "--- Iteration 8378: Training loss = 0.0081, 0.0424 s ---\n",
            "--- Iteration 8379: Training loss = 0.0067, 0.0427 s ---\n",
            "--- Iteration 8380: Training loss = 0.0109, 0.0436 s ---\n",
            "--- Iteration 8380: Test loss = 0.0530 ---\n",
            "\n",
            "--- Iteration 8381: Training loss = 0.0095, 0.0442 s ---\n",
            "--- Iteration 8382: Training loss = 0.0104, 0.0441 s ---\n",
            "--- Iteration 8383: Training loss = 0.0123, 0.0435 s ---\n",
            "--- Iteration 8384: Training loss = 0.0083, 0.0435 s ---\n",
            "--- Iteration 8385: Training loss = 0.0110, 0.0497 s ---\n",
            "--- Iteration 8386: Training loss = 0.0113, 0.0440 s ---\n",
            "--- Iteration 8387: Training loss = 0.0120, 0.0443 s ---\n",
            "--- Iteration 8388: Training loss = 0.0057, 0.0442 s ---\n",
            "--- Iteration 8389: Training loss = 0.0102, 0.0430 s ---\n",
            "--- Iteration 8390: Training loss = 0.0070, 0.0425 s ---\n",
            "--- Iteration 8390: Test loss = 0.0719 ---\n",
            "\n",
            "--- Iteration 8391: Training loss = 0.0138, 0.0420 s ---\n",
            "--- Iteration 8392: Training loss = 0.0096, 0.0435 s ---\n",
            "--- Iteration 8393: Training loss = 0.0095, 0.0444 s ---\n",
            "--- Iteration 8394: Training loss = 0.0111, 0.0465 s ---\n",
            "--- Iteration 8395: Training loss = 0.0111, 0.0477 s ---\n",
            "--- Iteration 8396: Training loss = 0.0130, 0.0554 s ---\n",
            "--- Iteration 8397: Training loss = 0.0132, 0.0931 s ---\n",
            "--- Iteration 8398: Training loss = 0.0108, 0.0471 s ---\n",
            "--- Iteration 8399: Training loss = 0.0077, 0.0578 s ---\n",
            "--- Iteration 8400: Training loss = 0.0067, 0.0497 s ---\n",
            "--- Iteration 8400: Test loss = 0.0373 ---\n",
            "\n",
            "--- Iteration 8401: Training loss = 0.0086, 0.0459 s ---\n",
            "--- Iteration 8402: Training loss = 0.0111, 0.0469 s ---\n",
            "--- Iteration 8403: Training loss = 0.0099, 0.0500 s ---\n",
            "--- Iteration 8404: Training loss = 0.0088, 0.0481 s ---\n",
            "--- Iteration 8405: Training loss = 0.0080, 0.0504 s ---\n",
            "--- Iteration 8406: Training loss = 0.0093, 0.0472 s ---\n",
            "--- Iteration 8407: Training loss = 0.0090, 0.0474 s ---\n",
            "--- Iteration 8408: Training loss = 0.0073, 0.0555 s ---\n",
            "--- Iteration 8409: Training loss = 0.0090, 0.0459 s ---\n",
            "--- Iteration 8410: Training loss = 0.0106, 0.0478 s ---\n",
            "--- Iteration 8410: Test loss = 0.0870 ---\n",
            "\n",
            "--- Iteration 8411: Training loss = 0.0059, 0.0439 s ---\n",
            "--- Iteration 8412: Training loss = 0.0057, 0.0464 s ---\n",
            "--- Iteration 8413: Training loss = 0.0068, 0.0463 s ---\n",
            "--- Iteration 8414: Training loss = 0.0083, 0.0487 s ---\n",
            "--- Iteration 8415: Training loss = 0.0114, 0.0477 s ---\n",
            "--- Iteration 8416: Training loss = 0.0070, 0.0481 s ---\n",
            "--- Iteration 8417: Training loss = 0.0082, 0.0547 s ---\n",
            "--- Iteration 8418: Training loss = 0.0068, 0.0466 s ---\n",
            "--- Iteration 8419: Training loss = 0.0113, 0.0464 s ---\n",
            "--- Iteration 8420: Training loss = 0.0082, 0.0471 s ---\n",
            "--- Iteration 8420: Test loss = 0.0741 ---\n",
            "\n",
            "--- Iteration 8421: Training loss = 0.0048, 0.0458 s ---\n",
            "--- Iteration 8422: Training loss = 0.0052, 0.0489 s ---\n",
            "--- Iteration 8423: Training loss = 0.0130, 0.0455 s ---\n",
            "--- Iteration 8424: Training loss = 0.0061, 0.0459 s ---\n",
            "--- Iteration 8425: Training loss = 0.0089, 0.0483 s ---\n",
            "--- Iteration 8426: Training loss = 0.0062, 0.0512 s ---\n",
            "--- Iteration 8427: Training loss = 0.0065, 0.0482 s ---\n",
            "--- Iteration 8428: Training loss = 0.0083, 0.0454 s ---\n",
            "--- Iteration 8429: Training loss = 0.0076, 0.0470 s ---\n",
            "--- Iteration 8430: Training loss = 0.0154, 0.0471 s ---\n",
            "--- Iteration 8430: Test loss = 0.0483 ---\n",
            "\n",
            "--- Iteration 8431: Training loss = 0.0067, 0.0468 s ---\n",
            "--- Iteration 8432: Training loss = 0.0156, 0.0486 s ---\n",
            "--- Iteration 8433: Training loss = 0.0058, 0.0469 s ---\n",
            "--- Iteration 8434: Training loss = 0.0072, 0.0492 s ---\n",
            "--- Iteration 8435: Training loss = 0.0137, 0.0512 s ---\n",
            "--- Iteration 8436: Training loss = 0.0114, 0.0514 s ---\n",
            "--- Iteration 8437: Training loss = 0.0072, 0.0466 s ---\n",
            "--- Iteration 8438: Training loss = 0.0106, 0.0464 s ---\n",
            "--- Iteration 8439: Training loss = 0.0083, 0.0487 s ---\n",
            "--- Iteration 8440: Training loss = 0.0079, 0.0512 s ---\n",
            "--- Iteration 8440: Test loss = 0.0486 ---\n",
            "\n",
            "--- Iteration 8441: Training loss = 0.0116, 0.0430 s ---\n",
            "--- Iteration 8442: Training loss = 0.0102, 0.0446 s ---\n",
            "--- Iteration 8443: Training loss = 0.0084, 0.0436 s ---\n",
            "--- Iteration 8444: Training loss = 0.0077, 0.0465 s ---\n",
            "--- Iteration 8445: Training loss = 0.0141, 0.0482 s ---\n",
            "--- Iteration 8446: Training loss = 0.0086, 0.0442 s ---\n",
            "--- Iteration 8447: Training loss = 0.0142, 0.0474 s ---\n",
            "--- Iteration 8448: Training loss = 0.0072, 0.0423 s ---\n",
            "--- Iteration 8449: Training loss = 0.0110, 0.0426 s ---\n",
            "--- Iteration 8450: Training loss = 0.0066, 0.0431 s ---\n",
            "--- Iteration 8450: Test loss = 0.0636 ---\n",
            "\n",
            "--- Iteration 8451: Training loss = 0.0085, 0.0444 s ---\n",
            "--- Iteration 8452: Training loss = 0.0115, 0.0451 s ---\n",
            "--- Iteration 8453: Training loss = 0.0115, 0.0431 s ---\n",
            "--- Iteration 8454: Training loss = 0.0062, 0.0437 s ---\n",
            "--- Iteration 8455: Training loss = 0.0127, 0.0482 s ---\n",
            "--- Iteration 8456: Training loss = 0.0075, 0.0476 s ---\n",
            "--- Iteration 8457: Training loss = 0.0119, 0.0428 s ---\n",
            "--- Iteration 8458: Training loss = 0.0091, 0.0433 s ---\n",
            "--- Iteration 8459: Training loss = 0.0118, 0.0432 s ---\n",
            "--- Iteration 8460: Training loss = 0.0101, 0.0441 s ---\n",
            "--- Iteration 8460: Test loss = 0.0413 ---\n",
            "\n",
            "--- Iteration 8461: Training loss = 0.0101, 0.0466 s ---\n",
            "--- Iteration 8462: Training loss = 0.0122, 0.0434 s ---\n",
            "--- Iteration 8463: Training loss = 0.0096, 0.0430 s ---\n",
            "--- Iteration 8464: Training loss = 0.0074, 0.0423 s ---\n",
            "--- Iteration 8465: Training loss = 0.0056, 0.0477 s ---\n",
            "--- Iteration 8466: Training loss = 0.0098, 0.0444 s ---\n",
            "--- Iteration 8467: Training loss = 0.0098, 0.0432 s ---\n",
            "--- Iteration 8468: Training loss = 0.0067, 0.0428 s ---\n",
            "--- Iteration 8469: Training loss = 0.0075, 0.0468 s ---\n",
            "--- Iteration 8470: Training loss = 0.0091, 0.0451 s ---\n",
            "--- Iteration 8470: Test loss = 0.0413 ---\n",
            "\n",
            "--- Iteration 8471: Training loss = 0.0088, 0.0428 s ---\n",
            "--- Iteration 8472: Training loss = 0.0124, 0.0437 s ---\n",
            "--- Iteration 8473: Training loss = 0.0084, 0.0437 s ---\n",
            "--- Iteration 8474: Training loss = 0.0125, 0.0431 s ---\n",
            "--- Iteration 8475: Training loss = 0.0142, 0.0460 s ---\n",
            "--- Iteration 8476: Training loss = 0.0080, 0.0448 s ---\n",
            "--- Iteration 8477: Training loss = 0.0142, 0.0440 s ---\n",
            "--- Iteration 8478: Training loss = 0.0111, 0.0441 s ---\n",
            "--- Iteration 8479: Training loss = 0.0100, 0.0474 s ---\n",
            "--- Iteration 8480: Training loss = 0.0128, 0.0525 s ---\n",
            "--- Iteration 8480: Test loss = 0.0567 ---\n",
            "\n",
            "--- Iteration 8481: Training loss = 0.0094, 0.0433 s ---\n",
            "--- Iteration 8482: Training loss = 0.0098, 0.0489 s ---\n",
            "--- Iteration 8483: Training loss = 0.0071, 0.0429 s ---\n",
            "--- Iteration 8484: Training loss = 0.0077, 0.0432 s ---\n",
            "--- Iteration 8485: Training loss = 0.0104, 0.0467 s ---\n",
            "--- Iteration 8486: Training loss = 0.0107, 0.0436 s ---\n",
            "--- Iteration 8487: Training loss = 0.0154, 0.0447 s ---\n",
            "--- Iteration 8488: Training loss = 0.0123, 0.0436 s ---\n",
            "--- Iteration 8489: Training loss = 0.0049, 0.0429 s ---\n",
            "--- Iteration 8490: Training loss = 0.0105, 0.0421 s ---\n",
            "--- Iteration 8490: Test loss = 0.0564 ---\n",
            "\n",
            "--- Iteration 8491: Training loss = 0.0114, 0.0413 s ---\n",
            "--- Iteration 8492: Training loss = 0.0117, 0.0424 s ---\n",
            "--- Iteration 8493: Training loss = 0.0093, 0.0437 s ---\n",
            "--- Iteration 8494: Training loss = 0.0119, 0.0453 s ---\n",
            "--- Iteration 8495: Training loss = 0.0080, 0.0439 s ---\n",
            "--- Iteration 8496: Training loss = 0.0101, 0.0425 s ---\n",
            "--- Iteration 8497: Training loss = 0.0052, 0.0434 s ---\n",
            "--- Iteration 8498: Training loss = 0.0126, 0.0434 s ---\n",
            "--- Iteration 8499: Training loss = 0.0119, 0.0433 s ---\n",
            "--- Iteration 8500: Training loss = 0.0151, 0.0427 s ---\n",
            "--- Iteration 8500: Test loss = 0.0375 ---\n",
            "\n",
            "--- Iteration 8501: Training loss = 0.0089, 0.0430 s ---\n",
            "--- Iteration 8502: Training loss = 0.0065, 0.0445 s ---\n",
            "--- Iteration 8503: Training loss = 0.0097, 0.0452 s ---\n",
            "--- Iteration 8504: Training loss = 0.0096, 0.0561 s ---\n",
            "--- Iteration 8505: Training loss = 0.0084, 0.0503 s ---\n",
            "--- Iteration 8506: Training loss = 0.0101, 0.0596 s ---\n",
            "--- Iteration 8507: Training loss = 0.0107, 0.1130 s ---\n",
            "--- Iteration 8508: Training loss = 0.0064, 0.0460 s ---\n",
            "--- Iteration 8509: Training loss = 0.0102, 0.0570 s ---\n",
            "--- Iteration 8510: Training loss = 0.0125, 0.0665 s ---\n",
            "--- Iteration 8510: Test loss = 0.0249 ---\n",
            "\n",
            "--- Iteration 8511: Training loss = 0.0078, 0.0636 s ---\n",
            "--- Iteration 8512: Training loss = 0.0156, 0.0455 s ---\n",
            "--- Iteration 8513: Training loss = 0.0108, 0.0446 s ---\n",
            "--- Iteration 8514: Training loss = 0.0094, 0.0437 s ---\n",
            "--- Iteration 8515: Training loss = 0.0077, 0.0455 s ---\n",
            "--- Iteration 8516: Training loss = 0.0110, 0.0428 s ---\n",
            "--- Iteration 8517: Training loss = 0.0109, 0.0460 s ---\n",
            "--- Iteration 8518: Training loss = 0.0105, 0.0448 s ---\n",
            "--- Iteration 8519: Training loss = 0.0068, 0.0432 s ---\n",
            "--- Iteration 8520: Training loss = 0.0062, 0.0419 s ---\n",
            "--- Iteration 8520: Test loss = 0.0771 ---\n",
            "\n",
            "--- Iteration 8521: Training loss = 0.0057, 0.0415 s ---\n",
            "--- Iteration 8522: Training loss = 0.0110, 0.0425 s ---\n",
            "--- Iteration 8523: Training loss = 0.0096, 0.0430 s ---\n",
            "--- Iteration 8524: Training loss = 0.0054, 0.0431 s ---\n",
            "--- Iteration 8525: Training loss = 0.0106, 0.0446 s ---\n",
            "--- Iteration 8526: Training loss = 0.0093, 0.0449 s ---\n",
            "--- Iteration 8527: Training loss = 0.0111, 0.0438 s ---\n",
            "--- Iteration 8528: Training loss = 0.0132, 0.0433 s ---\n",
            "--- Iteration 8529: Training loss = 0.0079, 0.0433 s ---\n",
            "--- Iteration 8530: Training loss = 0.0116, 0.0453 s ---\n",
            "--- Iteration 8530: Test loss = 0.0254 ---\n",
            "\n",
            "--- Iteration 8531: Training loss = 0.0084, 0.0431 s ---\n",
            "--- Iteration 8532: Training loss = 0.0095, 0.0445 s ---\n",
            "--- Iteration 8533: Training loss = 0.0072, 0.0433 s ---\n",
            "--- Iteration 8534: Training loss = 0.0073, 0.0493 s ---\n",
            "--- Iteration 8535: Training loss = 0.0088, 0.0446 s ---\n",
            "--- Iteration 8536: Training loss = 0.0120, 0.0449 s ---\n",
            "--- Iteration 8537: Training loss = 0.0091, 0.0431 s ---\n",
            "--- Iteration 8538: Training loss = 0.0080, 0.0469 s ---\n",
            "--- Iteration 8539: Training loss = 0.0105, 0.0439 s ---\n",
            "--- Iteration 8540: Training loss = 0.0097, 0.0449 s ---\n",
            "--- Iteration 8540: Test loss = 0.0321 ---\n",
            "\n",
            "--- Iteration 8541: Training loss = 0.0074, 0.0421 s ---\n",
            "--- Iteration 8542: Training loss = 0.0088, 0.0430 s ---\n",
            "--- Iteration 8543: Training loss = 0.0112, 0.0442 s ---\n",
            "--- Iteration 8544: Training loss = 0.0056, 0.0463 s ---\n",
            "--- Iteration 8545: Training loss = 0.0061, 0.0446 s ---\n",
            "--- Iteration 8546: Training loss = 0.0106, 0.0445 s ---\n",
            "--- Iteration 8547: Training loss = 0.0108, 0.0436 s ---\n",
            "--- Iteration 8548: Training loss = 0.0097, 0.0430 s ---\n",
            "--- Iteration 8549: Training loss = 0.0088, 0.0430 s ---\n",
            "--- Iteration 8550: Training loss = 0.0074, 0.0440 s ---\n",
            "--- Iteration 8550: Test loss = 0.0203 ---\n",
            "\n",
            "--- Iteration 8551: Training loss = 0.0104, 0.0433 s ---\n",
            "--- Iteration 8552: Training loss = 0.0091, 0.0493 s ---\n",
            "--- Iteration 8553: Training loss = 0.0097, 0.0425 s ---\n",
            "--- Iteration 8554: Training loss = 0.0123, 0.0485 s ---\n",
            "--- Iteration 8555: Training loss = 0.0100, 0.0473 s ---\n",
            "--- Iteration 8556: Training loss = 0.0122, 0.0454 s ---\n",
            "--- Iteration 8557: Training loss = 0.0042, 0.0429 s ---\n",
            "--- Iteration 8558: Training loss = 0.0112, 0.0434 s ---\n",
            "--- Iteration 8559: Training loss = 0.0060, 0.0431 s ---\n",
            "--- Iteration 8560: Training loss = 0.0086, 0.0430 s ---\n",
            "--- Iteration 8560: Test loss = 0.0575 ---\n",
            "\n",
            "--- Iteration 8561: Training loss = 0.0056, 0.0435 s ---\n",
            "--- Iteration 8562: Training loss = 0.0098, 0.0438 s ---\n",
            "--- Iteration 8563: Training loss = 0.0143, 0.0434 s ---\n",
            "--- Iteration 8564: Training loss = 0.0075, 0.0473 s ---\n",
            "--- Iteration 8565: Training loss = 0.0103, 0.0433 s ---\n",
            "--- Iteration 8566: Training loss = 0.0061, 0.0449 s ---\n",
            "--- Iteration 8567: Training loss = 0.0085, 0.0447 s ---\n",
            "--- Iteration 8568: Training loss = 0.0119, 0.0431 s ---\n",
            "--- Iteration 8569: Training loss = 0.0080, 0.0430 s ---\n",
            "--- Iteration 8570: Training loss = 0.0081, 0.0477 s ---\n",
            "--- Iteration 8570: Test loss = 0.0604 ---\n",
            "\n",
            "--- Iteration 8571: Training loss = 0.0069, 0.0435 s ---\n",
            "--- Iteration 8572: Training loss = 0.0078, 0.0440 s ---\n",
            "--- Iteration 8573: Training loss = 0.0128, 0.0433 s ---\n",
            "--- Iteration 8574: Training loss = 0.0087, 0.0494 s ---\n",
            "--- Iteration 8575: Training loss = 0.0113, 0.0424 s ---\n",
            "--- Iteration 8576: Training loss = 0.0087, 0.0452 s ---\n",
            "--- Iteration 8577: Training loss = 0.0086, 0.0442 s ---\n",
            "--- Iteration 8578: Training loss = 0.0079, 0.0423 s ---\n",
            "--- Iteration 8579: Training loss = 0.0074, 0.0430 s ---\n",
            "--- Iteration 8580: Training loss = 0.0145, 0.0433 s ---\n",
            "--- Iteration 8580: Test loss = 0.0287 ---\n",
            "\n",
            "--- Iteration 8581: Training loss = 0.0093, 0.0428 s ---\n",
            "--- Iteration 8582: Training loss = 0.0102, 0.0439 s ---\n",
            "--- Iteration 8583: Training loss = 0.0106, 0.0450 s ---\n",
            "--- Iteration 8584: Training loss = 0.0095, 0.0468 s ---\n",
            "--- Iteration 8585: Training loss = 0.0143, 0.0435 s ---\n",
            "--- Iteration 8586: Training loss = 0.0105, 0.0438 s ---\n",
            "--- Iteration 8587: Training loss = 0.0101, 0.0474 s ---\n",
            "--- Iteration 8588: Training loss = 0.0067, 0.0441 s ---\n",
            "--- Iteration 8589: Training loss = 0.0112, 0.0436 s ---\n",
            "--- Iteration 8590: Training loss = 0.0061, 0.0427 s ---\n",
            "--- Iteration 8590: Test loss = 0.0289 ---\n",
            "\n",
            "--- Iteration 8591: Training loss = 0.0082, 0.0417 s ---\n",
            "--- Iteration 8592: Training loss = 0.0067, 0.0436 s ---\n",
            "--- Iteration 8593: Training loss = 0.0093, 0.0441 s ---\n",
            "--- Iteration 8594: Training loss = 0.0087, 0.0438 s ---\n",
            "--- Iteration 8595: Training loss = 0.0114, 0.0443 s ---\n",
            "--- Iteration 8596: Training loss = 0.0124, 0.0438 s ---\n",
            "--- Iteration 8597: Training loss = 0.0063, 0.0436 s ---\n",
            "--- Iteration 8598: Training loss = 0.0119, 0.0437 s ---\n",
            "--- Iteration 8599: Training loss = 0.0084, 0.0439 s ---\n",
            "--- Iteration 8600: Training loss = 0.0087, 0.0447 s ---\n",
            "--- Iteration 8600: Test loss = 0.0218 ---\n",
            "\n",
            "--- Iteration 8601: Training loss = 0.0131, 0.0425 s ---\n",
            "--- Iteration 8602: Training loss = 0.0145, 0.0431 s ---\n",
            "--- Iteration 8603: Training loss = 0.0114, 0.0428 s ---\n",
            "--- Iteration 8604: Training loss = 0.0105, 0.0449 s ---\n",
            "--- Iteration 8605: Training loss = 0.0136, 0.0425 s ---\n",
            "--- Iteration 8606: Training loss = 0.0083, 0.0452 s ---\n",
            "--- Iteration 8607: Training loss = 0.0088, 0.0456 s ---\n",
            "--- Iteration 8608: Training loss = 0.0097, 0.0435 s ---\n",
            "--- Iteration 8609: Training loss = 0.0141, 0.0440 s ---\n",
            "--- Iteration 8610: Training loss = 0.0094, 0.0434 s ---\n",
            "--- Iteration 8610: Test loss = 0.0509 ---\n",
            "\n",
            "--- Iteration 8611: Training loss = 0.0088, 0.0431 s ---\n",
            "--- Iteration 8612: Training loss = 0.0072, 0.0441 s ---\n",
            "--- Iteration 8613: Training loss = 0.0075, 0.0439 s ---\n",
            "--- Iteration 8614: Training loss = 0.0088, 0.0480 s ---\n",
            "--- Iteration 8615: Training loss = 0.0126, 0.0434 s ---\n",
            "--- Iteration 8616: Training loss = 0.0150, 0.0445 s ---\n",
            "--- Iteration 8617: Training loss = 0.0058, 0.0479 s ---\n",
            "--- Iteration 8618: Training loss = 0.0164, 0.0428 s ---\n",
            "--- Iteration 8619: Training loss = 0.0094, 0.0423 s ---\n",
            "--- Iteration 8620: Training loss = 0.0109, 0.0436 s ---\n",
            "--- Iteration 8620: Test loss = 0.0544 ---\n",
            "\n",
            "--- Iteration 8621: Training loss = 0.0079, 0.0426 s ---\n",
            "--- Iteration 8622: Training loss = 0.0073, 0.0449 s ---\n",
            "--- Iteration 8623: Training loss = 0.0096, 0.0442 s ---\n",
            "--- Iteration 8624: Training loss = 0.0081, 0.0449 s ---\n",
            "--- Iteration 8625: Training loss = 0.0095, 0.0432 s ---\n",
            "--- Iteration 8626: Training loss = 0.0072, 0.0429 s ---\n",
            "--- Iteration 8627: Training loss = 0.0098, 0.0431 s ---\n",
            "--- Iteration 8628: Training loss = 0.0122, 0.0445 s ---\n",
            "--- Iteration 8629: Training loss = 0.0140, 0.0449 s ---\n",
            "--- Iteration 8630: Training loss = 0.0085, 0.0429 s ---\n",
            "--- Iteration 8630: Test loss = 0.0495 ---\n",
            "\n",
            "--- Iteration 8631: Training loss = 0.0112, 0.0411 s ---\n",
            "--- Iteration 8632: Training loss = 0.0114, 0.0429 s ---\n",
            "--- Iteration 8633: Training loss = 0.0050, 0.0425 s ---\n",
            "--- Iteration 8634: Training loss = 0.0101, 0.0420 s ---\n",
            "--- Iteration 8635: Training loss = 0.0098, 0.0423 s ---\n",
            "--- Iteration 8636: Training loss = 0.0140, 0.0429 s ---\n",
            "--- Iteration 8637: Training loss = 0.0074, 0.0442 s ---\n",
            "--- Iteration 8638: Training loss = 0.0089, 0.0458 s ---\n",
            "--- Iteration 8639: Training loss = 0.0074, 0.0429 s ---\n",
            "--- Iteration 8640: Training loss = 0.0058, 0.0435 s ---\n",
            "--- Iteration 8640: Test loss = 0.0833 ---\n",
            "\n",
            "--- Iteration 8641: Training loss = 0.0081, 0.0418 s ---\n",
            "--- Iteration 8642: Training loss = 0.0094, 0.0445 s ---\n",
            "--- Iteration 8643: Training loss = 0.0094, 0.0435 s ---\n",
            "--- Iteration 8644: Training loss = 0.0041, 0.0497 s ---\n",
            "--- Iteration 8645: Training loss = 0.0132, 0.0437 s ---\n",
            "--- Iteration 8646: Training loss = 0.0082, 0.0472 s ---\n",
            "--- Iteration 8647: Training loss = 0.0108, 0.0420 s ---\n",
            "--- Iteration 8648: Training loss = 0.0038, 0.0426 s ---\n",
            "--- Iteration 8649: Training loss = 0.0135, 0.0424 s ---\n",
            "--- Iteration 8650: Training loss = 0.0111, 0.0419 s ---\n",
            "--- Iteration 8650: Test loss = 0.0876 ---\n",
            "\n",
            "--- Iteration 8651: Training loss = 0.0088, 0.0426 s ---\n",
            "--- Iteration 8652: Training loss = 0.0121, 0.0453 s ---\n",
            "--- Iteration 8653: Training loss = 0.0090, 0.0443 s ---\n",
            "--- Iteration 8654: Training loss = 0.0076, 0.0473 s ---\n",
            "--- Iteration 8655: Training loss = 0.0125, 0.0426 s ---\n",
            "--- Iteration 8656: Training loss = 0.0122, 0.0444 s ---\n",
            "--- Iteration 8657: Training loss = 0.0121, 0.0460 s ---\n",
            "--- Iteration 8658: Training loss = 0.0085, 0.0426 s ---\n",
            "--- Iteration 8659: Training loss = 0.0086, 0.0430 s ---\n",
            "--- Iteration 8660: Training loss = 0.0133, 0.0432 s ---\n",
            "--- Iteration 8660: Test loss = 0.0446 ---\n",
            "\n",
            "--- Iteration 8661: Training loss = 0.0117, 0.0416 s ---\n",
            "--- Iteration 8662: Training loss = 0.0108, 0.0432 s ---\n",
            "--- Iteration 8663: Training loss = 0.0132, 0.0442 s ---\n",
            "--- Iteration 8664: Training loss = 0.0092, 0.0454 s ---\n",
            "--- Iteration 8665: Training loss = 0.0095, 0.0444 s ---\n",
            "--- Iteration 8666: Training loss = 0.0092, 0.0430 s ---\n",
            "--- Iteration 8667: Training loss = 0.0112, 0.0430 s ---\n",
            "--- Iteration 8668: Training loss = 0.0085, 0.0428 s ---\n",
            "--- Iteration 8669: Training loss = 0.0105, 0.0433 s ---\n",
            "--- Iteration 8670: Training loss = 0.0048, 0.0508 s ---\n",
            "--- Iteration 8670: Test loss = 0.0334 ---\n",
            "\n",
            "--- Iteration 8671: Training loss = 0.0080, 0.0411 s ---\n",
            "--- Iteration 8672: Training loss = 0.0068, 0.0427 s ---\n",
            "--- Iteration 8673: Training loss = 0.0101, 0.0422 s ---\n",
            "--- Iteration 8674: Training loss = 0.0104, 0.0466 s ---\n",
            "--- Iteration 8675: Training loss = 0.0121, 0.0438 s ---\n",
            "--- Iteration 8676: Training loss = 0.0157, 0.0458 s ---\n",
            "--- Iteration 8677: Training loss = 0.0096, 0.0433 s ---\n",
            "--- Iteration 8678: Training loss = 0.0122, 0.0428 s ---\n",
            "--- Iteration 8679: Training loss = 0.0099, 0.0457 s ---\n",
            "--- Iteration 8680: Training loss = 0.0093, 0.0441 s ---\n",
            "--- Iteration 8680: Test loss = 0.0415 ---\n",
            "\n",
            "--- Iteration 8681: Training loss = 0.0062, 0.0438 s ---\n",
            "--- Iteration 8682: Training loss = 0.0108, 0.0445 s ---\n",
            "--- Iteration 8683: Training loss = 0.0093, 0.0432 s ---\n",
            "--- Iteration 8684: Training loss = 0.0117, 0.0476 s ---\n",
            "--- Iteration 8685: Training loss = 0.0094, 0.0447 s ---\n",
            "--- Iteration 8686: Training loss = 0.0108, 0.0443 s ---\n",
            "--- Iteration 8687: Training loss = 0.0087, 0.0437 s ---\n",
            "--- Iteration 8688: Training loss = 0.0095, 0.0420 s ---\n",
            "--- Iteration 8689: Training loss = 0.0058, 0.0436 s ---\n",
            "--- Iteration 8690: Training loss = 0.0116, 0.0435 s ---\n",
            "--- Iteration 8690: Test loss = 0.0259 ---\n",
            "\n",
            "--- Iteration 8691: Training loss = 0.0107, 0.0429 s ---\n",
            "--- Iteration 8692: Training loss = 0.0093, 0.0437 s ---\n",
            "--- Iteration 8693: Training loss = 0.0118, 0.0445 s ---\n",
            "--- Iteration 8694: Training loss = 0.0073, 0.0485 s ---\n",
            "--- Iteration 8695: Training loss = 0.0104, 0.0435 s ---\n",
            "--- Iteration 8696: Training loss = 0.0123, 0.0455 s ---\n",
            "--- Iteration 8697: Training loss = 0.0091, 0.0449 s ---\n",
            "--- Iteration 8698: Training loss = 0.0166, 0.0440 s ---\n",
            "--- Iteration 8699: Training loss = 0.0084, 0.0429 s ---\n",
            "--- Iteration 8700: Training loss = 0.0101, 0.0416 s ---\n",
            "--- Iteration 8700: Test loss = 0.0652 ---\n",
            "\n",
            "--- Iteration 8701: Training loss = 0.0120, 0.0412 s ---\n",
            "--- Iteration 8702: Training loss = 0.0090, 0.0422 s ---\n",
            "--- Iteration 8703: Training loss = 0.0068, 0.0423 s ---\n",
            "--- Iteration 8704: Training loss = 0.0082, 0.0434 s ---\n",
            "--- Iteration 8705: Training loss = 0.0098, 0.0442 s ---\n",
            "--- Iteration 8706: Training loss = 0.0111, 0.0472 s ---\n",
            "--- Iteration 8707: Training loss = 0.0087, 0.0489 s ---\n",
            "--- Iteration 8708: Training loss = 0.0100, 0.0432 s ---\n",
            "--- Iteration 8709: Training loss = 0.0135, 0.0449 s ---\n",
            "--- Iteration 8710: Training loss = 0.0071, 0.0439 s ---\n",
            "--- Iteration 8710: Test loss = 0.0468 ---\n",
            "\n",
            "--- Iteration 8711: Training loss = 0.0078, 0.0413 s ---\n",
            "--- Iteration 8712: Training loss = 0.0037, 0.0430 s ---\n",
            "--- Iteration 8713: Training loss = 0.0070, 0.0432 s ---\n",
            "--- Iteration 8714: Training loss = 0.0134, 0.0440 s ---\n",
            "--- Iteration 8715: Training loss = 0.0101, 0.0441 s ---\n",
            "--- Iteration 8716: Training loss = 0.0074, 0.0459 s ---\n",
            "--- Iteration 8717: Training loss = 0.0063, 0.0443 s ---\n",
            "--- Iteration 8718: Training loss = 0.0071, 0.0446 s ---\n",
            "--- Iteration 8719: Training loss = 0.0062, 0.0438 s ---\n",
            "--- Iteration 8720: Training loss = 0.0107, 0.0436 s ---\n",
            "--- Iteration 8720: Test loss = 0.0439 ---\n",
            "\n",
            "--- Iteration 8721: Training loss = 0.0083, 0.0426 s ---\n",
            "--- Iteration 8722: Training loss = 0.0080, 0.0452 s ---\n",
            "--- Iteration 8723: Training loss = 0.0071, 0.0447 s ---\n",
            "--- Iteration 8724: Training loss = 0.0102, 0.0457 s ---\n",
            "--- Iteration 8725: Training loss = 0.0104, 0.0432 s ---\n",
            "--- Iteration 8726: Training loss = 0.0088, 0.0429 s ---\n",
            "--- Iteration 8727: Training loss = 0.0077, 0.0445 s ---\n",
            "--- Iteration 8728: Training loss = 0.0103, 0.0441 s ---\n",
            "--- Iteration 8729: Training loss = 0.0078, 0.0439 s ---\n",
            "--- Iteration 8730: Training loss = 0.0095, 0.0421 s ---\n",
            "--- Iteration 8730: Test loss = 0.0438 ---\n",
            "\n",
            "--- Iteration 8731: Training loss = 0.0113, 0.0415 s ---\n",
            "--- Iteration 8732: Training loss = 0.0088, 0.0423 s ---\n",
            "--- Iteration 8733: Training loss = 0.0090, 0.0443 s ---\n",
            "--- Iteration 8734: Training loss = 0.0104, 0.0481 s ---\n",
            "--- Iteration 8735: Training loss = 0.0107, 0.0416 s ---\n",
            "--- Iteration 8736: Training loss = 0.0095, 0.0431 s ---\n",
            "--- Iteration 8737: Training loss = 0.0093, 0.0433 s ---\n",
            "--- Iteration 8738: Training loss = 0.0085, 0.0429 s ---\n",
            "--- Iteration 8739: Training loss = 0.0066, 0.0441 s ---\n",
            "--- Iteration 8740: Training loss = 0.0116, 0.0453 s ---\n",
            "--- Iteration 8740: Test loss = 0.0697 ---\n",
            "\n",
            "--- Iteration 8741: Training loss = 0.0087, 0.0427 s ---\n",
            "--- Iteration 8742: Training loss = 0.0103, 0.0424 s ---\n",
            "--- Iteration 8743: Training loss = 0.0098, 0.0454 s ---\n",
            "--- Iteration 8744: Training loss = 0.0067, 0.0458 s ---\n",
            "--- Iteration 8745: Training loss = 0.0099, 0.0442 s ---\n",
            "--- Iteration 8746: Training loss = 0.0132, 0.0437 s ---\n",
            "--- Iteration 8747: Training loss = 0.0122, 0.0430 s ---\n",
            "--- Iteration 8748: Training loss = 0.0071, 0.0430 s ---\n",
            "--- Iteration 8749: Training loss = 0.0119, 0.0437 s ---\n",
            "--- Iteration 8750: Training loss = 0.0050, 0.0516 s ---\n",
            "--- Iteration 8750: Test loss = 0.0455 ---\n",
            "\n",
            "--- Iteration 8751: Training loss = 0.0082, 0.0484 s ---\n",
            "--- Iteration 8752: Training loss = 0.0087, 0.0431 s ---\n",
            "--- Iteration 8753: Training loss = 0.0104, 0.0435 s ---\n",
            "--- Iteration 8754: Training loss = 0.0143, 0.0483 s ---\n",
            "--- Iteration 8755: Training loss = 0.0129, 0.0444 s ---\n",
            "--- Iteration 8756: Training loss = 0.0060, 0.0426 s ---\n",
            "--- Iteration 8757: Training loss = 0.0033, 0.0437 s ---\n",
            "--- Iteration 8758: Training loss = 0.0065, 0.0445 s ---\n",
            "--- Iteration 8759: Training loss = 0.0087, 0.0471 s ---\n",
            "--- Iteration 8760: Training loss = 0.0103, 0.0443 s ---\n",
            "--- Iteration 8760: Test loss = 0.0445 ---\n",
            "\n",
            "--- Iteration 8761: Training loss = 0.0103, 0.0407 s ---\n",
            "--- Iteration 8762: Training loss = 0.0145, 0.0431 s ---\n",
            "--- Iteration 8763: Training loss = 0.0115, 0.0446 s ---\n",
            "--- Iteration 8764: Training loss = 0.0052, 0.0416 s ---\n",
            "--- Iteration 8765: Training loss = 0.0072, 0.0422 s ---\n",
            "--- Iteration 8766: Training loss = 0.0095, 0.0444 s ---\n",
            "--- Iteration 8767: Training loss = 0.0105, 0.0446 s ---\n",
            "--- Iteration 8768: Training loss = 0.0088, 0.0440 s ---\n",
            "--- Iteration 8769: Training loss = 0.0074, 0.0432 s ---\n",
            "--- Iteration 8770: Training loss = 0.0114, 0.0420 s ---\n",
            "--- Iteration 8770: Test loss = 0.0471 ---\n",
            "\n",
            "--- Iteration 8771: Training loss = 0.0079, 0.0422 s ---\n",
            "--- Iteration 8772: Training loss = 0.0104, 0.0426 s ---\n",
            "--- Iteration 8773: Training loss = 0.0129, 0.0433 s ---\n",
            "--- Iteration 8774: Training loss = 0.0078, 0.0452 s ---\n",
            "--- Iteration 8775: Training loss = 0.0101, 0.0437 s ---\n",
            "--- Iteration 8776: Training loss = 0.0108, 0.0427 s ---\n",
            "--- Iteration 8777: Training loss = 0.0093, 0.0422 s ---\n",
            "--- Iteration 8778: Training loss = 0.0103, 0.0426 s ---\n",
            "--- Iteration 8779: Training loss = 0.0115, 0.0431 s ---\n",
            "--- Iteration 8780: Training loss = 0.0117, 0.0427 s ---\n",
            "--- Iteration 8780: Test loss = 0.0394 ---\n",
            "\n",
            "--- Iteration 8781: Training loss = 0.0118, 0.0415 s ---\n",
            "--- Iteration 8782: Training loss = 0.0144, 0.0439 s ---\n",
            "--- Iteration 8783: Training loss = 0.0054, 0.0451 s ---\n",
            "--- Iteration 8784: Training loss = 0.0105, 0.0443 s ---\n",
            "--- Iteration 8785: Training loss = 0.0126, 0.0417 s ---\n",
            "--- Iteration 8786: Training loss = 0.0092, 0.0431 s ---\n",
            "--- Iteration 8787: Training loss = 0.0114, 0.0419 s ---\n",
            "--- Iteration 8788: Training loss = 0.0108, 0.0422 s ---\n",
            "--- Iteration 8789: Training loss = 0.0099, 0.0451 s ---\n",
            "--- Iteration 8790: Training loss = 0.0092, 0.0444 s ---\n",
            "--- Iteration 8790: Test loss = 0.0444 ---\n",
            "\n",
            "--- Iteration 8791: Training loss = 0.0142, 0.0424 s ---\n",
            "--- Iteration 8792: Training loss = 0.0105, 0.0435 s ---\n",
            "--- Iteration 8793: Training loss = 0.0056, 0.0433 s ---\n",
            "--- Iteration 8794: Training loss = 0.0127, 0.0425 s ---\n",
            "--- Iteration 8795: Training loss = 0.0120, 0.0421 s ---\n",
            "--- Iteration 8796: Training loss = 0.0100, 0.0481 s ---\n",
            "--- Iteration 8797: Training loss = 0.0056, 0.0443 s ---\n",
            "--- Iteration 8798: Training loss = 0.0147, 0.0431 s ---\n",
            "--- Iteration 8799: Training loss = 0.0141, 0.0419 s ---\n",
            "--- Iteration 8800: Training loss = 0.0076, 0.0428 s ---\n",
            "--- Iteration 8800: Test loss = 0.0367 ---\n",
            "\n",
            "--- Iteration 8801: Training loss = 0.0078, 0.0412 s ---\n",
            "--- Iteration 8802: Training loss = 0.0091, 0.0427 s ---\n",
            "--- Iteration 8803: Training loss = 0.0131, 0.0453 s ---\n",
            "--- Iteration 8804: Training loss = 0.0090, 0.0444 s ---\n",
            "--- Iteration 8805: Training loss = 0.0096, 0.0449 s ---\n",
            "--- Iteration 8806: Training loss = 0.0102, 0.0441 s ---\n",
            "--- Iteration 8807: Training loss = 0.0119, 0.0430 s ---\n",
            "--- Iteration 8808: Training loss = 0.0099, 0.0431 s ---\n",
            "--- Iteration 8809: Training loss = 0.0085, 0.0443 s ---\n",
            "--- Iteration 8810: Training loss = 0.0078, 0.0432 s ---\n",
            "--- Iteration 8810: Test loss = 0.0602 ---\n",
            "\n",
            "--- Iteration 8811: Training loss = 0.0103, 0.0433 s ---\n",
            "--- Iteration 8812: Training loss = 0.0104, 0.0439 s ---\n",
            "--- Iteration 8813: Training loss = 0.0086, 0.0448 s ---\n",
            "--- Iteration 8814: Training loss = 0.0100, 0.0431 s ---\n",
            "--- Iteration 8815: Training loss = 0.0127, 0.0428 s ---\n",
            "--- Iteration 8816: Training loss = 0.0120, 0.0422 s ---\n",
            "--- Iteration 8817: Training loss = 0.0079, 0.0438 s ---\n",
            "--- Iteration 8818: Training loss = 0.0069, 0.0440 s ---\n",
            "--- Iteration 8819: Training loss = 0.0133, 0.0474 s ---\n",
            "--- Iteration 8820: Training loss = 0.0085, 0.0431 s ---\n",
            "--- Iteration 8820: Test loss = 0.0630 ---\n",
            "\n",
            "--- Iteration 8821: Training loss = 0.0092, 0.0421 s ---\n",
            "--- Iteration 8822: Training loss = 0.0129, 0.0427 s ---\n",
            "--- Iteration 8823: Training loss = 0.0067, 0.0465 s ---\n",
            "--- Iteration 8824: Training loss = 0.0070, 0.0438 s ---\n",
            "--- Iteration 8825: Training loss = 0.0107, 0.0429 s ---\n",
            "--- Iteration 8826: Training loss = 0.0083, 0.0425 s ---\n",
            "--- Iteration 8827: Training loss = 0.0100, 0.0434 s ---\n",
            "--- Iteration 8828: Training loss = 0.0069, 0.0433 s ---\n",
            "--- Iteration 8829: Training loss = 0.0083, 0.0453 s ---\n",
            "--- Iteration 8830: Training loss = 0.0071, 0.0443 s ---\n",
            "--- Iteration 8830: Test loss = 0.0557 ---\n",
            "\n",
            "--- Iteration 8831: Training loss = 0.0107, 0.0417 s ---\n",
            "--- Iteration 8832: Training loss = 0.0098, 0.0438 s ---\n",
            "--- Iteration 8833: Training loss = 0.0080, 0.0437 s ---\n",
            "--- Iteration 8834: Training loss = 0.0115, 0.0419 s ---\n",
            "--- Iteration 8835: Training loss = 0.0087, 0.0427 s ---\n",
            "--- Iteration 8836: Training loss = 0.0109, 0.0429 s ---\n",
            "--- Iteration 8837: Training loss = 0.0127, 0.0441 s ---\n",
            "--- Iteration 8838: Training loss = 0.0075, 0.0434 s ---\n",
            "--- Iteration 8839: Training loss = 0.0102, 0.0425 s ---\n",
            "--- Iteration 8840: Training loss = 0.0098, 0.0425 s ---\n",
            "--- Iteration 8840: Test loss = 0.0259 ---\n",
            "\n",
            "--- Iteration 8841: Training loss = 0.0067, 0.0481 s ---\n",
            "--- Iteration 8842: Training loss = 0.0127, 0.0431 s ---\n",
            "--- Iteration 8843: Training loss = 0.0079, 0.0476 s ---\n",
            "--- Iteration 8844: Training loss = 0.0115, 0.0426 s ---\n",
            "--- Iteration 8845: Training loss = 0.0055, 0.0423 s ---\n",
            "--- Iteration 8846: Training loss = 0.0078, 0.0420 s ---\n",
            "--- Iteration 8847: Training loss = 0.0088, 0.0427 s ---\n",
            "--- Iteration 8848: Training loss = 0.0118, 0.0424 s ---\n",
            "--- Iteration 8849: Training loss = 0.0092, 0.0435 s ---\n",
            "--- Iteration 8850: Training loss = 0.0084, 0.0449 s ---\n",
            "--- Iteration 8850: Test loss = 0.0255 ---\n",
            "\n",
            "--- Iteration 8851: Training loss = 0.0086, 0.0424 s ---\n",
            "--- Iteration 8852: Training loss = 0.0059, 0.0455 s ---\n",
            "--- Iteration 8853: Training loss = 0.0071, 0.0452 s ---\n",
            "--- Iteration 8854: Training loss = 0.0091, 0.0431 s ---\n",
            "--- Iteration 8855: Training loss = 0.0082, 0.0424 s ---\n",
            "--- Iteration 8856: Training loss = 0.0131, 0.0447 s ---\n",
            "--- Iteration 8857: Training loss = 0.0049, 0.0440 s ---\n",
            "--- Iteration 8858: Training loss = 0.0074, 0.0427 s ---\n",
            "--- Iteration 8859: Training loss = 0.0064, 0.0423 s ---\n",
            "--- Iteration 8860: Training loss = 0.0090, 0.0423 s ---\n",
            "--- Iteration 8860: Test loss = 0.0723 ---\n",
            "\n",
            "--- Iteration 8861: Training loss = 0.0112, 0.0417 s ---\n",
            "--- Iteration 8862: Training loss = 0.0083, 0.0429 s ---\n",
            "--- Iteration 8863: Training loss = 0.0102, 0.0431 s ---\n",
            "--- Iteration 8864: Training loss = 0.0101, 0.0436 s ---\n",
            "--- Iteration 8865: Training loss = 0.0113, 0.0443 s ---\n",
            "--- Iteration 8866: Training loss = 0.0067, 0.0435 s ---\n",
            "--- Iteration 8867: Training loss = 0.0061, 0.0424 s ---\n",
            "--- Iteration 8868: Training loss = 0.0076, 0.0424 s ---\n",
            "--- Iteration 8869: Training loss = 0.0121, 0.0449 s ---\n",
            "--- Iteration 8870: Training loss = 0.0113, 0.0422 s ---\n",
            "--- Iteration 8870: Test loss = 0.0428 ---\n",
            "\n",
            "--- Iteration 8871: Training loss = 0.0085, 0.0425 s ---\n",
            "--- Iteration 8872: Training loss = 0.0114, 0.0466 s ---\n",
            "--- Iteration 8873: Training loss = 0.0081, 0.0475 s ---\n",
            "--- Iteration 8874: Training loss = 0.0071, 0.0421 s ---\n",
            "--- Iteration 8875: Training loss = 0.0096, 0.0425 s ---\n",
            "--- Iteration 8876: Training loss = 0.0058, 0.0425 s ---\n",
            "--- Iteration 8877: Training loss = 0.0108, 0.0428 s ---\n",
            "--- Iteration 8878: Training loss = 0.0073, 0.0435 s ---\n",
            "--- Iteration 8879: Training loss = 0.0082, 0.0441 s ---\n",
            "--- Iteration 8880: Training loss = 0.0066, 0.0432 s ---\n",
            "--- Iteration 8880: Test loss = 0.0651 ---\n",
            "\n",
            "--- Iteration 8881: Training loss = 0.0049, 0.0417 s ---\n",
            "--- Iteration 8882: Training loss = 0.0101, 0.0427 s ---\n",
            "--- Iteration 8883: Training loss = 0.0074, 0.0484 s ---\n",
            "--- Iteration 8884: Training loss = 0.0084, 0.0455 s ---\n",
            "--- Iteration 8885: Training loss = 0.0110, 0.0430 s ---\n",
            "--- Iteration 8886: Training loss = 0.0076, 0.0461 s ---\n",
            "--- Iteration 8887: Training loss = 0.0080, 0.0423 s ---\n",
            "--- Iteration 8888: Training loss = 0.0084, 0.0428 s ---\n",
            "--- Iteration 8889: Training loss = 0.0109, 0.0445 s ---\n",
            "--- Iteration 8890: Training loss = 0.0152, 0.0451 s ---\n",
            "--- Iteration 8890: Test loss = 0.0646 ---\n",
            "\n",
            "--- Iteration 8891: Training loss = 0.0134, 0.0422 s ---\n",
            "--- Iteration 8892: Training loss = 0.0122, 0.0423 s ---\n",
            "--- Iteration 8893: Training loss = 0.0090, 0.0444 s ---\n",
            "--- Iteration 8894: Training loss = 0.0117, 0.0430 s ---\n",
            "--- Iteration 8895: Training loss = 0.0110, 0.0437 s ---\n",
            "--- Iteration 8896: Training loss = 0.0128, 0.0436 s ---\n",
            "--- Iteration 8897: Training loss = 0.0116, 0.0445 s ---\n",
            "--- Iteration 8898: Training loss = 0.0144, 0.0436 s ---\n",
            "--- Iteration 8899: Training loss = 0.0096, 0.0430 s ---\n",
            "--- Iteration 8900: Training loss = 0.0067, 0.0427 s ---\n",
            "--- Iteration 8900: Test loss = 0.0244 ---\n",
            "\n",
            "--- Iteration 8901: Training loss = 0.0083, 0.0414 s ---\n",
            "--- Iteration 8902: Training loss = 0.0109, 0.0429 s ---\n",
            "--- Iteration 8903: Training loss = 0.0075, 0.0523 s ---\n",
            "--- Iteration 8904: Training loss = 0.0115, 0.0434 s ---\n",
            "--- Iteration 8905: Training loss = 0.0074, 0.0441 s ---\n",
            "--- Iteration 8906: Training loss = 0.0069, 0.0431 s ---\n",
            "--- Iteration 8907: Training loss = 0.0121, 0.0434 s ---\n",
            "--- Iteration 8908: Training loss = 0.0066, 0.0450 s ---\n",
            "--- Iteration 8909: Training loss = 0.0125, 0.0449 s ---\n",
            "--- Iteration 8910: Training loss = 0.0076, 0.0438 s ---\n",
            "--- Iteration 8910: Test loss = 0.0547 ---\n",
            "\n",
            "--- Iteration 8911: Training loss = 0.0104, 0.0414 s ---\n",
            "--- Iteration 8912: Training loss = 0.0043, 0.0427 s ---\n",
            "--- Iteration 8913: Training loss = 0.0095, 0.0442 s ---\n",
            "--- Iteration 8914: Training loss = 0.0073, 0.0433 s ---\n",
            "--- Iteration 8915: Training loss = 0.0070, 0.0441 s ---\n",
            "--- Iteration 8916: Training loss = 0.0081, 0.0447 s ---\n",
            "--- Iteration 8917: Training loss = 0.0071, 0.0453 s ---\n",
            "--- Iteration 8918: Training loss = 0.0079, 0.0430 s ---\n",
            "--- Iteration 8919: Training loss = 0.0096, 0.0431 s ---\n",
            "--- Iteration 8920: Training loss = 0.0064, 0.0437 s ---\n",
            "--- Iteration 8920: Test loss = 0.0204 ---\n",
            "\n",
            "--- Iteration 8921: Training loss = 0.0091, 0.0428 s ---\n",
            "--- Iteration 8922: Training loss = 0.0080, 0.0442 s ---\n",
            "--- Iteration 8923: Training loss = 0.0104, 0.0449 s ---\n",
            "--- Iteration 8924: Training loss = 0.0126, 0.0432 s ---\n",
            "--- Iteration 8925: Training loss = 0.0093, 0.0453 s ---\n",
            "--- Iteration 8926: Training loss = 0.0076, 0.0440 s ---\n",
            "--- Iteration 8927: Training loss = 0.0097, 0.0455 s ---\n",
            "--- Iteration 8928: Training loss = 0.0084, 0.0441 s ---\n",
            "--- Iteration 8929: Training loss = 0.0067, 0.0432 s ---\n",
            "--- Iteration 8930: Training loss = 0.0099, 0.0428 s ---\n",
            "--- Iteration 8930: Test loss = 0.0432 ---\n",
            "\n",
            "--- Iteration 8931: Training loss = 0.0084, 0.0419 s ---\n",
            "--- Iteration 8932: Training loss = 0.0090, 0.0437 s ---\n",
            "--- Iteration 8933: Training loss = 0.0114, 0.0457 s ---\n",
            "--- Iteration 8934: Training loss = 0.0093, 0.0466 s ---\n",
            "--- Iteration 8935: Training loss = 0.0116, 0.0478 s ---\n",
            "--- Iteration 8936: Training loss = 0.0123, 0.0438 s ---\n",
            "--- Iteration 8937: Training loss = 0.0078, 0.0445 s ---\n",
            "--- Iteration 8938: Training loss = 0.0116, 0.0461 s ---\n",
            "--- Iteration 8939: Training loss = 0.0056, 0.0436 s ---\n",
            "--- Iteration 8940: Training loss = 0.0086, 0.0427 s ---\n",
            "--- Iteration 8940: Test loss = 0.0363 ---\n",
            "\n",
            "--- Iteration 8941: Training loss = 0.0090, 0.0412 s ---\n",
            "--- Iteration 8942: Training loss = 0.0078, 0.0424 s ---\n",
            "--- Iteration 8943: Training loss = 0.0102, 0.0434 s ---\n",
            "--- Iteration 8944: Training loss = 0.0104, 0.0442 s ---\n",
            "--- Iteration 8945: Training loss = 0.0109, 0.0449 s ---\n",
            "--- Iteration 8946: Training loss = 0.0120, 0.0481 s ---\n",
            "--- Iteration 8947: Training loss = 0.0061, 0.0424 s ---\n",
            "--- Iteration 8948: Training loss = 0.0069, 0.0429 s ---\n",
            "--- Iteration 8949: Training loss = 0.0087, 0.0466 s ---\n",
            "--- Iteration 8950: Training loss = 0.0067, 0.0436 s ---\n",
            "--- Iteration 8950: Test loss = 0.0646 ---\n",
            "\n",
            "--- Iteration 8951: Training loss = 0.0077, 0.0426 s ---\n",
            "--- Iteration 8952: Training loss = 0.0106, 0.0433 s ---\n",
            "--- Iteration 8953: Training loss = 0.0092, 0.0458 s ---\n",
            "--- Iteration 8954: Training loss = 0.0068, 0.0429 s ---\n",
            "--- Iteration 8955: Training loss = 0.0084, 0.0438 s ---\n",
            "--- Iteration 8956: Training loss = 0.0139, 0.0449 s ---\n",
            "--- Iteration 8957: Training loss = 0.0084, 0.0443 s ---\n",
            "--- Iteration 8958: Training loss = 0.0112, 0.0430 s ---\n",
            "--- Iteration 8959: Training loss = 0.0070, 0.0421 s ---\n",
            "--- Iteration 8960: Training loss = 0.0085, 0.0433 s ---\n",
            "--- Iteration 8960: Test loss = 0.0437 ---\n",
            "\n",
            "--- Iteration 8961: Training loss = 0.0096, 0.0421 s ---\n",
            "--- Iteration 8962: Training loss = 0.0093, 0.0425 s ---\n",
            "--- Iteration 8963: Training loss = 0.0096, 0.0432 s ---\n",
            "--- Iteration 8964: Training loss = 0.0087, 0.0451 s ---\n",
            "--- Iteration 8965: Training loss = 0.0094, 0.0438 s ---\n",
            "--- Iteration 8966: Training loss = 0.0089, 0.0442 s ---\n",
            "--- Iteration 8967: Training loss = 0.0104, 0.0422 s ---\n",
            "--- Iteration 8968: Training loss = 0.0079, 0.0427 s ---\n",
            "--- Iteration 8969: Training loss = 0.0093, 0.0444 s ---\n",
            "--- Iteration 8970: Training loss = 0.0088, 0.0438 s ---\n",
            "--- Iteration 8970: Test loss = 0.0318 ---\n",
            "\n",
            "--- Iteration 8971: Training loss = 0.0111, 0.0433 s ---\n",
            "--- Iteration 8972: Training loss = 0.0079, 0.0432 s ---\n",
            "--- Iteration 8973: Training loss = 0.0112, 0.0443 s ---\n",
            "--- Iteration 8974: Training loss = 0.0071, 0.0426 s ---\n",
            "--- Iteration 8975: Training loss = 0.0149, 0.0427 s ---\n",
            "--- Iteration 8976: Training loss = 0.0089, 0.0424 s ---\n",
            "--- Iteration 8977: Training loss = 0.0107, 0.0439 s ---\n",
            "--- Iteration 8978: Training loss = 0.0087, 0.0463 s ---\n",
            "--- Iteration 8979: Training loss = 0.0082, 0.0432 s ---\n",
            "--- Iteration 8980: Training loss = 0.0113, 0.0428 s ---\n",
            "--- Iteration 8980: Test loss = 0.0338 ---\n",
            "\n",
            "--- Iteration 8981: Training loss = 0.0085, 0.0448 s ---\n",
            "--- Iteration 8982: Training loss = 0.0087, 0.0414 s ---\n",
            "--- Iteration 8983: Training loss = 0.0069, 0.0444 s ---\n",
            "--- Iteration 8984: Training loss = 0.0105, 0.0445 s ---\n",
            "--- Iteration 8985: Training loss = 0.0094, 0.0442 s ---\n",
            "--- Iteration 8986: Training loss = 0.0090, 0.0434 s ---\n",
            "--- Iteration 8987: Training loss = 0.0067, 0.0432 s ---\n",
            "--- Iteration 8988: Training loss = 0.0129, 0.0426 s ---\n",
            "--- Iteration 8989: Training loss = 0.0061, 0.0452 s ---\n",
            "--- Iteration 8990: Training loss = 0.0113, 0.0478 s ---\n",
            "--- Iteration 8990: Test loss = 0.0399 ---\n",
            "\n",
            "--- Iteration 8991: Training loss = 0.0071, 0.0424 s ---\n",
            "--- Iteration 8992: Training loss = 0.0061, 0.0430 s ---\n",
            "--- Iteration 8993: Training loss = 0.0048, 0.0476 s ---\n",
            "--- Iteration 8994: Training loss = 0.0084, 0.0430 s ---\n",
            "--- Iteration 8995: Training loss = 0.0063, 0.0430 s ---\n",
            "--- Iteration 8996: Training loss = 0.0072, 0.0446 s ---\n",
            "--- Iteration 8997: Training loss = 0.0096, 0.0484 s ---\n",
            "--- Iteration 8998: Training loss = 0.0123, 0.0418 s ---\n",
            "--- Iteration 8999: Training loss = 0.0111, 0.0459 s ---\n",
            "--- Iteration 9000: Training loss = 0.0082, 0.0435 s ---\n",
            "--- Iteration 9000: Test loss = 0.0475 ---\n",
            "\n",
            "--- Iteration 9001: Training loss = 0.0115, 0.0436 s ---\n",
            "--- Iteration 9002: Training loss = 0.0132, 0.0444 s ---\n",
            "--- Iteration 9003: Training loss = 0.0069, 0.0463 s ---\n",
            "--- Iteration 9004: Training loss = 0.0077, 0.0430 s ---\n",
            "--- Iteration 9005: Training loss = 0.0090, 0.0435 s ---\n",
            "--- Iteration 9006: Training loss = 0.0070, 0.0448 s ---\n",
            "--- Iteration 9007: Training loss = 0.0118, 0.0442 s ---\n",
            "--- Iteration 9008: Training loss = 0.0102, 0.0431 s ---\n",
            "--- Iteration 9009: Training loss = 0.0084, 0.0429 s ---\n",
            "--- Iteration 9010: Training loss = 0.0063, 0.0422 s ---\n",
            "--- Iteration 9010: Test loss = 0.0504 ---\n",
            "\n",
            "--- Iteration 9011: Training loss = 0.0118, 0.0412 s ---\n",
            "--- Iteration 9012: Training loss = 0.0089, 0.0430 s ---\n",
            "--- Iteration 9013: Training loss = 0.0125, 0.0430 s ---\n",
            "--- Iteration 9014: Training loss = 0.0071, 0.0434 s ---\n",
            "--- Iteration 9015: Training loss = 0.0111, 0.0447 s ---\n",
            "--- Iteration 9016: Training loss = 0.0089, 0.0444 s ---\n",
            "--- Iteration 9017: Training loss = 0.0041, 0.0432 s ---\n",
            "--- Iteration 9018: Training loss = 0.0097, 0.0423 s ---\n",
            "--- Iteration 9019: Training loss = 0.0104, 0.0434 s ---\n",
            "--- Iteration 9020: Training loss = 0.0087, 0.0430 s ---\n",
            "--- Iteration 9020: Test loss = 0.0499 ---\n",
            "\n",
            "--- Iteration 9021: Training loss = 0.0078, 0.0425 s ---\n",
            "--- Iteration 9022: Training loss = 0.0091, 0.0442 s ---\n",
            "--- Iteration 9023: Training loss = 0.0099, 0.0455 s ---\n",
            "--- Iteration 9024: Training loss = 0.0129, 0.0432 s ---\n",
            "--- Iteration 9025: Training loss = 0.0087, 0.0455 s ---\n",
            "--- Iteration 9026: Training loss = 0.0120, 0.0433 s ---\n",
            "--- Iteration 9027: Training loss = 0.0085, 0.0434 s ---\n",
            "--- Iteration 9028: Training loss = 0.0080, 0.0447 s ---\n",
            "--- Iteration 9029: Training loss = 0.0099, 0.0462 s ---\n",
            "--- Iteration 9030: Training loss = 0.0095, 0.0427 s ---\n",
            "--- Iteration 9030: Test loss = 0.0252 ---\n",
            "\n",
            "--- Iteration 9031: Training loss = 0.0122, 0.0402 s ---\n",
            "--- Iteration 9032: Training loss = 0.0136, 0.0419 s ---\n",
            "--- Iteration 9033: Training loss = 0.0132, 0.0423 s ---\n",
            "--- Iteration 9034: Training loss = 0.0116, 0.0433 s ---\n",
            "--- Iteration 9035: Training loss = 0.0094, 0.0430 s ---\n",
            "--- Iteration 9036: Training loss = 0.0076, 0.0443 s ---\n",
            "--- Iteration 9037: Training loss = 0.0104, 0.0451 s ---\n",
            "--- Iteration 9038: Training loss = 0.0124, 0.0434 s ---\n",
            "--- Iteration 9039: Training loss = 0.0045, 0.0434 s ---\n",
            "--- Iteration 9040: Training loss = 0.0085, 0.0425 s ---\n",
            "--- Iteration 9040: Test loss = 0.0793 ---\n",
            "\n",
            "--- Iteration 9041: Training loss = 0.0073, 0.0414 s ---\n",
            "--- Iteration 9042: Training loss = 0.0064, 0.0429 s ---\n",
            "--- Iteration 9043: Training loss = 0.0095, 0.0439 s ---\n",
            "--- Iteration 9044: Training loss = 0.0136, 0.0447 s ---\n",
            "--- Iteration 9045: Training loss = 0.0117, 0.0436 s ---\n",
            "--- Iteration 9046: Training loss = 0.0095, 0.0437 s ---\n",
            "--- Iteration 9047: Training loss = 0.0112, 0.0426 s ---\n",
            "--- Iteration 9048: Training loss = 0.0093, 0.0433 s ---\n",
            "--- Iteration 9049: Training loss = 0.0112, 0.0454 s ---\n",
            "--- Iteration 9050: Training loss = 0.0071, 0.0452 s ---\n",
            "--- Iteration 9050: Test loss = 0.0250 ---\n",
            "\n",
            "--- Iteration 9051: Training loss = 0.0110, 0.0430 s ---\n",
            "--- Iteration 9052: Training loss = 0.0110, 0.0427 s ---\n",
            "--- Iteration 9053: Training loss = 0.0098, 0.0468 s ---\n",
            "--- Iteration 9054: Training loss = 0.0068, 0.0433 s ---\n",
            "--- Iteration 9055: Training loss = 0.0090, 0.0433 s ---\n",
            "--- Iteration 9056: Training loss = 0.0072, 0.0439 s ---\n",
            "--- Iteration 9057: Training loss = 0.0130, 0.0442 s ---\n",
            "--- Iteration 9058: Training loss = 0.0121, 0.0435 s ---\n",
            "--- Iteration 9059: Training loss = 0.0102, 0.0436 s ---\n",
            "--- Iteration 9060: Training loss = 0.0093, 0.0428 s ---\n",
            "--- Iteration 9060: Test loss = 0.0389 ---\n",
            "\n",
            "--- Iteration 9061: Training loss = 0.0135, 0.0418 s ---\n",
            "--- Iteration 9062: Training loss = 0.0109, 0.0441 s ---\n",
            "--- Iteration 9063: Training loss = 0.0050, 0.0486 s ---\n",
            "--- Iteration 9064: Training loss = 0.0111, 0.0432 s ---\n",
            "--- Iteration 9065: Training loss = 0.0073, 0.0438 s ---\n",
            "--- Iteration 9066: Training loss = 0.0085, 0.0436 s ---\n",
            "--- Iteration 9067: Training loss = 0.0105, 0.0446 s ---\n",
            "--- Iteration 9068: Training loss = 0.0052, 0.0445 s ---\n",
            "--- Iteration 9069: Training loss = 0.0057, 0.0419 s ---\n",
            "--- Iteration 9070: Training loss = 0.0066, 0.0419 s ---\n",
            "--- Iteration 9070: Test loss = 0.0750 ---\n",
            "\n",
            "--- Iteration 9071: Training loss = 0.0065, 0.0407 s ---\n",
            "--- Iteration 9072: Training loss = 0.0093, 0.0430 s ---\n",
            "--- Iteration 9073: Training loss = 0.0069, 0.0424 s ---\n",
            "--- Iteration 9074: Training loss = 0.0088, 0.0424 s ---\n",
            "--- Iteration 9075: Training loss = 0.0085, 0.0428 s ---\n",
            "--- Iteration 9076: Training loss = 0.0100, 0.0448 s ---\n",
            "--- Iteration 9077: Training loss = 0.0075, 0.0445 s ---\n",
            "--- Iteration 9078: Training loss = 0.0085, 0.0439 s ---\n",
            "--- Iteration 9079: Training loss = 0.0095, 0.0430 s ---\n",
            "--- Iteration 9080: Training loss = 0.0117, 0.0426 s ---\n",
            "--- Iteration 9080: Test loss = 0.0296 ---\n",
            "\n",
            "--- Iteration 9081: Training loss = 0.0088, 0.0419 s ---\n",
            "--- Iteration 9082: Training loss = 0.0110, 0.0427 s ---\n",
            "--- Iteration 9083: Training loss = 0.0120, 0.0425 s ---\n",
            "--- Iteration 9084: Training loss = 0.0093, 0.0437 s ---\n",
            "--- Iteration 9085: Training loss = 0.0098, 0.0451 s ---\n",
            "--- Iteration 9086: Training loss = 0.0082, 0.0433 s ---\n",
            "--- Iteration 9087: Training loss = 0.0127, 0.0428 s ---\n",
            "--- Iteration 9088: Training loss = 0.0101, 0.0420 s ---\n",
            "--- Iteration 9089: Training loss = 0.0086, 0.0417 s ---\n",
            "--- Iteration 9090: Training loss = 0.0052, 0.0424 s ---\n",
            "--- Iteration 9090: Test loss = 0.0201 ---\n",
            "\n",
            "--- Iteration 9091: Training loss = 0.0105, 0.0414 s ---\n",
            "--- Iteration 9092: Training loss = 0.0068, 0.0420 s ---\n",
            "--- Iteration 9093: Training loss = 0.0128, 0.0434 s ---\n",
            "--- Iteration 9094: Training loss = 0.0110, 0.0448 s ---\n",
            "--- Iteration 9095: Training loss = 0.0081, 0.0446 s ---\n",
            "--- Iteration 9096: Training loss = 0.0093, 0.0434 s ---\n",
            "--- Iteration 9097: Training loss = 0.0112, 0.0426 s ---\n",
            "--- Iteration 9098: Training loss = 0.0070, 0.0421 s ---\n",
            "--- Iteration 9099: Training loss = 0.0061, 0.0434 s ---\n",
            "--- Iteration 9100: Training loss = 0.0131, 0.0438 s ---\n",
            "--- Iteration 9100: Test loss = 0.0338 ---\n",
            "\n",
            "--- Iteration 9101: Training loss = 0.0083, 0.0442 s ---\n",
            "--- Iteration 9102: Training loss = 0.0114, 0.0440 s ---\n",
            "--- Iteration 9103: Training loss = 0.0143, 0.0496 s ---\n",
            "--- Iteration 9104: Training loss = 0.0054, 0.0454 s ---\n",
            "--- Iteration 9105: Training loss = 0.0089, 0.0439 s ---\n",
            "--- Iteration 9106: Training loss = 0.0112, 0.0428 s ---\n",
            "--- Iteration 9107: Training loss = 0.0083, 0.0436 s ---\n",
            "--- Iteration 9108: Training loss = 0.0081, 0.0430 s ---\n",
            "--- Iteration 9109: Training loss = 0.0064, 0.0469 s ---\n",
            "--- Iteration 9110: Training loss = 0.0085, 0.0443 s ---\n",
            "--- Iteration 9110: Test loss = 0.0396 ---\n",
            "\n",
            "--- Iteration 9111: Training loss = 0.0125, 0.0422 s ---\n",
            "--- Iteration 9112: Training loss = 0.0069, 0.0430 s ---\n",
            "--- Iteration 9113: Training loss = 0.0074, 0.0468 s ---\n",
            "--- Iteration 9114: Training loss = 0.0099, 0.0424 s ---\n",
            "--- Iteration 9115: Training loss = 0.0068, 0.0427 s ---\n",
            "--- Iteration 9116: Training loss = 0.0092, 0.0443 s ---\n",
            "--- Iteration 9117: Training loss = 0.0085, 0.0465 s ---\n",
            "--- Iteration 9118: Training loss = 0.0082, 0.0423 s ---\n",
            "--- Iteration 9119: Training loss = 0.0121, 0.0439 s ---\n",
            "--- Iteration 9120: Training loss = 0.0093, 0.0434 s ---\n",
            "--- Iteration 9120: Test loss = 0.0190 ---\n",
            "\n",
            "--- Iteration 9121: Training loss = 0.0134, 0.0424 s ---\n",
            "--- Iteration 9122: Training loss = 0.0076, 0.0433 s ---\n",
            "--- Iteration 9123: Training loss = 0.0107, 0.0466 s ---\n",
            "--- Iteration 9124: Training loss = 0.0104, 0.0450 s ---\n",
            "--- Iteration 9125: Training loss = 0.0081, 0.0424 s ---\n",
            "--- Iteration 9126: Training loss = 0.0074, 0.0428 s ---\n",
            "--- Iteration 9127: Training loss = 0.0061, 0.0427 s ---\n",
            "--- Iteration 9128: Training loss = 0.0084, 0.0426 s ---\n",
            "--- Iteration 9129: Training loss = 0.0084, 0.0463 s ---\n",
            "--- Iteration 9130: Training loss = 0.0071, 0.0442 s ---\n",
            "--- Iteration 9130: Test loss = 0.0474 ---\n",
            "\n",
            "--- Iteration 9131: Training loss = 0.0122, 0.0540 s ---\n",
            "--- Iteration 9132: Training loss = 0.0104, 0.0427 s ---\n",
            "--- Iteration 9133: Training loss = 0.0059, 0.0448 s ---\n",
            "--- Iteration 9134: Training loss = 0.0093, 0.0457 s ---\n",
            "--- Iteration 9135: Training loss = 0.0091, 0.0431 s ---\n",
            "--- Iteration 9136: Training loss = 0.0099, 0.0433 s ---\n",
            "--- Iteration 9137: Training loss = 0.0111, 0.0432 s ---\n",
            "--- Iteration 9138: Training loss = 0.0106, 0.0437 s ---\n",
            "--- Iteration 9139: Training loss = 0.0113, 0.0429 s ---\n",
            "--- Iteration 9140: Training loss = 0.0065, 0.0442 s ---\n",
            "--- Iteration 9140: Test loss = 0.0533 ---\n",
            "\n",
            "--- Iteration 9141: Training loss = 0.0066, 0.0432 s ---\n",
            "--- Iteration 9142: Training loss = 0.0092, 0.0462 s ---\n",
            "--- Iteration 9143: Training loss = 0.0060, 0.0433 s ---\n",
            "--- Iteration 9144: Training loss = 0.0057, 0.0421 s ---\n",
            "--- Iteration 9145: Training loss = 0.0088, 0.0430 s ---\n",
            "--- Iteration 9146: Training loss = 0.0099, 0.0432 s ---\n",
            "--- Iteration 9147: Training loss = 0.0078, 0.0443 s ---\n",
            "--- Iteration 9148: Training loss = 0.0106, 0.0448 s ---\n",
            "--- Iteration 9149: Training loss = 0.0140, 0.0427 s ---\n",
            "--- Iteration 9150: Training loss = 0.0085, 0.0420 s ---\n",
            "--- Iteration 9150: Test loss = 0.0327 ---\n",
            "\n",
            "--- Iteration 9151: Training loss = 0.0072, 0.0411 s ---\n",
            "--- Iteration 9152: Training loss = 0.0098, 0.0452 s ---\n",
            "--- Iteration 9153: Training loss = 0.0103, 0.0428 s ---\n",
            "--- Iteration 9154: Training loss = 0.0059, 0.0438 s ---\n",
            "--- Iteration 9155: Training loss = 0.0097, 0.0454 s ---\n",
            "--- Iteration 9156: Training loss = 0.0064, 0.0446 s ---\n",
            "--- Iteration 9157: Training loss = 0.0042, 0.0428 s ---\n",
            "--- Iteration 9158: Training loss = 0.0069, 0.0437 s ---\n",
            "--- Iteration 9159: Training loss = 0.0094, 0.0429 s ---\n",
            "--- Iteration 9160: Training loss = 0.0090, 0.0432 s ---\n",
            "--- Iteration 9160: Test loss = 0.0547 ---\n",
            "\n",
            "--- Iteration 9161: Training loss = 0.0097, 0.0433 s ---\n",
            "--- Iteration 9162: Training loss = 0.0069, 0.0447 s ---\n",
            "--- Iteration 9163: Training loss = 0.0089, 0.0436 s ---\n",
            "--- Iteration 9164: Training loss = 0.0110, 0.0427 s ---\n",
            "--- Iteration 9165: Training loss = 0.0102, 0.0433 s ---\n",
            "--- Iteration 9166: Training loss = 0.0093, 0.0429 s ---\n",
            "--- Iteration 9167: Training loss = 0.0085, 0.0432 s ---\n",
            "--- Iteration 9168: Training loss = 0.0092, 0.0442 s ---\n",
            "--- Iteration 9169: Training loss = 0.0072, 0.0444 s ---\n",
            "--- Iteration 9170: Training loss = 0.0131, 0.0434 s ---\n",
            "--- Iteration 9170: Test loss = 0.0230 ---\n",
            "\n",
            "--- Iteration 9171: Training loss = 0.0078, 0.0416 s ---\n",
            "--- Iteration 9172: Training loss = 0.0107, 0.0463 s ---\n",
            "--- Iteration 9173: Training loss = 0.0096, 0.0434 s ---\n",
            "--- Iteration 9174: Training loss = 0.0092, 0.0431 s ---\n",
            "--- Iteration 9175: Training loss = 0.0089, 0.0444 s ---\n",
            "--- Iteration 9176: Training loss = 0.0087, 0.0456 s ---\n",
            "--- Iteration 9177: Training loss = 0.0091, 0.0433 s ---\n",
            "--- Iteration 9178: Training loss = 0.0100, 0.0425 s ---\n",
            "--- Iteration 9179: Training loss = 0.0047, 0.0422 s ---\n",
            "--- Iteration 9180: Training loss = 0.0118, 0.0425 s ---\n",
            "--- Iteration 9180: Test loss = 0.0444 ---\n",
            "\n",
            "--- Iteration 9181: Training loss = 0.0064, 0.0414 s ---\n",
            "--- Iteration 9182: Training loss = 0.0103, 0.0424 s ---\n",
            "--- Iteration 9183: Training loss = 0.0077, 0.0438 s ---\n",
            "--- Iteration 9184: Training loss = 0.0088, 0.0442 s ---\n",
            "--- Iteration 9185: Training loss = 0.0059, 0.0440 s ---\n",
            "--- Iteration 9186: Training loss = 0.0097, 0.0434 s ---\n",
            "--- Iteration 9187: Training loss = 0.0080, 0.0437 s ---\n",
            "--- Iteration 9188: Training loss = 0.0063, 0.0423 s ---\n",
            "--- Iteration 9189: Training loss = 0.0091, 0.0436 s ---\n",
            "--- Iteration 9190: Training loss = 0.0121, 0.0446 s ---\n",
            "--- Iteration 9190: Test loss = 0.0541 ---\n",
            "\n",
            "--- Iteration 9191: Training loss = 0.0068, 0.0428 s ---\n",
            "--- Iteration 9192: Training loss = 0.0109, 0.0441 s ---\n",
            "--- Iteration 9193: Training loss = 0.0123, 0.0428 s ---\n",
            "--- Iteration 9194: Training loss = 0.0066, 0.0426 s ---\n",
            "--- Iteration 9195: Training loss = 0.0081, 0.0428 s ---\n",
            "--- Iteration 9196: Training loss = 0.0109, 0.0429 s ---\n",
            "--- Iteration 9197: Training loss = 0.0101, 0.0426 s ---\n",
            "--- Iteration 9198: Training loss = 0.0106, 0.0429 s ---\n",
            "--- Iteration 9199: Training loss = 0.0138, 0.0443 s ---\n",
            "--- Iteration 9200: Training loss = 0.0100, 0.0442 s ---\n",
            "--- Iteration 9200: Test loss = 0.0734 ---\n",
            "\n",
            "--- Iteration 9201: Training loss = 0.0083, 0.0410 s ---\n",
            "--- Iteration 9202: Training loss = 0.0109, 0.0460 s ---\n",
            "--- Iteration 9203: Training loss = 0.0086, 0.0430 s ---\n",
            "--- Iteration 9204: Training loss = 0.0085, 0.0420 s ---\n",
            "--- Iteration 9205: Training loss = 0.0067, 0.0433 s ---\n",
            "--- Iteration 9206: Training loss = 0.0072, 0.0444 s ---\n",
            "--- Iteration 9207: Training loss = 0.0062, 0.0443 s ---\n",
            "--- Iteration 9208: Training loss = 0.0087, 0.0434 s ---\n",
            "--- Iteration 9209: Training loss = 0.0056, 0.0422 s ---\n",
            "--- Iteration 9210: Training loss = 0.0081, 0.0425 s ---\n",
            "--- Iteration 9210: Test loss = 0.0452 ---\n",
            "\n",
            "--- Iteration 9211: Training loss = 0.0086, 0.0416 s ---\n",
            "--- Iteration 9212: Training loss = 0.0092, 0.0470 s ---\n",
            "--- Iteration 9213: Training loss = 0.0076, 0.0439 s ---\n",
            "--- Iteration 9214: Training loss = 0.0147, 0.0445 s ---\n",
            "--- Iteration 9215: Training loss = 0.0111, 0.0437 s ---\n",
            "--- Iteration 9216: Training loss = 0.0100, 0.0429 s ---\n",
            "--- Iteration 9217: Training loss = 0.0111, 0.0424 s ---\n",
            "--- Iteration 9218: Training loss = 0.0057, 0.0426 s ---\n",
            "--- Iteration 9219: Training loss = 0.0071, 0.0430 s ---\n",
            "--- Iteration 9220: Training loss = 0.0096, 0.0429 s ---\n",
            "--- Iteration 9220: Test loss = 0.0290 ---\n",
            "\n",
            "--- Iteration 9221: Training loss = 0.0156, 0.0428 s ---\n",
            "--- Iteration 9222: Training loss = 0.0148, 0.0482 s ---\n",
            "--- Iteration 9223: Training loss = 0.0094, 0.0443 s ---\n",
            "--- Iteration 9224: Training loss = 0.0110, 0.0427 s ---\n",
            "--- Iteration 9225: Training loss = 0.0103, 0.0435 s ---\n",
            "--- Iteration 9226: Training loss = 0.0083, 0.0447 s ---\n",
            "--- Iteration 9227: Training loss = 0.0080, 0.0446 s ---\n",
            "--- Iteration 9228: Training loss = 0.0075, 0.0431 s ---\n",
            "--- Iteration 9229: Training loss = 0.0059, 0.0423 s ---\n",
            "--- Iteration 9230: Training loss = 0.0072, 0.0424 s ---\n",
            "--- Iteration 9230: Test loss = 0.0306 ---\n",
            "\n",
            "--- Iteration 9231: Training loss = 0.0125, 0.0418 s ---\n",
            "--- Iteration 9232: Training loss = 0.0084, 0.0427 s ---\n",
            "--- Iteration 9233: Training loss = 0.0099, 0.0438 s ---\n",
            "--- Iteration 9234: Training loss = 0.0060, 0.0448 s ---\n",
            "--- Iteration 9235: Training loss = 0.0046, 0.0444 s ---\n",
            "--- Iteration 9236: Training loss = 0.0069, 0.0426 s ---\n",
            "--- Iteration 9237: Training loss = 0.0090, 0.0420 s ---\n",
            "--- Iteration 9238: Training loss = 0.0078, 0.0426 s ---\n",
            "--- Iteration 9239: Training loss = 0.0057, 0.0424 s ---\n",
            "--- Iteration 9240: Training loss = 0.0117, 0.0429 s ---\n",
            "--- Iteration 9240: Test loss = 0.0505 ---\n",
            "\n",
            "--- Iteration 9241: Training loss = 0.0088, 0.0429 s ---\n",
            "--- Iteration 9242: Training loss = 0.0066, 0.0467 s ---\n",
            "--- Iteration 9243: Training loss = 0.0082, 0.0478 s ---\n",
            "--- Iteration 9244: Training loss = 0.0083, 0.0442 s ---\n",
            "--- Iteration 9245: Training loss = 0.0074, 0.0440 s ---\n",
            "--- Iteration 9246: Training loss = 0.0046, 0.0443 s ---\n",
            "--- Iteration 9247: Training loss = 0.0076, 0.0440 s ---\n",
            "--- Iteration 9248: Training loss = 0.0090, 0.0428 s ---\n",
            "--- Iteration 9249: Training loss = 0.0052, 0.0428 s ---\n",
            "--- Iteration 9250: Training loss = 0.0082, 0.0426 s ---\n",
            "--- Iteration 9250: Test loss = 0.0484 ---\n",
            "\n",
            "--- Iteration 9251: Training loss = 0.0066, 0.0415 s ---\n",
            "--- Iteration 9252: Training loss = 0.0110, 0.0422 s ---\n",
            "--- Iteration 9253: Training loss = 0.0072, 0.0432 s ---\n",
            "--- Iteration 9254: Training loss = 0.0068, 0.0433 s ---\n",
            "--- Iteration 9255: Training loss = 0.0061, 0.0446 s ---\n",
            "--- Iteration 9256: Training loss = 0.0087, 0.0444 s ---\n",
            "--- Iteration 9257: Training loss = 0.0095, 0.0420 s ---\n",
            "--- Iteration 9258: Training loss = 0.0093, 0.0422 s ---\n",
            "--- Iteration 9259: Training loss = 0.0114, 0.0439 s ---\n",
            "--- Iteration 9260: Training loss = 0.0054, 0.0439 s ---\n",
            "--- Iteration 9260: Test loss = 0.0364 ---\n",
            "\n",
            "--- Iteration 9261: Training loss = 0.0107, 0.0434 s ---\n",
            "--- Iteration 9262: Training loss = 0.0059, 0.0500 s ---\n",
            "--- Iteration 9263: Training loss = 0.0090, 0.0434 s ---\n",
            "--- Iteration 9264: Training loss = 0.0074, 0.0437 s ---\n",
            "--- Iteration 9265: Training loss = 0.0114, 0.0499 s ---\n",
            "--- Iteration 9266: Training loss = 0.0114, 0.0457 s ---\n",
            "--- Iteration 9267: Training loss = 0.0104, 0.0431 s ---\n",
            "--- Iteration 9268: Training loss = 0.0105, 0.0442 s ---\n",
            "--- Iteration 9269: Training loss = 0.0108, 0.0449 s ---\n",
            "--- Iteration 9270: Training loss = 0.0092, 0.0462 s ---\n",
            "--- Iteration 9270: Test loss = 0.0332 ---\n",
            "\n",
            "--- Iteration 9271: Training loss = 0.0096, 0.0425 s ---\n",
            "--- Iteration 9272: Training loss = 0.0087, 0.0474 s ---\n",
            "--- Iteration 9273: Training loss = 0.0118, 0.0434 s ---\n",
            "--- Iteration 9274: Training loss = 0.0159, 0.0441 s ---\n",
            "--- Iteration 9275: Training loss = 0.0089, 0.0447 s ---\n",
            "--- Iteration 9276: Training loss = 0.0067, 0.0433 s ---\n",
            "--- Iteration 9277: Training loss = 0.0103, 0.0431 s ---\n",
            "--- Iteration 9278: Training loss = 0.0089, 0.0429 s ---\n",
            "--- Iteration 9279: Training loss = 0.0071, 0.0430 s ---\n",
            "--- Iteration 9280: Training loss = 0.0114, 0.0463 s ---\n",
            "--- Iteration 9280: Test loss = 0.0713 ---\n",
            "\n",
            "--- Iteration 9281: Training loss = 0.0100, 0.0434 s ---\n",
            "--- Iteration 9282: Training loss = 0.0080, 0.0457 s ---\n",
            "--- Iteration 9283: Training loss = 0.0055, 0.0419 s ---\n",
            "--- Iteration 9284: Training loss = 0.0031, 0.0426 s ---\n",
            "--- Iteration 9285: Training loss = 0.0148, 0.0431 s ---\n",
            "--- Iteration 9286: Training loss = 0.0064, 0.0438 s ---\n",
            "--- Iteration 9287: Training loss = 0.0086, 0.0445 s ---\n",
            "--- Iteration 9288: Training loss = 0.0135, 0.0442 s ---\n",
            "--- Iteration 9289: Training loss = 0.0126, 0.0436 s ---\n",
            "--- Iteration 9290: Training loss = 0.0063, 0.0420 s ---\n",
            "--- Iteration 9290: Test loss = 0.0441 ---\n",
            "\n",
            "--- Iteration 9291: Training loss = 0.0077, 0.0406 s ---\n",
            "--- Iteration 9292: Training loss = 0.0076, 0.0458 s ---\n",
            "--- Iteration 9293: Training loss = 0.0123, 0.0426 s ---\n",
            "--- Iteration 9294: Training loss = 0.0077, 0.0426 s ---\n",
            "--- Iteration 9295: Training loss = 0.0075, 0.0444 s ---\n",
            "--- Iteration 9296: Training loss = 0.0060, 0.0446 s ---\n",
            "--- Iteration 9297: Training loss = 0.0056, 0.0423 s ---\n",
            "--- Iteration 9298: Training loss = 0.0070, 0.0430 s ---\n",
            "--- Iteration 9299: Training loss = 0.0101, 0.0425 s ---\n",
            "--- Iteration 9300: Training loss = 0.0109, 0.0427 s ---\n",
            "--- Iteration 9300: Test loss = 0.0504 ---\n",
            "\n",
            "--- Iteration 9301: Training loss = 0.0086, 0.0412 s ---\n",
            "--- Iteration 9302: Training loss = 0.0146, 0.0432 s ---\n",
            "--- Iteration 9303: Training loss = 0.0118, 0.0434 s ---\n",
            "--- Iteration 9304: Training loss = 0.0164, 0.0439 s ---\n",
            "--- Iteration 9305: Training loss = 0.0104, 0.0439 s ---\n",
            "--- Iteration 9306: Training loss = 0.0115, 0.0439 s ---\n",
            "--- Iteration 9307: Training loss = 0.0074, 0.0425 s ---\n",
            "--- Iteration 9308: Training loss = 0.0094, 0.0418 s ---\n",
            "--- Iteration 9309: Training loss = 0.0069, 0.0421 s ---\n",
            "--- Iteration 9310: Training loss = 0.0090, 0.0473 s ---\n",
            "--- Iteration 9310: Test loss = 0.0594 ---\n",
            "\n",
            "--- Iteration 9311: Training loss = 0.0087, 0.0436 s ---\n",
            "--- Iteration 9312: Training loss = 0.0127, 0.0507 s ---\n",
            "--- Iteration 9313: Training loss = 0.0099, 0.0411 s ---\n",
            "--- Iteration 9314: Training loss = 0.0079, 0.0433 s ---\n",
            "--- Iteration 9315: Training loss = 0.0089, 0.0433 s ---\n",
            "--- Iteration 9316: Training loss = 0.0098, 0.0441 s ---\n",
            "--- Iteration 9317: Training loss = 0.0078, 0.0475 s ---\n",
            "--- Iteration 9318: Training loss = 0.0111, 0.0425 s ---\n",
            "--- Iteration 9319: Training loss = 0.0081, 0.0416 s ---\n",
            "--- Iteration 9320: Training loss = 0.0088, 0.0421 s ---\n",
            "--- Iteration 9320: Test loss = 0.0596 ---\n",
            "\n",
            "--- Iteration 9321: Training loss = 0.0070, 0.0417 s ---\n",
            "--- Iteration 9322: Training loss = 0.0087, 0.0421 s ---\n",
            "--- Iteration 9323: Training loss = 0.0112, 0.0427 s ---\n",
            "--- Iteration 9324: Training loss = 0.0095, 0.0421 s ---\n",
            "--- Iteration 9325: Training loss = 0.0060, 0.0441 s ---\n",
            "--- Iteration 9326: Training loss = 0.0117, 0.0450 s ---\n",
            "--- Iteration 9327: Training loss = 0.0081, 0.0436 s ---\n",
            "--- Iteration 9328: Training loss = 0.0107, 0.0422 s ---\n",
            "--- Iteration 9329: Training loss = 0.0085, 0.0420 s ---\n",
            "--- Iteration 9330: Training loss = 0.0066, 0.0434 s ---\n",
            "--- Iteration 9330: Test loss = 0.0407 ---\n",
            "\n",
            "--- Iteration 9331: Training loss = 0.0074, 0.0435 s ---\n",
            "--- Iteration 9332: Training loss = 0.0097, 0.0493 s ---\n",
            "--- Iteration 9333: Training loss = 0.0092, 0.0440 s ---\n",
            "--- Iteration 9334: Training loss = 0.0096, 0.0430 s ---\n",
            "--- Iteration 9335: Training loss = 0.0074, 0.0429 s ---\n",
            "--- Iteration 9336: Training loss = 0.0149, 0.0424 s ---\n",
            "--- Iteration 9337: Training loss = 0.0132, 0.0438 s ---\n",
            "--- Iteration 9338: Training loss = 0.0083, 0.0440 s ---\n",
            "--- Iteration 9339: Training loss = 0.0063, 0.0447 s ---\n",
            "--- Iteration 9340: Training loss = 0.0093, 0.0437 s ---\n",
            "--- Iteration 9340: Test loss = 0.0348 ---\n",
            "\n",
            "--- Iteration 9341: Training loss = 0.0053, 0.0406 s ---\n",
            "--- Iteration 9342: Training loss = 0.0118, 0.0439 s ---\n",
            "--- Iteration 9343: Training loss = 0.0099, 0.0431 s ---\n",
            "--- Iteration 9344: Training loss = 0.0085, 0.0425 s ---\n",
            "--- Iteration 9345: Training loss = 0.0129, 0.0429 s ---\n",
            "--- Iteration 9346: Training loss = 0.0079, 0.0427 s ---\n",
            "--- Iteration 9347: Training loss = 0.0058, 0.0443 s ---\n",
            "--- Iteration 9348: Training loss = 0.0103, 0.0431 s ---\n",
            "--- Iteration 9349: Training loss = 0.0093, 0.0431 s ---\n",
            "--- Iteration 9350: Training loss = 0.0070, 0.0425 s ---\n",
            "--- Iteration 9350: Test loss = 0.0398 ---\n",
            "\n",
            "--- Iteration 9351: Training loss = 0.0062, 0.0411 s ---\n",
            "--- Iteration 9352: Training loss = 0.0076, 0.0424 s ---\n",
            "--- Iteration 9353: Training loss = 0.0095, 0.0426 s ---\n",
            "--- Iteration 9354: Training loss = 0.0073, 0.0441 s ---\n",
            "--- Iteration 9355: Training loss = 0.0081, 0.0435 s ---\n",
            "--- Iteration 9356: Training loss = 0.0070, 0.0449 s ---\n",
            "--- Iteration 9357: Training loss = 0.0077, 0.0444 s ---\n",
            "--- Iteration 9358: Training loss = 0.0048, 0.0438 s ---\n",
            "--- Iteration 9359: Training loss = 0.0120, 0.0432 s ---\n",
            "--- Iteration 9360: Training loss = 0.0076, 0.0437 s ---\n",
            "--- Iteration 9360: Test loss = 0.0428 ---\n",
            "\n",
            "--- Iteration 9361: Training loss = 0.0089, 0.0447 s ---\n",
            "--- Iteration 9362: Training loss = 0.0057, 0.0505 s ---\n",
            "--- Iteration 9363: Training loss = 0.0120, 0.0480 s ---\n",
            "--- Iteration 9364: Training loss = 0.0059, 0.0466 s ---\n",
            "--- Iteration 9365: Training loss = 0.0064, 0.0428 s ---\n",
            "--- Iteration 9366: Training loss = 0.0084, 0.0427 s ---\n",
            "--- Iteration 9367: Training loss = 0.0096, 0.0431 s ---\n",
            "--- Iteration 9368: Training loss = 0.0101, 0.0444 s ---\n",
            "--- Iteration 9369: Training loss = 0.0091, 0.0452 s ---\n",
            "--- Iteration 9370: Training loss = 0.0082, 0.0435 s ---\n",
            "--- Iteration 9370: Test loss = 0.0371 ---\n",
            "\n",
            "--- Iteration 9371: Training loss = 0.0083, 0.0416 s ---\n",
            "--- Iteration 9372: Training loss = 0.0077, 0.0463 s ---\n",
            "--- Iteration 9373: Training loss = 0.0102, 0.0419 s ---\n",
            "--- Iteration 9374: Training loss = 0.0083, 0.0429 s ---\n",
            "--- Iteration 9375: Training loss = 0.0092, 0.0435 s ---\n",
            "--- Iteration 9376: Training loss = 0.0098, 0.0461 s ---\n",
            "--- Iteration 9377: Training loss = 0.0127, 0.0432 s ---\n",
            "--- Iteration 9378: Training loss = 0.0101, 0.0434 s ---\n",
            "--- Iteration 9379: Training loss = 0.0106, 0.0430 s ---\n",
            "--- Iteration 9380: Training loss = 0.0113, 0.0437 s ---\n",
            "--- Iteration 9380: Test loss = 0.0572 ---\n",
            "\n",
            "--- Iteration 9381: Training loss = 0.0108, 0.0426 s ---\n",
            "--- Iteration 9382: Training loss = 0.0080, 0.0437 s ---\n",
            "--- Iteration 9383: Training loss = 0.0062, 0.0452 s ---\n",
            "--- Iteration 9384: Training loss = 0.0089, 0.0441 s ---\n",
            "--- Iteration 9385: Training loss = 0.0081, 0.0435 s ---\n",
            "--- Iteration 9386: Training loss = 0.0109, 0.0421 s ---\n",
            "--- Iteration 9387: Training loss = 0.0067, 0.0421 s ---\n",
            "--- Iteration 9388: Training loss = 0.0089, 0.0452 s ---\n",
            "--- Iteration 9389: Training loss = 0.0124, 0.0451 s ---\n",
            "--- Iteration 9390: Training loss = 0.0098, 0.0444 s ---\n",
            "--- Iteration 9390: Test loss = 0.0399 ---\n",
            "\n",
            "--- Iteration 9391: Training loss = 0.0083, 0.0415 s ---\n",
            "--- Iteration 9392: Training loss = 0.0122, 0.0430 s ---\n",
            "--- Iteration 9393: Training loss = 0.0096, 0.0443 s ---\n",
            "--- Iteration 9394: Training loss = 0.0100, 0.0426 s ---\n",
            "--- Iteration 9395: Training loss = 0.0126, 0.0427 s ---\n",
            "--- Iteration 9396: Training loss = 0.0095, 0.0444 s ---\n",
            "--- Iteration 9397: Training loss = 0.0072, 0.0440 s ---\n",
            "--- Iteration 9398: Training loss = 0.0077, 0.0435 s ---\n",
            "--- Iteration 9399: Training loss = 0.0122, 0.0455 s ---\n",
            "--- Iteration 9400: Training loss = 0.0104, 0.0421 s ---\n",
            "--- Iteration 9400: Test loss = 0.0348 ---\n",
            "\n",
            "--- Iteration 9401: Training loss = 0.0104, 0.0413 s ---\n",
            "--- Iteration 9402: Training loss = 0.0123, 0.0423 s ---\n",
            "--- Iteration 9403: Training loss = 0.0050, 0.0420 s ---\n",
            "--- Iteration 9404: Training loss = 0.0054, 0.0448 s ---\n",
            "--- Iteration 9405: Training loss = 0.0093, 0.0451 s ---\n",
            "--- Iteration 9406: Training loss = 0.0082, 0.0435 s ---\n",
            "--- Iteration 9407: Training loss = 0.0085, 0.0430 s ---\n",
            "--- Iteration 9408: Training loss = 0.0093, 0.0420 s ---\n",
            "--- Iteration 9409: Training loss = 0.0077, 0.0424 s ---\n",
            "--- Iteration 9410: Training loss = 0.0088, 0.0430 s ---\n",
            "--- Iteration 9410: Test loss = 0.0654 ---\n",
            "\n",
            "--- Iteration 9411: Training loss = 0.0115, 0.0420 s ---\n",
            "--- Iteration 9412: Training loss = 0.0095, 0.0430 s ---\n",
            "--- Iteration 9413: Training loss = 0.0085, 0.0452 s ---\n",
            "--- Iteration 9414: Training loss = 0.0087, 0.0440 s ---\n",
            "--- Iteration 9415: Training loss = 0.0094, 0.0449 s ---\n",
            "--- Iteration 9416: Training loss = 0.0115, 0.0430 s ---\n",
            "--- Iteration 9417: Training loss = 0.0083, 0.0425 s ---\n",
            "--- Iteration 9418: Training loss = 0.0093, 0.0425 s ---\n",
            "--- Iteration 9419: Training loss = 0.0105, 0.0433 s ---\n",
            "--- Iteration 9420: Training loss = 0.0085, 0.0453 s ---\n",
            "--- Iteration 9420: Test loss = 0.0293 ---\n",
            "\n",
            "--- Iteration 9421: Training loss = 0.0096, 0.0464 s ---\n",
            "--- Iteration 9422: Training loss = 0.0076, 0.0481 s ---\n",
            "--- Iteration 9423: Training loss = 0.0094, 0.0421 s ---\n",
            "--- Iteration 9424: Training loss = 0.0106, 0.0444 s ---\n",
            "--- Iteration 9425: Training loss = 0.0108, 0.0441 s ---\n",
            "--- Iteration 9426: Training loss = 0.0100, 0.0433 s ---\n",
            "--- Iteration 9427: Training loss = 0.0101, 0.0451 s ---\n",
            "--- Iteration 9428: Training loss = 0.0043, 0.0439 s ---\n",
            "--- Iteration 9429: Training loss = 0.0048, 0.0445 s ---\n",
            "--- Iteration 9430: Training loss = 0.0076, 0.0446 s ---\n",
            "--- Iteration 9430: Test loss = 0.0383 ---\n",
            "\n",
            "--- Iteration 9431: Training loss = 0.0086, 0.0427 s ---\n",
            "--- Iteration 9432: Training loss = 0.0100, 0.0448 s ---\n",
            "--- Iteration 9433: Training loss = 0.0106, 0.0430 s ---\n",
            "--- Iteration 9434: Training loss = 0.0084, 0.0424 s ---\n",
            "--- Iteration 9435: Training loss = 0.0059, 0.0430 s ---\n",
            "--- Iteration 9436: Training loss = 0.0102, 0.0434 s ---\n",
            "--- Iteration 9437: Training loss = 0.0055, 0.0465 s ---\n",
            "--- Iteration 9438: Training loss = 0.0084, 0.0437 s ---\n",
            "--- Iteration 9439: Training loss = 0.0104, 0.0428 s ---\n",
            "--- Iteration 9440: Training loss = 0.0073, 0.0428 s ---\n",
            "--- Iteration 9440: Test loss = 0.0208 ---\n",
            "\n",
            "--- Iteration 9441: Training loss = 0.0107, 0.0414 s ---\n",
            "--- Iteration 9442: Training loss = 0.0044, 0.0429 s ---\n",
            "--- Iteration 9443: Training loss = 0.0090, 0.0426 s ---\n",
            "--- Iteration 9444: Training loss = 0.0136, 0.0437 s ---\n",
            "--- Iteration 9445: Training loss = 0.0083, 0.0448 s ---\n",
            "--- Iteration 9446: Training loss = 0.0102, 0.0443 s ---\n",
            "--- Iteration 9447: Training loss = 0.0089, 0.0429 s ---\n",
            "--- Iteration 9448: Training loss = 0.0086, 0.0433 s ---\n",
            "--- Iteration 9449: Training loss = 0.0107, 0.0496 s ---\n",
            "--- Iteration 9450: Training loss = 0.0106, 0.0447 s ---\n",
            "--- Iteration 9450: Test loss = 0.0488 ---\n",
            "\n",
            "--- Iteration 9451: Training loss = 0.0074, 0.0415 s ---\n",
            "--- Iteration 9452: Training loss = 0.0090, 0.0496 s ---\n",
            "--- Iteration 9453: Training loss = 0.0085, 0.0425 s ---\n",
            "--- Iteration 9454: Training loss = 0.0088, 0.0456 s ---\n",
            "--- Iteration 9455: Training loss = 0.0137, 0.0438 s ---\n",
            "--- Iteration 9456: Training loss = 0.0080, 0.0427 s ---\n",
            "--- Iteration 9457: Training loss = 0.0086, 0.0427 s ---\n",
            "--- Iteration 9458: Training loss = 0.0067, 0.0426 s ---\n",
            "--- Iteration 9459: Training loss = 0.0076, 0.0421 s ---\n",
            "--- Iteration 9460: Training loss = 0.0095, 0.0422 s ---\n",
            "--- Iteration 9460: Test loss = 0.0243 ---\n",
            "\n",
            "--- Iteration 9461: Training loss = 0.0137, 0.0462 s ---\n",
            "--- Iteration 9462: Training loss = 0.0095, 0.0581 s ---\n",
            "--- Iteration 9463: Training loss = 0.0100, 0.0487 s ---\n",
            "--- Iteration 9464: Training loss = 0.0076, 0.0469 s ---\n",
            "--- Iteration 9465: Training loss = 0.0095, 0.0519 s ---\n",
            "--- Iteration 9466: Training loss = 0.0090, 0.0461 s ---\n",
            "--- Iteration 9467: Training loss = 0.0120, 0.0505 s ---\n",
            "--- Iteration 9468: Training loss = 0.0133, 0.0527 s ---\n",
            "--- Iteration 9469: Training loss = 0.0080, 0.0465 s ---\n",
            "--- Iteration 9470: Training loss = 0.0100, 0.0469 s ---\n",
            "--- Iteration 9470: Test loss = 0.0775 ---\n",
            "\n",
            "--- Iteration 9471: Training loss = 0.0059, 0.0457 s ---\n",
            "--- Iteration 9472: Training loss = 0.0131, 0.0481 s ---\n",
            "--- Iteration 9473: Training loss = 0.0149, 0.0470 s ---\n",
            "--- Iteration 9474: Training loss = 0.0052, 0.0450 s ---\n",
            "--- Iteration 9475: Training loss = 0.0076, 0.0449 s ---\n",
            "--- Iteration 9476: Training loss = 0.0109, 0.0520 s ---\n",
            "--- Iteration 9477: Training loss = 0.0091, 0.0465 s ---\n",
            "--- Iteration 9478: Training loss = 0.0091, 0.0660 s ---\n",
            "--- Iteration 9479: Training loss = 0.0111, 0.0489 s ---\n",
            "--- Iteration 9480: Training loss = 0.0073, 0.0459 s ---\n",
            "--- Iteration 9480: Test loss = 0.0437 ---\n",
            "\n",
            "--- Iteration 9481: Training loss = 0.0096, 0.0458 s ---\n",
            "--- Iteration 9482: Training loss = 0.0058, 0.0454 s ---\n",
            "--- Iteration 9483: Training loss = 0.0066, 0.0466 s ---\n",
            "--- Iteration 9484: Training loss = 0.0070, 0.0478 s ---\n",
            "--- Iteration 9485: Training loss = 0.0152, 0.0526 s ---\n",
            "--- Iteration 9486: Training loss = 0.0080, 0.0468 s ---\n",
            "--- Iteration 9487: Training loss = 0.0042, 0.0448 s ---\n",
            "--- Iteration 9488: Training loss = 0.0076, 0.0454 s ---\n",
            "--- Iteration 9489: Training loss = 0.0071, 0.0505 s ---\n",
            "--- Iteration 9490: Training loss = 0.0078, 0.0454 s ---\n",
            "--- Iteration 9490: Test loss = 0.0369 ---\n",
            "\n",
            "--- Iteration 9491: Training loss = 0.0109, 0.0454 s ---\n",
            "--- Iteration 9492: Training loss = 0.0080, 0.0468 s ---\n",
            "--- Iteration 9493: Training loss = 0.0099, 0.0455 s ---\n",
            "--- Iteration 9494: Training loss = 0.0068, 0.0475 s ---\n",
            "--- Iteration 9495: Training loss = 0.0105, 0.0468 s ---\n",
            "--- Iteration 9496: Training loss = 0.0053, 0.1056 s ---\n",
            "--- Iteration 9497: Training loss = 0.0121, 0.1658 s ---\n",
            "--- Iteration 9498: Training loss = 0.0045, 0.1139 s ---\n",
            "--- Iteration 9499: Training loss = 0.0059, 0.1143 s ---\n",
            "--- Iteration 9500: Training loss = 0.0072, 0.0449 s ---\n",
            "--- Iteration 9500: Test loss = 0.0313 ---\n",
            "\n",
            "--- Iteration 9501: Training loss = 0.0097, 0.0422 s ---\n",
            "--- Iteration 9502: Training loss = 0.0079, 0.0428 s ---\n",
            "--- Iteration 9503: Training loss = 0.0054, 0.0466 s ---\n",
            "--- Iteration 9504: Training loss = 0.0059, 0.0497 s ---\n",
            "--- Iteration 9505: Training loss = 0.0090, 0.0446 s ---\n",
            "--- Iteration 9506: Training loss = 0.0128, 0.0432 s ---\n",
            "--- Iteration 9507: Training loss = 0.0096, 0.0425 s ---\n",
            "--- Iteration 9508: Training loss = 0.0092, 0.0433 s ---\n",
            "--- Iteration 9509: Training loss = 0.0056, 0.0430 s ---\n",
            "--- Iteration 9510: Training loss = 0.0113, 0.0435 s ---\n",
            "--- Iteration 9510: Test loss = 0.0539 ---\n",
            "\n",
            "--- Iteration 9511: Training loss = 0.0048, 0.1052 s ---\n",
            "--- Iteration 9512: Training loss = 0.0068, 0.0945 s ---\n",
            "--- Iteration 9513: Training loss = 0.0080, 0.1048 s ---\n",
            "--- Iteration 9514: Training loss = 0.0059, 0.0679 s ---\n",
            "--- Iteration 9515: Training loss = 0.0055, 0.0517 s ---\n",
            "--- Iteration 9516: Training loss = 0.0098, 0.0461 s ---\n",
            "--- Iteration 9517: Training loss = 0.0082, 0.0480 s ---\n",
            "--- Iteration 9518: Training loss = 0.0086, 0.0513 s ---\n",
            "--- Iteration 9519: Training loss = 0.0060, 0.0700 s ---\n",
            "--- Iteration 9520: Training loss = 0.0078, 0.0807 s ---\n",
            "--- Iteration 9520: Test loss = 0.0856 ---\n",
            "\n",
            "--- Iteration 9521: Training loss = 0.0049, 0.0597 s ---\n",
            "--- Iteration 9522: Training loss = 0.0095, 0.0474 s ---\n",
            "--- Iteration 9523: Training loss = 0.0057, 0.0737 s ---\n",
            "--- Iteration 9524: Training loss = 0.0092, 0.0898 s ---\n",
            "--- Iteration 9525: Training loss = 0.0080, 0.0820 s ---\n",
            "--- Iteration 9526: Training loss = 0.0085, 0.0441 s ---\n",
            "--- Iteration 9527: Training loss = 0.0070, 0.0433 s ---\n",
            "--- Iteration 9528: Training loss = 0.0071, 0.0428 s ---\n",
            "--- Iteration 9529: Training loss = 0.0098, 0.0423 s ---\n",
            "--- Iteration 9530: Training loss = 0.0090, 0.0427 s ---\n",
            "--- Iteration 9530: Test loss = 0.0568 ---\n",
            "\n",
            "--- Iteration 9531: Training loss = 0.0090, 0.0415 s ---\n",
            "--- Iteration 9532: Training loss = 0.0106, 0.0429 s ---\n",
            "--- Iteration 9533: Training loss = 0.0091, 0.0430 s ---\n",
            "--- Iteration 9534: Training loss = 0.0101, 0.0466 s ---\n",
            "--- Iteration 9535: Training loss = 0.0103, 0.0449 s ---\n",
            "--- Iteration 9536: Training loss = 0.0093, 0.0428 s ---\n",
            "--- Iteration 9537: Training loss = 0.0099, 0.0432 s ---\n",
            "--- Iteration 9538: Training loss = 0.0082, 0.0437 s ---\n",
            "--- Iteration 9539: Training loss = 0.0113, 0.0447 s ---\n",
            "--- Iteration 9540: Training loss = 0.0076, 0.0478 s ---\n",
            "--- Iteration 9540: Test loss = 0.0498 ---\n",
            "\n",
            "--- Iteration 9541: Training loss = 0.0084, 0.0476 s ---\n",
            "--- Iteration 9542: Training loss = 0.0036, 0.0578 s ---\n",
            "--- Iteration 9543: Training loss = 0.0087, 0.0640 s ---\n",
            "--- Iteration 9544: Training loss = 0.0064, 0.0676 s ---\n",
            "--- Iteration 9545: Training loss = 0.0064, 0.0559 s ---\n",
            "--- Iteration 9546: Training loss = 0.0095, 0.0497 s ---\n",
            "--- Iteration 9547: Training loss = 0.0094, 0.0481 s ---\n",
            "--- Iteration 9548: Training loss = 0.0083, 0.0544 s ---\n",
            "--- Iteration 9549: Training loss = 0.0058, 0.0642 s ---\n",
            "--- Iteration 9550: Training loss = 0.0064, 0.0678 s ---\n",
            "--- Iteration 9550: Test loss = 0.0637 ---\n",
            "\n",
            "--- Iteration 9551: Training loss = 0.0083, 0.1059 s ---\n",
            "--- Iteration 9552: Training loss = 0.0091, 0.0535 s ---\n",
            "--- Iteration 9553: Training loss = 0.0051, 0.0631 s ---\n",
            "--- Iteration 9554: Training loss = 0.0135, 0.0553 s ---\n",
            "--- Iteration 9555: Training loss = 0.0067, 0.0725 s ---\n",
            "--- Iteration 9556: Training loss = 0.0116, 0.0486 s ---\n",
            "--- Iteration 9557: Training loss = 0.0089, 0.0444 s ---\n",
            "--- Iteration 9558: Training loss = 0.0108, 0.0421 s ---\n",
            "--- Iteration 9559: Training loss = 0.0084, 0.0427 s ---\n",
            "--- Iteration 9560: Training loss = 0.0135, 0.0431 s ---\n",
            "--- Iteration 9560: Test loss = 0.0311 ---\n",
            "\n",
            "--- Iteration 9561: Training loss = 0.0106, 0.0422 s ---\n",
            "--- Iteration 9562: Training loss = 0.0102, 0.0476 s ---\n",
            "--- Iteration 9563: Training loss = 0.0103, 0.0443 s ---\n",
            "--- Iteration 9564: Training loss = 0.0131, 0.0435 s ---\n",
            "--- Iteration 9565: Training loss = 0.0073, 0.0486 s ---\n",
            "--- Iteration 9566: Training loss = 0.0070, 0.0437 s ---\n",
            "--- Iteration 9567: Training loss = 0.0089, 0.0451 s ---\n",
            "--- Iteration 9568: Training loss = 0.0091, 0.0431 s ---\n",
            "--- Iteration 9569: Training loss = 0.0057, 0.0424 s ---\n",
            "--- Iteration 9570: Training loss = 0.0100, 0.0431 s ---\n",
            "--- Iteration 9570: Test loss = 0.0556 ---\n",
            "\n",
            "--- Iteration 9571: Training loss = 0.0076, 0.0407 s ---\n",
            "--- Iteration 9572: Training loss = 0.0097, 0.0444 s ---\n",
            "--- Iteration 9573: Training loss = 0.0057, 0.0434 s ---\n",
            "--- Iteration 9574: Training loss = 0.0110, 0.0537 s ---\n",
            "--- Iteration 9575: Training loss = 0.0056, 0.0566 s ---\n",
            "--- Iteration 9576: Training loss = 0.0127, 0.0917 s ---\n",
            "--- Iteration 9577: Training loss = 0.0079, 0.0473 s ---\n",
            "--- Iteration 9578: Training loss = 0.0137, 0.0473 s ---\n",
            "--- Iteration 9579: Training loss = 0.0103, 0.0569 s ---\n",
            "--- Iteration 9580: Training loss = 0.0120, 0.0473 s ---\n",
            "--- Iteration 9580: Test loss = 0.0668 ---\n",
            "\n",
            "--- Iteration 9581: Training loss = 0.0116, 0.0569 s ---\n",
            "--- Iteration 9582: Training loss = 0.0071, 0.0508 s ---\n",
            "--- Iteration 9583: Training loss = 0.0098, 0.0603 s ---\n",
            "--- Iteration 9584: Training loss = 0.0092, 0.0482 s ---\n",
            "--- Iteration 9585: Training loss = 0.0047, 0.0516 s ---\n",
            "--- Iteration 9586: Training loss = 0.0062, 0.0624 s ---\n",
            "--- Iteration 9587: Training loss = 0.0066, 0.0588 s ---\n",
            "--- Iteration 9588: Training loss = 0.0057, 0.0517 s ---\n",
            "--- Iteration 9589: Training loss = 0.0083, 0.0524 s ---\n",
            "--- Iteration 9590: Training loss = 0.0068, 0.0482 s ---\n",
            "--- Iteration 9590: Test loss = 0.0453 ---\n",
            "\n",
            "--- Iteration 9591: Training loss = 0.0069, 0.0415 s ---\n",
            "--- Iteration 9592: Training loss = 0.0136, 0.0435 s ---\n",
            "--- Iteration 9593: Training loss = 0.0064, 0.0451 s ---\n",
            "--- Iteration 9594: Training loss = 0.0039, 0.0607 s ---\n",
            "--- Iteration 9595: Training loss = 0.0065, 0.0489 s ---\n",
            "--- Iteration 9596: Training loss = 0.0076, 0.0759 s ---\n",
            "--- Iteration 9597: Training loss = 0.0066, 0.0623 s ---\n",
            "--- Iteration 9598: Training loss = 0.0058, 0.1016 s ---\n",
            "--- Iteration 9599: Training loss = 0.0095, 0.0753 s ---\n",
            "--- Iteration 9600: Training loss = 0.0088, 0.1385 s ---\n",
            "--- Iteration 9600: Test loss = 0.0667 ---\n",
            "\n",
            "--- Iteration 9601: Training loss = 0.0160, 0.1508 s ---\n",
            "--- Iteration 9602: Training loss = 0.0085, 0.1627 s ---\n",
            "--- Iteration 9603: Training loss = 0.0092, 0.1544 s ---\n",
            "--- Iteration 9604: Training loss = 0.0088, 0.1032 s ---\n",
            "--- Iteration 9605: Training loss = 0.0093, 0.1649 s ---\n",
            "--- Iteration 9606: Training loss = 0.0116, 0.1924 s ---\n",
            "--- Iteration 9607: Training loss = 0.0093, 0.1818 s ---\n",
            "--- Iteration 9608: Training loss = 0.0054, 0.2028 s ---\n",
            "--- Iteration 9609: Training loss = 0.0076, 0.2468 s ---\n",
            "--- Iteration 9610: Training loss = 0.0103, 0.2299 s ---\n",
            "--- Iteration 9610: Test loss = 0.0347 ---\n",
            "\n",
            "--- Iteration 9611: Training loss = 0.0075, 0.1774 s ---\n",
            "--- Iteration 9612: Training loss = 0.0099, 0.1677 s ---\n",
            "--- Iteration 9613: Training loss = 0.0078, 0.1780 s ---\n",
            "--- Iteration 9614: Training loss = 0.0125, 0.1147 s ---\n",
            "--- Iteration 9615: Training loss = 0.0071, 0.1116 s ---\n",
            "--- Iteration 9616: Training loss = 0.0066, 0.0934 s ---\n",
            "--- Iteration 9617: Training loss = 0.0101, 0.0992 s ---\n",
            "--- Iteration 9618: Training loss = 0.0108, 0.1022 s ---\n",
            "--- Iteration 9619: Training loss = 0.0104, 0.0891 s ---\n",
            "--- Iteration 9620: Training loss = 0.0103, 0.0575 s ---\n",
            "--- Iteration 9620: Test loss = 0.0370 ---\n",
            "\n",
            "--- Iteration 9621: Training loss = 0.0072, 0.0559 s ---\n",
            "--- Iteration 9622: Training loss = 0.0067, 0.0552 s ---\n",
            "--- Iteration 9623: Training loss = 0.0081, 0.0647 s ---\n",
            "--- Iteration 9624: Training loss = 0.0052, 0.0772 s ---\n",
            "--- Iteration 9625: Training loss = 0.0071, 0.0510 s ---\n",
            "--- Iteration 9626: Training loss = 0.0069, 0.0542 s ---\n",
            "--- Iteration 9627: Training loss = 0.0108, 0.0689 s ---\n",
            "--- Iteration 9628: Training loss = 0.0111, 0.0497 s ---\n",
            "--- Iteration 9629: Training loss = 0.0099, 0.0539 s ---\n",
            "--- Iteration 9630: Training loss = 0.0116, 0.0471 s ---\n",
            "--- Iteration 9630: Test loss = 0.0313 ---\n",
            "\n",
            "--- Iteration 9631: Training loss = 0.0141, 0.0529 s ---\n",
            "--- Iteration 9632: Training loss = 0.0093, 0.0604 s ---\n",
            "--- Iteration 9633: Training loss = 0.0105, 0.0834 s ---\n",
            "--- Iteration 9634: Training loss = 0.0098, 0.0471 s ---\n",
            "--- Iteration 9635: Training loss = 0.0067, 0.0482 s ---\n",
            "--- Iteration 9636: Training loss = 0.0104, 0.0598 s ---\n",
            "--- Iteration 9637: Training loss = 0.0102, 0.0504 s ---\n",
            "--- Iteration 9638: Training loss = 0.0094, 0.0436 s ---\n",
            "--- Iteration 9639: Training loss = 0.0129, 0.0439 s ---\n",
            "--- Iteration 9640: Training loss = 0.0068, 0.0445 s ---\n",
            "--- Iteration 9640: Test loss = 0.0267 ---\n",
            "\n",
            "--- Iteration 9641: Training loss = 0.0093, 0.0409 s ---\n",
            "--- Iteration 9642: Training loss = 0.0081, 0.0436 s ---\n",
            "--- Iteration 9643: Training loss = 0.0082, 0.0426 s ---\n",
            "--- Iteration 9644: Training loss = 0.0073, 0.0430 s ---\n",
            "--- Iteration 9645: Training loss = 0.0107, 0.0432 s ---\n",
            "--- Iteration 9646: Training loss = 0.0062, 0.0444 s ---\n",
            "--- Iteration 9647: Training loss = 0.0077, 0.0442 s ---\n",
            "--- Iteration 9648: Training loss = 0.0109, 0.0432 s ---\n",
            "--- Iteration 9649: Training loss = 0.0101, 0.0438 s ---\n",
            "--- Iteration 9650: Training loss = 0.0089, 0.0431 s ---\n",
            "--- Iteration 9650: Test loss = 0.0420 ---\n",
            "\n",
            "--- Iteration 9651: Training loss = 0.0081, 0.0422 s ---\n",
            "--- Iteration 9652: Training loss = 0.0139, 0.0455 s ---\n",
            "--- Iteration 9653: Training loss = 0.0123, 0.0452 s ---\n",
            "--- Iteration 9654: Training loss = 0.0064, 0.0433 s ---\n",
            "--- Iteration 9655: Training loss = 0.0105, 0.0430 s ---\n",
            "--- Iteration 9656: Training loss = 0.0113, 0.0449 s ---\n",
            "--- Iteration 9657: Training loss = 0.0066, 0.0429 s ---\n",
            "--- Iteration 9658: Training loss = 0.0070, 0.0454 s ---\n",
            "--- Iteration 9659: Training loss = 0.0035, 0.0445 s ---\n",
            "--- Iteration 9660: Training loss = 0.0075, 0.0434 s ---\n",
            "--- Iteration 9660: Test loss = 0.0578 ---\n",
            "\n",
            "--- Iteration 9661: Training loss = 0.0097, 0.0411 s ---\n",
            "--- Iteration 9662: Training loss = 0.0059, 0.0433 s ---\n",
            "--- Iteration 9663: Training loss = 0.0158, 0.0424 s ---\n",
            "--- Iteration 9664: Training loss = 0.0101, 0.0429 s ---\n",
            "--- Iteration 9665: Training loss = 0.0113, 0.0432 s ---\n",
            "--- Iteration 9666: Training loss = 0.0108, 0.0438 s ---\n",
            "--- Iteration 9667: Training loss = 0.0123, 0.0446 s ---\n",
            "--- Iteration 9668: Training loss = 0.0099, 0.0446 s ---\n",
            "--- Iteration 9669: Training loss = 0.0080, 0.0448 s ---\n",
            "--- Iteration 9670: Training loss = 0.0075, 0.0429 s ---\n",
            "--- Iteration 9670: Test loss = 0.0426 ---\n",
            "\n",
            "--- Iteration 9671: Training loss = 0.0124, 0.0425 s ---\n",
            "--- Iteration 9672: Training loss = 0.0111, 0.0444 s ---\n",
            "--- Iteration 9673: Training loss = 0.0048, 0.0452 s ---\n",
            "--- Iteration 9674: Training loss = 0.0114, 0.0452 s ---\n",
            "--- Iteration 9675: Training loss = 0.0058, 0.0427 s ---\n",
            "--- Iteration 9676: Training loss = 0.0054, 0.0434 s ---\n",
            "--- Iteration 9677: Training loss = 0.0067, 0.0428 s ---\n",
            "--- Iteration 9678: Training loss = 0.0081, 0.0437 s ---\n",
            "--- Iteration 9679: Training loss = 0.0084, 0.0435 s ---\n",
            "--- Iteration 9680: Training loss = 0.0054, 0.0443 s ---\n",
            "--- Iteration 9680: Test loss = 0.0463 ---\n",
            "\n",
            "--- Iteration 9681: Training loss = 0.0123, 0.0425 s ---\n",
            "--- Iteration 9682: Training loss = 0.0086, 0.0435 s ---\n",
            "--- Iteration 9683: Training loss = 0.0099, 0.0448 s ---\n",
            "--- Iteration 9684: Training loss = 0.0109, 0.0427 s ---\n",
            "--- Iteration 9685: Training loss = 0.0073, 0.0478 s ---\n",
            "--- Iteration 9686: Training loss = 0.0083, 0.0439 s ---\n",
            "--- Iteration 9687: Training loss = 0.0121, 0.0427 s ---\n",
            "--- Iteration 9688: Training loss = 0.0089, 0.0429 s ---\n",
            "--- Iteration 9689: Training loss = 0.0110, 0.0428 s ---\n",
            "--- Iteration 9690: Training loss = 0.0078, 0.0424 s ---\n",
            "--- Iteration 9690: Test loss = 0.0328 ---\n",
            "\n",
            "--- Iteration 9691: Training loss = 0.0081, 0.0481 s ---\n",
            "--- Iteration 9692: Training loss = 0.0138, 0.0467 s ---\n",
            "--- Iteration 9693: Training loss = 0.0091, 0.0425 s ---\n",
            "--- Iteration 9694: Training loss = 0.0145, 0.0420 s ---\n",
            "--- Iteration 9695: Training loss = 0.0057, 0.0422 s ---\n",
            "--- Iteration 9696: Training loss = 0.0112, 0.0441 s ---\n",
            "--- Iteration 9697: Training loss = 0.0088, 0.0445 s ---\n",
            "--- Iteration 9698: Training loss = 0.0088, 0.0433 s ---\n",
            "--- Iteration 9699: Training loss = 0.0098, 0.0429 s ---\n",
            "--- Iteration 9700: Training loss = 0.0115, 0.0417 s ---\n",
            "--- Iteration 9700: Test loss = 0.0436 ---\n",
            "\n",
            "--- Iteration 9701: Training loss = 0.0089, 0.0414 s ---\n",
            "--- Iteration 9702: Training loss = 0.0093, 0.0436 s ---\n",
            "--- Iteration 9703: Training loss = 0.0061, 0.0438 s ---\n",
            "--- Iteration 9704: Training loss = 0.0077, 0.0436 s ---\n",
            "--- Iteration 9705: Training loss = 0.0104, 0.0447 s ---\n",
            "--- Iteration 9706: Training loss = 0.0083, 0.0444 s ---\n",
            "--- Iteration 9707: Training loss = 0.0112, 0.0426 s ---\n",
            "--- Iteration 9708: Training loss = 0.0093, 0.0448 s ---\n",
            "--- Iteration 9709: Training loss = 0.0101, 0.0435 s ---\n",
            "--- Iteration 9710: Training loss = 0.0128, 0.0439 s ---\n",
            "--- Iteration 9710: Test loss = 0.0332 ---\n",
            "\n",
            "--- Iteration 9711: Training loss = 0.0101, 0.0431 s ---\n",
            "--- Iteration 9712: Training loss = 0.0082, 0.0455 s ---\n",
            "--- Iteration 9713: Training loss = 0.0116, 0.0425 s ---\n",
            "--- Iteration 9714: Training loss = 0.0098, 0.0426 s ---\n",
            "--- Iteration 9715: Training loss = 0.0117, 0.0425 s ---\n",
            "--- Iteration 9716: Training loss = 0.0101, 0.0431 s ---\n",
            "--- Iteration 9717: Training loss = 0.0114, 0.0432 s ---\n",
            "--- Iteration 9718: Training loss = 0.0080, 0.0452 s ---\n",
            "--- Iteration 9719: Training loss = 0.0055, 0.0441 s ---\n",
            "--- Iteration 9720: Training loss = 0.0064, 0.0427 s ---\n",
            "--- Iteration 9720: Test loss = 0.0114 ---\n",
            "\n",
            "--- Iteration 9721: Training loss = 0.0099, 0.0417 s ---\n",
            "--- Iteration 9722: Training loss = 0.0111, 0.0458 s ---\n",
            "--- Iteration 9723: Training loss = 0.0081, 0.0439 s ---\n",
            "--- Iteration 9724: Training loss = 0.0048, 0.0453 s ---\n",
            "--- Iteration 9725: Training loss = 0.0088, 0.0457 s ---\n",
            "--- Iteration 9726: Training loss = 0.0090, 0.0432 s ---\n",
            "--- Iteration 9727: Training loss = 0.0075, 0.0434 s ---\n",
            "--- Iteration 9728: Training loss = 0.0082, 0.0436 s ---\n",
            "--- Iteration 9729: Training loss = 0.0094, 0.0437 s ---\n",
            "--- Iteration 9730: Training loss = 0.0077, 0.0516 s ---\n",
            "--- Iteration 9730: Test loss = 0.0599 ---\n",
            "\n",
            "--- Iteration 9731: Training loss = 0.0064, 0.0452 s ---\n",
            "--- Iteration 9732: Training loss = 0.0102, 0.0482 s ---\n",
            "--- Iteration 9733: Training loss = 0.0080, 0.0445 s ---\n",
            "--- Iteration 9734: Training loss = 0.0070, 0.0446 s ---\n",
            "--- Iteration 9735: Training loss = 0.0046, 0.0419 s ---\n",
            "--- Iteration 9736: Training loss = 0.0134, 0.0430 s ---\n",
            "--- Iteration 9737: Training loss = 0.0079, 0.0435 s ---\n",
            "--- Iteration 9738: Training loss = 0.0091, 0.0438 s ---\n",
            "--- Iteration 9739: Training loss = 0.0055, 0.0450 s ---\n",
            "--- Iteration 9740: Training loss = 0.0087, 0.0437 s ---\n",
            "--- Iteration 9740: Test loss = 0.0433 ---\n",
            "\n",
            "--- Iteration 9741: Training loss = 0.0062, 0.0455 s ---\n",
            "--- Iteration 9742: Training loss = 0.0119, 0.0437 s ---\n",
            "--- Iteration 9743: Training loss = 0.0067, 0.0438 s ---\n",
            "--- Iteration 9744: Training loss = 0.0090, 0.0444 s ---\n",
            "--- Iteration 9745: Training loss = 0.0089, 0.0447 s ---\n",
            "--- Iteration 9746: Training loss = 0.0086, 0.0430 s ---\n",
            "--- Iteration 9747: Training loss = 0.0071, 0.0427 s ---\n",
            "--- Iteration 9748: Training loss = 0.0095, 0.0431 s ---\n",
            "--- Iteration 9749: Training loss = 0.0098, 0.0430 s ---\n",
            "--- Iteration 9750: Training loss = 0.0084, 0.0451 s ---\n",
            "--- Iteration 9750: Test loss = 0.0551 ---\n",
            "\n",
            "--- Iteration 9751: Training loss = 0.0107, 0.0460 s ---\n",
            "--- Iteration 9752: Training loss = 0.0056, 0.0473 s ---\n",
            "--- Iteration 9753: Training loss = 0.0080, 0.0424 s ---\n",
            "--- Iteration 9754: Training loss = 0.0084, 0.0432 s ---\n",
            "--- Iteration 9755: Training loss = 0.0060, 0.0432 s ---\n",
            "--- Iteration 9756: Training loss = 0.0074, 0.0458 s ---\n",
            "--- Iteration 9757: Training loss = 0.0070, 0.0435 s ---\n",
            "--- Iteration 9758: Training loss = 0.0078, 0.0434 s ---\n",
            "--- Iteration 9759: Training loss = 0.0096, 0.0430 s ---\n",
            "--- Iteration 9760: Training loss = 0.0113, 0.0425 s ---\n",
            "--- Iteration 9760: Test loss = 0.0842 ---\n",
            "\n",
            "--- Iteration 9761: Training loss = 0.0085, 0.0479 s ---\n",
            "--- Iteration 9762: Training loss = 0.0138, 0.0446 s ---\n",
            "--- Iteration 9763: Training loss = 0.0059, 0.0434 s ---\n",
            "--- Iteration 9764: Training loss = 0.0134, 0.0430 s ---\n",
            "--- Iteration 9765: Training loss = 0.0130, 0.0429 s ---\n",
            "--- Iteration 9766: Training loss = 0.0078, 0.0431 s ---\n",
            "--- Iteration 9767: Training loss = 0.0120, 0.0444 s ---\n",
            "--- Iteration 9768: Training loss = 0.0084, 0.0454 s ---\n",
            "--- Iteration 9769: Training loss = 0.0109, 0.0445 s ---\n",
            "--- Iteration 9770: Training loss = 0.0098, 0.0434 s ---\n",
            "--- Iteration 9770: Test loss = 0.0588 ---\n",
            "\n",
            "--- Iteration 9771: Training loss = 0.0063, 0.0467 s ---\n",
            "--- Iteration 9772: Training loss = 0.0050, 0.0444 s ---\n",
            "--- Iteration 9773: Training loss = 0.0076, 0.0440 s ---\n",
            "--- Iteration 9774: Training loss = 0.0078, 0.0481 s ---\n",
            "--- Iteration 9775: Training loss = 0.0133, 0.0425 s ---\n",
            "--- Iteration 9776: Training loss = 0.0093, 0.0425 s ---\n",
            "--- Iteration 9777: Training loss = 0.0061, 0.0428 s ---\n",
            "--- Iteration 9778: Training loss = 0.0099, 0.0452 s ---\n",
            "--- Iteration 9779: Training loss = 0.0102, 0.0442 s ---\n",
            "--- Iteration 9780: Training loss = 0.0111, 0.0430 s ---\n",
            "--- Iteration 9780: Test loss = 0.0434 ---\n",
            "\n",
            "--- Iteration 9781: Training loss = 0.0106, 0.0445 s ---\n",
            "--- Iteration 9782: Training loss = 0.0124, 0.0415 s ---\n",
            "--- Iteration 9783: Training loss = 0.0062, 0.0427 s ---\n",
            "--- Iteration 9784: Training loss = 0.0048, 0.0478 s ---\n",
            "--- Iteration 9785: Training loss = 0.0144, 0.0432 s ---\n",
            "--- Iteration 9786: Training loss = 0.0075, 0.0452 s ---\n",
            "--- Iteration 9787: Training loss = 0.0078, 0.0431 s ---\n",
            "--- Iteration 9788: Training loss = 0.0078, 0.0426 s ---\n",
            "--- Iteration 9789: Training loss = 0.0051, 0.0436 s ---\n",
            "--- Iteration 9790: Training loss = 0.0097, 0.0455 s ---\n",
            "--- Iteration 9790: Test loss = 0.0579 ---\n",
            "\n",
            "--- Iteration 9791: Training loss = 0.0095, 0.0435 s ---\n",
            "--- Iteration 9792: Training loss = 0.0113, 0.0429 s ---\n",
            "--- Iteration 9793: Training loss = 0.0055, 0.0425 s ---\n",
            "--- Iteration 9794: Training loss = 0.0092, 0.0428 s ---\n",
            "--- Iteration 9795: Training loss = 0.0110, 0.0426 s ---\n",
            "--- Iteration 9796: Training loss = 0.0085, 0.0426 s ---\n",
            "--- Iteration 9797: Training loss = 0.0076, 0.0454 s ---\n",
            "--- Iteration 9798: Training loss = 0.0087, 0.0441 s ---\n",
            "--- Iteration 9799: Training loss = 0.0110, 0.0430 s ---\n",
            "--- Iteration 9800: Training loss = 0.0105, 0.0430 s ---\n",
            "--- Iteration 9800: Test loss = 0.0328 ---\n",
            "\n",
            "--- Iteration 9801: Training loss = 0.0042, 0.0436 s ---\n",
            "--- Iteration 9802: Training loss = 0.0083, 0.0430 s ---\n",
            "--- Iteration 9803: Training loss = 0.0033, 0.0419 s ---\n",
            "--- Iteration 9804: Training loss = 0.0090, 0.0420 s ---\n",
            "--- Iteration 9805: Training loss = 0.0066, 0.0428 s ---\n",
            "--- Iteration 9806: Training loss = 0.0118, 0.0448 s ---\n",
            "--- Iteration 9807: Training loss = 0.0116, 0.0443 s ---\n",
            "--- Iteration 9808: Training loss = 0.0107, 0.0425 s ---\n",
            "--- Iteration 9809: Training loss = 0.0085, 0.0431 s ---\n",
            "--- Iteration 9810: Training loss = 0.0068, 0.0425 s ---\n",
            "--- Iteration 9810: Test loss = 0.0277 ---\n",
            "\n",
            "--- Iteration 9811: Training loss = 0.0114, 0.0417 s ---\n",
            "--- Iteration 9812: Training loss = 0.0093, 0.0435 s ---\n",
            "--- Iteration 9813: Training loss = 0.0097, 0.0430 s ---\n",
            "--- Iteration 9814: Training loss = 0.0057, 0.0455 s ---\n",
            "--- Iteration 9815: Training loss = 0.0061, 0.0445 s ---\n",
            "--- Iteration 9816: Training loss = 0.0073, 0.0428 s ---\n",
            "--- Iteration 9817: Training loss = 0.0070, 0.0415 s ---\n",
            "--- Iteration 9818: Training loss = 0.0096, 0.0427 s ---\n",
            "--- Iteration 9819: Training loss = 0.0083, 0.0482 s ---\n",
            "--- Iteration 9820: Training loss = 0.0044, 0.0443 s ---\n",
            "--- Iteration 9820: Test loss = 0.0499 ---\n",
            "\n",
            "--- Iteration 9821: Training loss = 0.0091, 0.0503 s ---\n",
            "--- Iteration 9822: Training loss = 0.0069, 0.0438 s ---\n",
            "--- Iteration 9823: Training loss = 0.0075, 0.0435 s ---\n",
            "--- Iteration 9824: Training loss = 0.0073, 0.0442 s ---\n",
            "--- Iteration 9825: Training loss = 0.0083, 0.0429 s ---\n",
            "--- Iteration 9826: Training loss = 0.0067, 0.0428 s ---\n",
            "--- Iteration 9827: Training loss = 0.0119, 0.0433 s ---\n",
            "--- Iteration 9828: Training loss = 0.0087, 0.0434 s ---\n",
            "--- Iteration 9829: Training loss = 0.0074, 0.0428 s ---\n",
            "--- Iteration 9830: Training loss = 0.0054, 0.0441 s ---\n",
            "--- Iteration 9830: Test loss = 0.0276 ---\n",
            "\n",
            "--- Iteration 9831: Training loss = 0.0065, 0.0441 s ---\n",
            "--- Iteration 9832: Training loss = 0.0067, 0.0431 s ---\n",
            "--- Iteration 9833: Training loss = 0.0113, 0.0429 s ---\n",
            "--- Iteration 9834: Training loss = 0.0088, 0.0426 s ---\n",
            "--- Iteration 9835: Training loss = 0.0075, 0.0452 s ---\n",
            "--- Iteration 9836: Training loss = 0.0082, 0.0439 s ---\n",
            "--- Iteration 9837: Training loss = 0.0093, 0.0458 s ---\n",
            "--- Iteration 9838: Training loss = 0.0069, 0.0439 s ---\n",
            "--- Iteration 9839: Training loss = 0.0100, 0.0432 s ---\n",
            "--- Iteration 9840: Training loss = 0.0100, 0.0438 s ---\n",
            "--- Iteration 9840: Test loss = 0.0385 ---\n",
            "\n",
            "--- Iteration 9841: Training loss = 0.0087, 0.0499 s ---\n",
            "--- Iteration 9842: Training loss = 0.0124, 0.0437 s ---\n",
            "--- Iteration 9843: Training loss = 0.0056, 0.0438 s ---\n",
            "--- Iteration 9844: Training loss = 0.0041, 0.0443 s ---\n",
            "--- Iteration 9845: Training loss = 0.0070, 0.0435 s ---\n",
            "--- Iteration 9846: Training loss = 0.0092, 0.0455 s ---\n",
            "--- Iteration 9847: Training loss = 0.0074, 0.0450 s ---\n",
            "--- Iteration 9848: Training loss = 0.0099, 0.0430 s ---\n",
            "--- Iteration 9849: Training loss = 0.0099, 0.0430 s ---\n",
            "--- Iteration 9850: Training loss = 0.0080, 0.0429 s ---\n",
            "--- Iteration 9850: Test loss = 0.0518 ---\n",
            "\n",
            "--- Iteration 9851: Training loss = 0.0101, 0.0439 s ---\n",
            "--- Iteration 9852: Training loss = 0.0068, 0.0443 s ---\n",
            "--- Iteration 9853: Training loss = 0.0080, 0.0440 s ---\n",
            "--- Iteration 9854: Training loss = 0.0071, 0.0431 s ---\n",
            "--- Iteration 9855: Training loss = 0.0103, 0.0435 s ---\n",
            "--- Iteration 9856: Training loss = 0.0078, 0.0435 s ---\n",
            "--- Iteration 9857: Training loss = 0.0119, 0.0428 s ---\n",
            "--- Iteration 9858: Training loss = 0.0090, 0.0440 s ---\n",
            "--- Iteration 9859: Training loss = 0.0096, 0.0451 s ---\n",
            "--- Iteration 9860: Training loss = 0.0066, 0.0443 s ---\n",
            "--- Iteration 9860: Test loss = 0.0451 ---\n",
            "\n",
            "--- Iteration 9861: Training loss = 0.0116, 0.0517 s ---\n",
            "--- Iteration 9862: Training loss = 0.0066, 0.0445 s ---\n",
            "--- Iteration 9863: Training loss = 0.0070, 0.0480 s ---\n",
            "--- Iteration 9864: Training loss = 0.0105, 0.0454 s ---\n",
            "--- Iteration 9865: Training loss = 0.0113, 0.0436 s ---\n",
            "--- Iteration 9866: Training loss = 0.0084, 0.0456 s ---\n",
            "--- Iteration 9867: Training loss = 0.0114, 0.0447 s ---\n",
            "--- Iteration 9868: Training loss = 0.0069, 0.0424 s ---\n",
            "--- Iteration 9869: Training loss = 0.0097, 0.0430 s ---\n",
            "--- Iteration 9870: Training loss = 0.0063, 0.0441 s ---\n",
            "--- Iteration 9870: Test loss = 0.0356 ---\n",
            "\n",
            "--- Iteration 9871: Training loss = 0.0120, 0.0489 s ---\n",
            "--- Iteration 9872: Training loss = 0.0072, 0.0426 s ---\n",
            "--- Iteration 9873: Training loss = 0.0123, 0.0426 s ---\n",
            "--- Iteration 9874: Training loss = 0.0053, 0.0423 s ---\n",
            "--- Iteration 9875: Training loss = 0.0097, 0.0423 s ---\n",
            "--- Iteration 9876: Training loss = 0.0087, 0.0458 s ---\n",
            "--- Iteration 9877: Training loss = 0.0090, 0.0459 s ---\n",
            "--- Iteration 9878: Training loss = 0.0061, 0.0438 s ---\n",
            "--- Iteration 9879: Training loss = 0.0057, 0.0430 s ---\n",
            "--- Iteration 9880: Training loss = 0.0084, 0.0429 s ---\n",
            "--- Iteration 9880: Test loss = 0.0472 ---\n",
            "\n",
            "--- Iteration 9881: Training loss = 0.0066, 0.0452 s ---\n",
            "--- Iteration 9882: Training loss = 0.0120, 0.0426 s ---\n",
            "--- Iteration 9883: Training loss = 0.0080, 0.0448 s ---\n",
            "--- Iteration 9884: Training loss = 0.0093, 0.0442 s ---\n",
            "--- Iteration 9885: Training loss = 0.0061, 0.0471 s ---\n",
            "--- Iteration 9886: Training loss = 0.0075, 0.0429 s ---\n",
            "--- Iteration 9887: Training loss = 0.0064, 0.0437 s ---\n",
            "--- Iteration 9888: Training loss = 0.0088, 0.0452 s ---\n",
            "--- Iteration 9889: Training loss = 0.0051, 0.0435 s ---\n",
            "--- Iteration 9890: Training loss = 0.0089, 0.0429 s ---\n",
            "--- Iteration 9890: Test loss = 0.0885 ---\n",
            "\n",
            "--- Iteration 9891: Training loss = 0.0094, 0.0430 s ---\n",
            "--- Iteration 9892: Training loss = 0.0091, 0.0433 s ---\n",
            "--- Iteration 9893: Training loss = 0.0049, 0.0438 s ---\n",
            "--- Iteration 9894: Training loss = 0.0111, 0.0430 s ---\n",
            "--- Iteration 9895: Training loss = 0.0053, 0.0455 s ---\n",
            "--- Iteration 9896: Training loss = 0.0053, 0.0444 s ---\n",
            "--- Iteration 9897: Training loss = 0.0132, 0.0432 s ---\n",
            "--- Iteration 9898: Training loss = 0.0084, 0.0421 s ---\n",
            "--- Iteration 9899: Training loss = 0.0097, 0.0458 s ---\n",
            "--- Iteration 9900: Training loss = 0.0098, 0.0427 s ---\n",
            "--- Iteration 9900: Test loss = 0.0492 ---\n",
            "\n",
            "--- Iteration 9901: Training loss = 0.0064, 0.0442 s ---\n",
            "--- Iteration 9902: Training loss = 0.0075, 0.0448 s ---\n",
            "--- Iteration 9903: Training loss = 0.0087, 0.0432 s ---\n",
            "--- Iteration 9904: Training loss = 0.0076, 0.0424 s ---\n",
            "--- Iteration 9905: Training loss = 0.0074, 0.0426 s ---\n",
            "--- Iteration 9906: Training loss = 0.0074, 0.0432 s ---\n",
            "--- Iteration 9907: Training loss = 0.0101, 0.0439 s ---\n",
            "--- Iteration 9908: Training loss = 0.0072, 0.0451 s ---\n",
            "--- Iteration 9909: Training loss = 0.0085, 0.0439 s ---\n",
            "--- Iteration 9910: Training loss = 0.0097, 0.0430 s ---\n",
            "--- Iteration 9910: Test loss = 0.0696 ---\n",
            "\n",
            "--- Iteration 9911: Training loss = 0.0100, 0.0423 s ---\n",
            "--- Iteration 9912: Training loss = 0.0068, 0.0427 s ---\n",
            "--- Iteration 9913: Training loss = 0.0094, 0.0422 s ---\n",
            "--- Iteration 9914: Training loss = 0.0054, 0.0433 s ---\n",
            "--- Iteration 9915: Training loss = 0.0077, 0.0436 s ---\n",
            "--- Iteration 9916: Training loss = 0.0093, 0.0449 s ---\n",
            "--- Iteration 9917: Training loss = 0.0115, 0.0451 s ---\n",
            "--- Iteration 9918: Training loss = 0.0098, 0.0429 s ---\n",
            "--- Iteration 9919: Training loss = 0.0045, 0.0430 s ---\n",
            "--- Iteration 9920: Training loss = 0.0072, 0.0431 s ---\n",
            "--- Iteration 9920: Test loss = 0.0573 ---\n",
            "\n",
            "--- Iteration 9921: Training loss = 0.0063, 0.0435 s ---\n",
            "--- Iteration 9922: Training loss = 0.0058, 0.0459 s ---\n",
            "--- Iteration 9923: Training loss = 0.0098, 0.0439 s ---\n",
            "--- Iteration 9924: Training loss = 0.0065, 0.0427 s ---\n",
            "--- Iteration 9925: Training loss = 0.0083, 0.0433 s ---\n",
            "--- Iteration 9926: Training loss = 0.0080, 0.0432 s ---\n",
            "--- Iteration 9927: Training loss = 0.0078, 0.0439 s ---\n",
            "--- Iteration 9928: Training loss = 0.0083, 0.0444 s ---\n",
            "--- Iteration 9929: Training loss = 0.0085, 0.0442 s ---\n",
            "--- Iteration 9930: Training loss = 0.0069, 0.0467 s ---\n",
            "--- Iteration 9930: Test loss = 0.0483 ---\n",
            "\n",
            "--- Iteration 9931: Training loss = 0.0099, 0.0445 s ---\n",
            "--- Iteration 9932: Training loss = 0.0105, 0.0443 s ---\n",
            "--- Iteration 9933: Training loss = 0.0078, 0.0441 s ---\n",
            "--- Iteration 9934: Training loss = 0.0071, 0.0439 s ---\n",
            "--- Iteration 9935: Training loss = 0.0093, 0.0430 s ---\n",
            "--- Iteration 9936: Training loss = 0.0064, 0.0434 s ---\n",
            "--- Iteration 9937: Training loss = 0.0058, 0.0425 s ---\n",
            "--- Iteration 9938: Training loss = 0.0112, 0.0430 s ---\n",
            "--- Iteration 9939: Training loss = 0.0084, 0.0437 s ---\n",
            "--- Iteration 9940: Training loss = 0.0079, 0.0443 s ---\n",
            "--- Iteration 9940: Test loss = 0.0457 ---\n",
            "\n",
            "--- Iteration 9941: Training loss = 0.0058, 0.0448 s ---\n",
            "--- Iteration 9942: Training loss = 0.0081, 0.0428 s ---\n",
            "--- Iteration 9943: Training loss = 0.0095, 0.0429 s ---\n",
            "--- Iteration 9944: Training loss = 0.0052, 0.0434 s ---\n",
            "--- Iteration 9945: Training loss = 0.0081, 0.0428 s ---\n",
            "--- Iteration 9946: Training loss = 0.0138, 0.0437 s ---\n",
            "--- Iteration 9947: Training loss = 0.0090, 0.0450 s ---\n",
            "--- Iteration 9948: Training loss = 0.0113, 0.0427 s ---\n",
            "--- Iteration 9949: Training loss = 0.0121, 0.0430 s ---\n",
            "--- Iteration 9950: Training loss = 0.0122, 0.0429 s ---\n",
            "--- Iteration 9950: Test loss = 0.0363 ---\n",
            "\n",
            "--- Iteration 9951: Training loss = 0.0141, 0.0481 s ---\n",
            "--- Iteration 9952: Training loss = 0.0061, 0.0450 s ---\n",
            "--- Iteration 9953: Training loss = 0.0087, 0.0439 s ---\n",
            "--- Iteration 9954: Training loss = 0.0099, 0.0440 s ---\n",
            "--- Iteration 9955: Training loss = 0.0085, 0.0425 s ---\n",
            "--- Iteration 9956: Training loss = 0.0096, 0.0427 s ---\n",
            "--- Iteration 9957: Training loss = 0.0039, 0.0436 s ---\n",
            "--- Iteration 9958: Training loss = 0.0058, 0.0431 s ---\n",
            "--- Iteration 9959: Training loss = 0.0096, 0.0463 s ---\n",
            "--- Iteration 9960: Training loss = 0.0108, 0.0431 s ---\n",
            "--- Iteration 9960: Test loss = 0.0459 ---\n",
            "\n",
            "--- Iteration 9961: Training loss = 0.0138, 0.0450 s ---\n",
            "--- Iteration 9962: Training loss = 0.0080, 0.0425 s ---\n",
            "--- Iteration 9963: Training loss = 0.0060, 0.0427 s ---\n",
            "--- Iteration 9964: Training loss = 0.0085, 0.0448 s ---\n",
            "--- Iteration 9965: Training loss = 0.0105, 0.0444 s ---\n",
            "--- Iteration 9966: Training loss = 0.0070, 0.0433 s ---\n",
            "--- Iteration 9967: Training loss = 0.0068, 0.0424 s ---\n",
            "--- Iteration 9968: Training loss = 0.0060, 0.0432 s ---\n",
            "--- Iteration 9969: Training loss = 0.0065, 0.0435 s ---\n",
            "--- Iteration 9970: Training loss = 0.0039, 0.0437 s ---\n",
            "--- Iteration 9970: Test loss = 0.0553 ---\n",
            "\n",
            "--- Iteration 9971: Training loss = 0.0078, 0.0446 s ---\n",
            "--- Iteration 9972: Training loss = 0.0071, 0.0438 s ---\n",
            "--- Iteration 9973: Training loss = 0.0090, 0.0434 s ---\n",
            "--- Iteration 9974: Training loss = 0.0120, 0.0474 s ---\n",
            "--- Iteration 9975: Training loss = 0.0079, 0.0442 s ---\n",
            "--- Iteration 9976: Training loss = 0.0073, 0.0444 s ---\n",
            "--- Iteration 9977: Training loss = 0.0090, 0.0438 s ---\n",
            "--- Iteration 9978: Training loss = 0.0092, 0.0426 s ---\n",
            "--- Iteration 9979: Training loss = 0.0107, 0.0434 s ---\n",
            "--- Iteration 9980: Training loss = 0.0081, 0.0433 s ---\n",
            "--- Iteration 9980: Test loss = 0.0414 ---\n",
            "\n",
            "--- Iteration 9981: Training loss = 0.0070, 0.0432 s ---\n",
            "--- Iteration 9982: Training loss = 0.0087, 0.0442 s ---\n",
            "--- Iteration 9983: Training loss = 0.0063, 0.0441 s ---\n",
            "--- Iteration 9984: Training loss = 0.0108, 0.0435 s ---\n",
            "--- Iteration 9985: Training loss = 0.0102, 0.0425 s ---\n",
            "--- Iteration 9986: Training loss = 0.0116, 0.0439 s ---\n",
            "--- Iteration 9987: Training loss = 0.0098, 0.0428 s ---\n",
            "--- Iteration 9988: Training loss = 0.0069, 0.0433 s ---\n",
            "--- Iteration 9989: Training loss = 0.0091, 0.0449 s ---\n",
            "--- Iteration 9990: Training loss = 0.0093, 0.0442 s ---\n",
            "--- Iteration 9990: Test loss = 0.0397 ---\n",
            "\n",
            "--- Iteration 9991: Training loss = 0.0099, 0.0443 s ---\n",
            "--- Iteration 9992: Training loss = 0.0099, 0.0430 s ---\n",
            "--- Iteration 9993: Training loss = 0.0084, 0.0424 s ---\n",
            "--- Iteration 9994: Training loss = 0.0067, 0.0424 s ---\n",
            "--- Iteration 9995: Training loss = 0.0092, 0.0432 s ---\n",
            "--- Iteration 9996: Training loss = 0.0117, 0.0434 s ---\n",
            "--- Iteration 9997: Training loss = 0.0087, 0.0453 s ---\n",
            "--- Iteration 9998: Training loss = 0.0048, 0.0437 s ---\n",
            "--- Iteration 9999: Training loss = 0.0122, 0.0429 s ---\n",
            "--- Iteration 10000: Training loss = 0.0096, 0.0425 s ---\n",
            "--- Iteration 10000: Test loss = 0.0609 ---\n",
            "\n",
            "Training took 469.044s in total.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualise the segmentation results"
      ],
      "metadata": {
        "id": "89yjxjGyb6yT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random batch of test images\n",
        "# Segment the images using the trained model\n",
        "images, labels = test_set.get_random_batch(4)\n",
        "images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
        "images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
        "model.eval()\n",
        "logits = model(images)\n",
        "prob = F.softmax(logits, dim=1)\n",
        "seg = torch.argmax(prob, dim=1)"
      ],
      "metadata": {
        "id": "czWxBFV2enU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise the image, automated segmentation and manual segmentation\n",
        "fig, axs = plt.subplots(4, 3, figsize=(12, 16))\n",
        "### Insert your code ###\n",
        "for i in range(4):\n",
        "    axs[i,0].imshow(images[i,0].cpu(), cmap='gray')\n",
        "    axs[i,0].set_title('Input')\n",
        "    axs[i,1].imshow(labels[i].cpu(), cmap='gray')\n",
        "    axs[i,1].set_title('GT')\n",
        "    axs[i,2].imshow(seg[i].cpu(), cmap='gray')\n",
        "    axs[i,2].set_title('Prediction')\n",
        "### End your code ###"
      ],
      "metadata": {
        "id": "wZeLE0qZjd2j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934
        },
        "outputId": "64b0a0fc-bee9-4db2-a256-2ad0adfea7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x1152 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GtD-HJNURLui"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}