{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dudeurv/SAM_MRI/blob/main/U_NET_old.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6EGVhay7QXJ"
      },
      "source": [
        "# Medical Image Segmentation with U-NET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHYRScZG7PdG"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import tarfile\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbWClN6T8Db2"
      },
      "source": [
        "## Download the imaging dataset\n",
        "\n",
        "The dataset is curated from the brain imaging dataset in [Medical Decathlon Challenge](http://medicaldecathlon.com/).\n",
        "The dataset consists of a training set and a test set. Each image is of dimension 120 x 120, with a corresponding label map of the same dimension. There are four number of classes in the label map:\n",
        "- 0: background\n",
        "- 1: edema\n",
        "- 2: non-enhancing tumour\n",
        "- 3: enhancing tumour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxRBQtHG8Dx-",
        "outputId": "740258b5-591c-49f7-8d22-74466a03929e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-26 23:15:01--  https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz [following]\n",
            "--2023-11-26 23:15:02--  https://www.dropbox.com/s/raw/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb021cf3807e35522573381bebc.dl.dropboxusercontent.com/cd/0/inline/CITeEIbVfG-xiym9AZiO8ZsyI8UjPROEq7WL6_pReJL7EVWGSGdDprpY3iJEJ-YUMhqEe2N-GOKGbaH-IzmQGi_v0c7Yjchgj7YpKkGYeohaE4dfwECHWkYGBFKQLF_Zv0Q/file# [following]\n",
            "--2023-11-26 23:15:02--  https://ucb021cf3807e35522573381bebc.dl.dropboxusercontent.com/cd/0/inline/CITeEIbVfG-xiym9AZiO8ZsyI8UjPROEq7WL6_pReJL7EVWGSGdDprpY3iJEJ-YUMhqEe2N-GOKGbaH-IzmQGi_v0c7Yjchgj7YpKkGYeohaE4dfwECHWkYGBFKQLF_Zv0Q/file\n",
            "Resolving ucb021cf3807e35522573381bebc.dl.dropboxusercontent.com (ucb021cf3807e35522573381bebc.dl.dropboxusercontent.com)... 162.125.4.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to ucb021cf3807e35522573381bebc.dl.dropboxusercontent.com (ucb021cf3807e35522573381bebc.dl.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CIQvk7o0pfTL85t0TmFygwUPInRiMHwpiRRgEO0SkGq2z1aRKkNvx1gijT7y5tHI_GIoUKYC8aonw39LpGPQU47koePXhpHqU3xn-OQCGSYKiIOsOaNlfYd-JU3yPW_EsMZHpn8-itVXdIjf5fkFZPBk8LS0rUQOk0XZIcU3O3KJ8727SOOJ9s-qgIIalnVS52EPZ7xBM3_8Sl80yzqkzBZEQP5xCeTfWONzhePhkaqZrpIfFFym2bplltdyRcjZcyrO9utRMcvdDnKzt31Pnal5x3NA-gCiO7wEwpP_1XmZaKxQf6gFhP29bv0jKBhJzkKAP7u835rmBmQdmJ9Dm5rvlCCFtDJxoVF1TLme_na59w/file [following]\n",
            "--2023-11-26 23:15:03--  https://ucb021cf3807e35522573381bebc.dl.dropboxusercontent.com/cd/0/inline2/CIQvk7o0pfTL85t0TmFygwUPInRiMHwpiRRgEO0SkGq2z1aRKkNvx1gijT7y5tHI_GIoUKYC8aonw39LpGPQU47koePXhpHqU3xn-OQCGSYKiIOsOaNlfYd-JU3yPW_EsMZHpn8-itVXdIjf5fkFZPBk8LS0rUQOk0XZIcU3O3KJ8727SOOJ9s-qgIIalnVS52EPZ7xBM3_8Sl80yzqkzBZEQP5xCeTfWONzhePhkaqZrpIfFFym2bplltdyRcjZcyrO9utRMcvdDnKzt31Pnal5x3NA-gCiO7wEwpP_1XmZaKxQf6gFhP29bv0jKBhJzkKAP7u835rmBmQdmJ9Dm5rvlCCFtDJxoVF1TLme_na59w/file\n",
            "Reusing existing connection to ucb021cf3807e35522573381bebc.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9251149 (8.8M) [application/octet-stream]\n",
            "Saving to: ‘Task01_BrainTumour_2D.tar.gz.2’\n",
            "\n",
            "Task01_BrainTumour_ 100%[===================>]   8.82M  12.9MB/s    in 0.7s    \n",
            "\n",
            "2023-11-26 23:15:05 (12.9 MB/s) - ‘Task01_BrainTumour_2D.tar.gz.2’ saved [9251149/9251149]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!wget https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
        "\n",
        "# Unzip the '.tar.gz' file to the current directory\n",
        "datafile = tarfile.open('Task01_BrainTumour_2D.tar.gz')\n",
        "datafile.extractall()\n",
        "datafile.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1BYUO-a7UHC"
      },
      "source": [
        "## Implement a dataset class\n",
        "\n",
        "  **Documentation**:\n",
        "  - os.listdir: [https://docs.python.org/3/library/os.html#os.listdir](https://docs.python.org/3/library/os.html#os.listdir)\n",
        "  - os.path.join: [https://docs.python.org/3/library/os.path.html#os.path.join](https://docs.python.org/3/library/os.path.html#os.path.join)\n",
        "  - imageio.imread: [https://imageio.readthedocs.io/en/stable/userapi.html#imageio.imread](https://imageio.readthedocs.io/en/stable/userapi.html#imageio.imread)\n",
        "  - random.sample: [https://docs.python.org/3/library/random.html#random.sample](https://docs.python.org/3/library/random.html#random.sample)\n",
        "  - numpy.expand_dims: [https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVviaEkj-tF8"
      },
      "outputs": [],
      "source": [
        "def normalise_intensity(image, ROI_thres=0.1):\n",
        "    \"\"\"\n",
        "    The function identifies the ROI in the image by applying a percentile-based threshold,\n",
        "    then standardizes the pixel values in this region by subtracting the mean and dividing\n",
        "    by the standard deviation.\n",
        "\n",
        "    Args:\n",
        "        image (np.array): Input image as a NumPy array.\n",
        "        ROI_thres (float): Percentile threshold for defining the ROI (default is 0.1).\n",
        "\n",
        "    Returns:\n",
        "        np.array: Image array with normalized intensity in the ROI.\n",
        "    \"\"\"\n",
        "    pixel_thres = np.percentile(image, ROI_thres)\n",
        "    ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "    mean = np.mean(ROI)\n",
        "    std = np.std(ROI)\n",
        "    ROI_norm = (ROI - mean)/std # Normalise ROI\n",
        "    return ROI_norm\n",
        "\n",
        "class BrainImage():\n",
        "    def __init__(self, image_path, label_path, deploy=False):\n",
        "      # Initialise instant variables\n",
        "      self.image_path = image_path\n",
        "      self.label_path = label_path\n",
        "      self.deploy = deploy # If deploy=True this means model is in testing mode\n",
        "      self.images = [] # List of loaded image arrays\n",
        "      self.labels = [] # List of loaded label arrays\n",
        "\n",
        "      image_names = sorted(os.listdir(image_path)) # Sorted list containing image filenames to ensure a consistent order for data processing.\n",
        "      for image_name in image_names:\n",
        "        full_image_path = os.path.join(image_path, image_name)\n",
        "        image = iio.imread(full_image_path) # Loads image into a processable NumPy array.\n",
        "        self.images.append(image)\n",
        "\n",
        "        if deploy == False: # If model is in training mode, load the labels as well\n",
        "          full_label_path = os.path.join(label_path, image_name)\n",
        "          label = iio.imread(full_label_path)\n",
        "          self.labels.append(label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get an image and perform intensity normalisation\n",
        "        image = normalise_intensity(self.images[idx])\n",
        "\n",
        "        # Get its label map\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "    def get_random_batch(self, batch_size):\n",
        "        # Get a batch of paired images and label maps\n",
        "        images_batch, labels_batch = [], []\n",
        "\n",
        "        idx_array = range(0, len(self.images)) # Creates an array of indices ranging from 0 to len(self.images)\n",
        "        batch_idx = random.sample(idx_array, batch_size) # Randomly selects a batch_size number of indices\n",
        "        for i in range(batch_size):\n",
        "            image, label = self.__getitem__(batch_idx[i])\n",
        "            images_batch.append(image)\n",
        "            labels_batch.append(label)\n",
        "\n",
        "        images_batch, labels_batch = np.array(images_batch), np.array(labels_batch)\n",
        "        images_batch = np.expand_dims(images_batch, 1)\n",
        "        return images_batch, labels_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB3bPQh--vRc"
      },
      "source": [
        "## Build a U-net architecture\n",
        "ters. This layer maps the deep features to the output classes or segments.\n",
        "\n",
        "#### Documentation\n",
        "\n",
        "- nn.Module: [https://pytorch.org/docs/stable/nn.html#module](https://pytorch.org/docs/stable/nn.html#module)\n",
        "- Conv2d: [https://pytorch.org/docs/stable/nn.html#conv2d](https://pytorch.org/docs/stable/nn.html#conv2d)\n",
        "- BatchNorm2d: [https://pytorch.org/docs/stable/nn.html#batchnorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d)\n",
        "- ReLU: [https://pytorch.org/docs/stable/nn.html#relu](https://pytorch.org/docs/stable/nn.html#relu)\n",
        "- ConvTranspose2d: [https://pytorch.org/docs/stable/nn.html#convtranspose2d](https://pytorch.org/docs/stable/nn.html#convtranspose2d)\n",
        "- torch.cat: [https://pytorch.org/docs/stable/generated/torch.cat.html](https://pytorch.org/docs/stable/generated/torch.cat.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOcyhyR7Cd6u",
        "outputId": "354cee22-4207-46d9-ba87-8d7d1722a3be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[ 0.0748, -0.1294,  0.0628,  ...,  0.2696,  0.8892,  0.6583],\n",
              "          [-0.6565, -0.1584,  0.5305,  ...,  0.1732,  0.2609,  0.5503],\n",
              "          [ 0.1902,  0.5981, -0.3722,  ...,  0.2152, -0.8992, -0.0185],\n",
              "          ...,\n",
              "          [ 0.3166,  0.7364,  0.8428,  ...,  0.9040,  0.0244,  0.2340],\n",
              "          [ 0.1234,  0.5722,  0.0969,  ...,  1.3587,  0.8660, -0.3447],\n",
              "          [-0.1741,  0.0303,  0.0670,  ..., -0.3541, -0.5911, -0.9321]]]],\n",
              "       grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Double_Convolution(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    # 2 sets of: 3x3 Convolution layers, a ReLU activation to add non-linearity, and Batch Normalisation\n",
        "    # Bias is set as false, as Batch Normalisation would remove the bias\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "class UNETModel(nn.Module):\n",
        "  # U-Net Model involves an encoder, a bottleneck and decoder section\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Encoder with 4 blocks of Double_Convolution layers\n",
        "    # Includes 4 sets of max pooling operations with kernel 2x2 and stride 2 for downsampling\n",
        "    self.down_1 = nn.Sequential(\n",
        "        Double_Convolution(1, 64),\n",
        "        nn.MaxPool2d(2, 2)\n",
        "    )\n",
        "    self.down_2 = nn.Sequential(\n",
        "        Double_Convolution(64, 128),\n",
        "        nn.MaxPool2d(2, 2)\n",
        "    )\n",
        "    self.down_3 = nn.Sequential(\n",
        "        Double_Convolution(128, 256),\n",
        "        nn.MaxPool2d(2, 2)\n",
        "    )\n",
        "    self.down_4 = nn.Sequential(\n",
        "        Double_Convolution(256, 512),\n",
        "        nn.MaxPool2d(2, 2)\n",
        "    )\n",
        "    # Bottleneck\n",
        "    # Includes a Double_Convolution layer followed by upsampling with ConvTranspose2d\n",
        "    self.bottleneck = nn.Sequential(\n",
        "        Double_Convolution(512, 1024),\n",
        "        nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
        "    )\n",
        "    # Decoder\n",
        "    # Concatinating with skip connections causes feature channels to double\n",
        "    # Double_Convolution and ConvTranspose2d each causes feature channels to halve\n",
        "    self.up_1 = nn.Sequential(\n",
        "        Double_Convolution(1024, 512),\n",
        "        nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
        "    )\n",
        "    self.up_2 = nn.Sequential(\n",
        "        Double_Convolution(512, 256),\n",
        "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
        "    )\n",
        "    self.up_3 = nn.Sequential(\n",
        "        Double_Convolution(256, 128),\n",
        "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
        "    )\n",
        "    # Final Output Layer\n",
        "    self.out = nn.Sequential(\n",
        "        Double_Convolution(128, 64),\n",
        "        nn.Conv2d(64, 1, 3, 1, 1, bias=False)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    # Encoder\n",
        "    x1 = self.down_1(x)  # Size [1, 64, 263, 263]\n",
        "    x2 = self.down_2(x1) # Size [1, 128, 131, 131]\n",
        "    x3 = self.down_3(x2) # Size [1, 256, 65, 65]\n",
        "    x4 = self.down_4(x3) # Size [1, 512, 32, 32]\n",
        "\n",
        "    # Bottleneck\n",
        "    x5 = self.bottleneck(x4) # Size [1, 512, 64, 64]\n",
        "\n",
        "    # Decoder\n",
        "    skip_x4 = F.interpolate(x4, size=x5.size()[2:])\n",
        "    x6 = self.up_1(torch.cat([x5, skip_x4], dim=1))\n",
        "\n",
        "    skip_x3 = F.interpolate(x3, size=x6.size()[2:])\n",
        "    x7 = self.up_2(torch.cat([x6, skip_x3], dim=1))\n",
        "\n",
        "    skip_x2 = F.interpolate(x2, size=x7.size()[2:])\n",
        "    x8 = self.up_3(torch.cat([x7, skip_x2], dim=1))\n",
        "\n",
        "    # Final Output Layer\n",
        "    skip_x1 = F.interpolate(x1, size=x8.size()[2:])\n",
        "    x9 = self.out(torch.cat([x8, skip_x1], dim=1))\n",
        "\n",
        "    return x9\n",
        "\n",
        "model = UNETModel()\n",
        "model(torch.rand(1, 1, 527, 527))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfIE0mrSCfeZ"
      },
      "source": [
        "## Train the segmentation model\n",
        "\n",
        "#### Documentation\n",
        "- Model Saving and Loading: [PyTorch Saving & Loading](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
        "- Optimizers: [PyTorch Optim](https://pytorch.org/docs/stable/optim.html)\n",
        "- Loss Functions: [PyTorch Losses](https://pytorch.org/docs/stable/nn.html#loss-functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfbExfTOH8Ko",
        "outputId": "39a340b9-2fb9-4783-a40a-df9452a5fb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device = cuda\n",
            "Loss during training is 22.904769897460938. Loss turing testing is 13.991179466247559 \n",
            "Loss during training is 13.491751670837402. Loss turing testing is 21.564285278320312 \n",
            "Loss during training is 13.673611640930176. Loss turing testing is 18.300764083862305 \n",
            "Loss during training is 17.817598342895508. Loss turing testing is 13.167884826660156 \n",
            "Loss during training is 14.705232620239258. Loss turing testing is 19.41193199157715 \n",
            "Loss during training is 9.767669677734375. Loss turing testing is 13.327753067016602 \n",
            "Loss during training is 12.218700408935547. Loss turing testing is 11.078338623046875 \n",
            "Loss during training is 18.093059539794922. Loss turing testing is 8.34848403930664 \n",
            "Loss during training is 12.94313907623291. Loss turing testing is 10.614462852478027 \n",
            "Loss during training is 8.7139253616333. Loss turing testing is 17.9211483001709 \n"
          ]
        }
      ],
      "source": [
        "# Use GPU if cuda is available\n",
        "device = \"cuda\"\n",
        "print(f\"Device = {device}\")\n",
        "\n",
        "# Instantiate the UNET model\n",
        "model = UNETModel()\n",
        "model = model.to(device)\n",
        "params = model.parameters()\n",
        "\n",
        "# Set up optimizer and loss function\n",
        "optimizer = optim.Adam(params, lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Instantiate BrainImageSet for both training and test sets with appropriate image and label paths\n",
        "BrainImage_train = BrainImage('Task01_BrainTumour_2D/training_images', 'Task01_BrainTumour_2D/training_labels')\n",
        "BrainImage_test = BrainImage('Task01_BrainTumour_2D/test_images', 'Task01_BrainTumour_2D/test_labels')\n",
        "\n",
        "epochs = 5000\n",
        "for epoch in range(epochs):\n",
        "  # Fetch a batch of training data and transfer it to the device\n",
        "  train_images, train_labels = BrainImage_train.get_random_batch(batch_size=30)\n",
        "  train_images, train_labels = torch.from_numpy(train_images).to(device, dtype=torch.float32), torch.from_numpy(train_labels).to(device, dtype=torch.float32)\n",
        "\n",
        "  # Perform a forward pass through the model\n",
        "  model_labels = model(train_images)\n",
        "\n",
        "  # Match model labels size to target labels\n",
        "  model_labels = F.interpolate(model_labels, size=train_labels.shape[1:])\n",
        "\n",
        "  # Ensure model output and labels have the same shape\n",
        "  model_labels = model_labels.squeeze()  # Remove the channel dimension if it's 1\n",
        "  train_labels = train_labels.squeeze()  # Same for labels\n",
        "\n",
        "  optimizer.zero_grad() # Clear previous gradients\n",
        "  loss = loss_fn(model_labels, train_labels) # Compute loss\n",
        "  loss.backward() # Carry out backpropagation and calculate gradients\n",
        "  optimizer.step() # Update model parameters\n",
        "\n",
        "  # Evaluate model\n",
        "  if epoch % 500 == 0:\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      # Fetch a batch of testing data and transfer it to the device\n",
        "      test_images, test_labels = BrainImage_test.get_random_batch(batch_size=30)\n",
        "      test_images, test_labels = torch.from_numpy(test_images).to(device, dtype=torch.float32), torch.from_numpy(test_labels).to(device, dtype=torch.float32)\n",
        "\n",
        "      pred_labels = model(test_images)\n",
        "\n",
        "      pred_labels = F.interpolate(pred_labels, size=test_labels.shape[1:])\n",
        "\n",
        "      # Ensure they have the same shape\n",
        "      pred_labels = pred_labels.squeeze()  # Remove the channel dimension if it's 1\n",
        "      test_labels = test_labels.squeeze()  # Same for labels\n",
        "\n",
        "      test_loss = loss_fn(pred_labels, test_labels)\n",
        "      print(f\"Loss during training is {loss}. Loss turing testing is {test_loss} \")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc8KwbLRS1/I8G3CeuI0V5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}