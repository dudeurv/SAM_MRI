{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL9mPLI/WySPQ7G8+nIZVq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dudeurv/SAM_MRI/blob/main/U_NET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medical Image Segmentation with U-NET Tutorial\n",
        "\n",
        "In this tutorial, you will develop and train a convolutional neural network for brain tumour image segmentation."
      ],
      "metadata": {
        "id": "A6EGVhay7QXJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rHYRScZG7PdG"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import tarfile\n",
        "import imageio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the imaging dataset\n",
        "\n",
        "The dataset is curated from the brain imaging dataset in [Medical Decathlon Challenge](http://medicaldecathlon.com/). To save the storage and reduce the computational cost for this tutorial, we extract 2D image slices from T1-Gd contrast enhanced 3D brain volumes and downsample the images.\n",
        "\n",
        "The dataset consists of a training set and a test set. Each image is of dimension 120 x 120, with a corresponding label map of the same dimension. There are four number of classes in the label map:\n",
        "\n",
        "- 0: background\n",
        "- 1: edema\n",
        "- 2: non-enhancing tumour\n",
        "- 3: enhancing tumour"
      ],
      "metadata": {
        "id": "xbWClN6T8Db2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!wget https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
        "\n",
        "# Unzip the '.tar.gz' file to the current directory\n",
        "datafile = tarfile.open('Task01_BrainTumour_2D.tar.gz')\n",
        "datafile.extractall()\n",
        "datafile.close()"
      ],
      "metadata": {
        "id": "IxRBQtHG8Dx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a dataset class\n",
        "\n",
        "#### 1. Implement `normalise_intensity` Function\n",
        "\n",
        "- **Goal**: Normalize the intensity of an image, focusing on the Region of Interest (ROI).\n",
        "  \n",
        "  **Inputs**:\n",
        "  1. `image`: A NumPy array (2D) of the image.\n",
        "  2. `thres_roi`: A float value for the percentile threshold (default 1.0).\n",
        "\n",
        "  **Process**:\n",
        "  1. Calculate the intensity threshold using `np.percentile`. This step isolates significant parts of the image for focused normalization.\n",
        "  2. Create a binary ROI mask where pixels greater than or equal to the threshold are marked True.\n",
        "  3. Compute the mean and standard deviation of the pixel values within the ROI. These statistics are crucial for normalizing the image.\n",
        "  4. Normalize the image: adjust pixel values based on the calculated mean and standard deviation. This standardization is key for enhancing model training and analysis effectiveness.\n",
        "\n",
        "  **Outputs**:\n",
        "  - Normalized image: A NumPy array with adjusted intensity values.\n",
        "\n",
        "  **Documentation**:\n",
        "  - NumPy Percentile: [https://numpy.org/doc/stable/reference/generated/numpy.percentile.html](https://numpy.org/doc/stable/reference/generated/numpy.percentile.html)\n",
        "  - NumPy Mean: [https://numpy.org/doc/stable/reference/generated/numpy.mean.html](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)\n",
        "  - NumPy Standard Deviation: [https://numpy.org/doc/stable/reference/generated/numpy.std.html](https://numpy.org/doc/stable/reference/generated/numpy.std.html)\n",
        "\n",
        "#### 2. Define `BrainImageSet` Class\n",
        "\n",
        "- **Goal**: Manage and preprocess a dataset of brain images for neural network models.\n",
        "\n",
        "  **Constructor `__init__`**:\n",
        "  - **Inputs**:\n",
        "    1. `image_path`: Path to image directory.\n",
        "    2. `label_path`: Path to label directory (optional, default empty).\n",
        "    3. `deploy`: Boolean for deployment mode (optional, default False).\n",
        "  - **Process**:\n",
        "    1. Initialize instance variables for paths, deployment status, and image/label lists.\n",
        "    2. Load and sort image filenames to ensure a consistent order for data processing.\n",
        "    3. Read images and optionally labels using `imageio.imread`, converting files into processable NumPy arrays.\n",
        "    4. Skip loading labels if `deploy` is True, as labels are not used in deployment mode.\n",
        "\n",
        "  **Method `__len__`**:\n",
        "  - **Outputs**: Total number of images in the dataset (Integer).\n",
        "\n",
        "  **Method `__getitem__`**:\n",
        "  - **Inputs**: `idx` (Integer index for an image/label pair).\n",
        "  - **Outputs**: Tuple of a normalized image and its label (both NumPy arrays).\n",
        "  - **Process**: Fetch and preprocess an image for the given index.\n",
        "\n",
        "  **Method `get_random_batch`**:\n",
        "  - **Inputs**: `batch_size` (Integer for the number of pairs to include).\n",
        "  - **Outputs**: Tuple of a batch of images and labels (both NumPy arrays).\n",
        "  - **Process**:\n",
        "    1. Select random indices to form a batch. This randomization is essential for effective neural network training.\n",
        "    2. Retrieve and normalize images and labels for these indices.\n",
        "    3. Convert the lists of images and labels to NumPy arrays.\n",
        "    4. Use `np.expand_dims` on images to add a necessary dimension. Neural networks typically expect a specific input shape, including a channel dimension even for grayscale images.\n",
        "\n",
        "  **Documentation**:\n",
        "  - os.listdir: [https://docs.python.org/3/library/os.html#os.listdir](https://docs.python.org/3/library/os.html#os.listdir)\n",
        "  - os.path.join: [https://docs.python.org/3/library/os.path.html#os.path.join](https://docs.python.org/3/library/os.path.html#os.path.join)\n",
        "  - imageio.imread: [https://imageio.readthedocs.io/en/stable/userapi.html#imageio.imread](https://imageio.readthedocs.io/en/stable/userapi.html#imageio.imread)\n",
        "  - random.sample: [https://docs.python.org/3/library/random.html#random.sample](https://docs.python.org/3/library/random.html#random.sample)\n",
        "  - numpy.expand_dims: [https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html)"
      ],
      "metadata": {
        "id": "J1BYUO-a7UHC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TVviaEkj-tF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a U-net architecture\n",
        "#### 1. Set Up the Basic Class Structure\n",
        "- Define a Python class named `UNet` that inherits from `nn.Module`.\n",
        "- In the `__init__` method, set up the basic structure:\n",
        "  - Define initial variables: `input_channel`, `output_channel`, `num_filter`.\n",
        "\n",
        "#### 2. Create Your First Encoder Block\n",
        "- Start with creating your first encoder block (`self.conv1`):\n",
        "  - Use `nn.Sequential` to chain layers.\n",
        "  - Add two `nn.Conv2d` layers, each followed by `nn.BatchNorm2d` and `nn.ReLU`. The first Conv2d layer should change the channel size from `input_channel` to `num_filter`, and the second Conv2d layer keeps the channel size at `num_filter`.\n",
        "  - For both Conv2d layers, use `kernel_size=3` and `padding=1`.\n",
        "\n",
        "#### 3. Add Subsequent Encoder Blocks\n",
        "- Create the next encoder blocks (`self.conv2`, `self.conv3`, `self.conv4`):\n",
        "  - For each block, double the number of filters (e.g., `num_filter * 2`, `num_filter * 4`, etc.).\n",
        "  - Add two Conv2d layers in each block, similar to `self.conv1`, but with the updated number of filters. For the first Conv2d layer in each block, use `stride=2` for downsampling.\n",
        "  - Remember to include BatchNorm and ReLU layers after each Conv2d layer.\n",
        "\n",
        "#### 4. Construct Decoder Blocks\n",
        "- For each decoder block (`self.up3`, `self.conv_up3`, `self.up2`, `self.conv_up2`, `self.up1`, `self.conv_up1`):\n",
        "  - Start with a `nn.ConvTranspose2d` for upsampling (reverse of downsampling). The number of filters should be halved compared to the preceding block.\n",
        "  - Follow it with a `nn.Sequential` containing two Conv2d layers (like the encoder), but this time the number of filters decreases with each block.\n",
        "\n",
        "#### 5. Define the Output Convolution\n",
        "- Create the final output layer (`self.out`) with a kernel size of 1 using `nn.Conv2d` with `output_channel` filters. This layer maps the deep features to the output classes or segments.\n",
        "\n",
        "#### 6. Define the Forward Pass\n",
        "\n",
        "1. **Encoder Forward Pass**:\n",
        "   - Sequentially pass input `x` through each encoder block.\n",
        "   - Save the output of each block for skip connections.\n",
        "\n",
        "2. **Decoder Forward Pass**:\n",
        "   - Upsample the output of the last encoder block.\n",
        "   - For each decoder block:\n",
        "     - Concatenate the upsampled output with the corresponding encoder output using `torch.cat`. Ensure that the feature dimensions are aligned.\n",
        "     - Pass the concatenated output through the next decoder block.\n",
        "\n",
        "3. **Final Output**:\n",
        "   - Pass the output of the last decoder block through the output layer to obtain the final result.\n",
        "\n",
        "#### Documentation\n",
        "\n",
        "- nn.Module: [https://pytorch.org/docs/stable/nn.html#module](https://pytorch.org/docs/stable/nn.html#module)\n",
        "- Conv2d: [https://pytorch.org/docs/stable/nn.html#conv2d](https://pytorch.org/docs/stable/nn.html#conv2d)\n",
        "- BatchNorm2d: [https://pytorch.org/docs/stable/nn.html#batchnorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d)\n",
        "- ReLU: [https://pytorch.org/docs/stable/nn.html#relu](https://pytorch.org/docs/stable/nn.html#relu)\n",
        "- ConvTranspose2d: [https://pytorch.org/docs/stable/nn.html#convtranspose2d](https://pytorch.org/docs/stable/nn.html#convtranspose2d)\n",
        "- torch.cat: [https://pytorch.org/docs/stable/generated/torch.cat.html](https://pytorch.org/docs/stable/generated/torch.cat.html)"
      ],
      "metadata": {
        "id": "FB3bPQh--vRc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SOcyhyR7Cd6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the segmentation model\n",
        "#### 1. Build the Model\n",
        "- **Goal**: Instantiate the U-Net model.\n",
        "- **Action**:\n",
        "  - Define `num_class`, the number of output classes for the segmentation task.\n",
        "  - Create an instance of the `UNet` class with specified input and output channels, and the number of filters.\n",
        "  - Transfer the model to the chosen device (`model.to(device)`).\n",
        "\n",
        "#### 2. Prepare for Saving Models\n",
        "- **Goal**: Set up a directory to save trained model parameters.\n",
        "- **Action**: Check if a directory (e.g., `saved_models`) exists; if not, create it.\n",
        "\n",
        "#### 3. Define the Optimizer\n",
        "- **Goal**: Set up the optimizer for training.\n",
        "- **Action**: Use `optim.Adam(params, lr=1e-3)` with the model's parameters and a learning rate.\n",
        "\n",
        "#### 4. Set Up the Loss Function\n",
        "- **Goal**: Define the loss function for the segmentation task.\n",
        "- **Action**: Use `nn.CrossEntropyLoss()` as the criterion.\n",
        "\n",
        "#### 5. Load Datasets\n",
        "- **Goal**: Prepare training and test datasets.\n",
        "- **Action**: Instantiate `BrainImageSet` for both training and test sets with appropriate image and label paths.\n",
        "\n",
        "#### 6. Train the Model\n",
        "- **Goal**: Implement the training loop.\n",
        "- **Actions**:\n",
        "  - Iterate over a specified number of iterations (`num_iter`).\n",
        "  - In each iteration:\n",
        "    - Set the model to training mode (`model.train()`).\n",
        "    - Fetch a batch of training data and transfer it to the device.\n",
        "    - Perform a forward pass through the model (`logits = model(images)`).\n",
        "    - Clear previous gradients (`optimizer.zero_grad()`).\n",
        "    - Compute loss using `criterion`.\n",
        "    - Perform backpropagation (`loss.backward()`).\n",
        "    - Update model parameters (`optimizer.step()`).\n",
        "    - Print training loss.\n",
        "\n",
        "#### 7. Evaluate the Model\n",
        "- **Goal**: Periodically evaluate the model on the test set.\n",
        "- **Actions**:\n",
        "  - Every few iterations (e.g., `it % 10 == 0`):\n",
        "    - Set the model to evaluation mode (`model.eval()`).\n",
        "    - Disable gradient calculations (`with torch.no_grad():`).\n",
        "    - Fetch a batch of test data and transfer it to the device.\n",
        "    - Perform a forward pass and compute the test loss.\n",
        "    - Print the test loss.\n",
        "\n",
        "#### 8. Save the Model\n",
        "- **Goal**: Save the model's state at certain intervals.\n",
        "- **Action**: Every few iterations (e.g., `it % 5000 == 0`), save the model's state dictionary.\n",
        "\n",
        "#### Documentation\n",
        "- Model Saving and Loading: [PyTorch Saving & Loading](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
        "- Optimizers: [PyTorch Optim](https://pytorch.org/docs/stable/optim.html)\n",
        "- Loss Functions: [PyTorch Losses](https://pytorch.org/docs/stable/nn.html#loss-functions)"
      ],
      "metadata": {
        "id": "yfIE0mrSCfeZ"
      }
    }
  ]
}