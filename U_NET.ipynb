{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPMZj3sZdb75xitky5ff8it",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dudeurv/SAM_MRI/blob/main/U_NET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medical Image Segmentation with U-NET Tutorial\n",
        "\n",
        "In this tutorial, you will develop and train a convolutional neural network for brain tumour image segmentation."
      ],
      "metadata": {
        "id": "A6EGVhay7QXJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rHYRScZG7PdG"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import tarfile\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the imaging dataset\n",
        "\n",
        "The dataset is curated from the brain imaging dataset in [Medical Decathlon Challenge](http://medicaldecathlon.com/). To save the storage and reduce the computational cost for this tutorial, we extract 2D image slices from T1-Gd contrast enhanced 3D brain volumes and downsample the images.\n",
        "\n",
        "The dataset consists of a training set and a test set. Each image is of dimension 120 x 120, with a corresponding label map of the same dimension. There are four number of classes in the label map:\n",
        "\n",
        "- 0: background\n",
        "- 1: edema\n",
        "- 2: non-enhancing tumour\n",
        "- 3: enhancing tumour"
      ],
      "metadata": {
        "id": "xbWClN6T8Db2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!wget https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
        "\n",
        "# Unzip the '.tar.gz' file to the current directory\n",
        "datafile = tarfile.open('Task01_BrainTumour_2D.tar.gz')\n",
        "datafile.extractall()\n",
        "datafile.close()"
      ],
      "metadata": {
        "id": "IxRBQtHG8Dx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d38f49-050a-4e2c-809b-27e780d5804d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-26 21:07:12--  https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz [following]\n",
            "--2023-11-26 21:07:13--  https://www.dropbox.com/s/raw/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucee335ccf10ef66ab61c598eb5e.dl.dropboxusercontent.com/cd/0/inline/CITEE5ropihrQGPKYCHZTlVUQgWyO8s8YbU5XZpdOnqxlUG9yjSYQ6O8BW8ZisBQJCrQXsrepXQajaRkTScAMV4sWDXEehEjdYg2Xf5Xt6kcIK4qv9ySFC97FanYk7xS2n4/file# [following]\n",
            "--2023-11-26 21:07:14--  https://ucee335ccf10ef66ab61c598eb5e.dl.dropboxusercontent.com/cd/0/inline/CITEE5ropihrQGPKYCHZTlVUQgWyO8s8YbU5XZpdOnqxlUG9yjSYQ6O8BW8ZisBQJCrQXsrepXQajaRkTScAMV4sWDXEehEjdYg2Xf5Xt6kcIK4qv9ySFC97FanYk7xS2n4/file\n",
            "Resolving ucee335ccf10ef66ab61c598eb5e.dl.dropboxusercontent.com (ucee335ccf10ef66ab61c598eb5e.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6022:15::a27d:420f\n",
            "Connecting to ucee335ccf10ef66ab61c598eb5e.dl.dropboxusercontent.com (ucee335ccf10ef66ab61c598eb5e.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CIT6KcVNAECIsBCPUrxDkbWos_OnGvIsl269ZEXZSZ9XbkxOOQYS5_yYf3guIau-Ve8XyMXd85gpjixx4hVJwd9SmLyarhrvHFaWmnEz68-R6yjOfLuNbwZ-2hwxNweVKzGQa15kQgzFe5VztYJbIA7eJc3vcKOQ3V52mgGRkInHUf5WQIpHtQSlM-mdhuiEqciOwdaZ-eRZIAN8S2WHHHZULVMH5xIbrM0NmhpqyD-KJcxP9SDsKjMi33Hwk-2uLpBK8WeAENVp5buY3oz3HrAc36GL8vGy6SizJRXa0qXsheLVviEUbTPpty9f4OOVg45NjNFA-7YN2ewov7kHcEywYWKl4CxbsPgu1fxClExvig/file [following]\n",
            "--2023-11-26 21:07:15--  https://ucee335ccf10ef66ab61c598eb5e.dl.dropboxusercontent.com/cd/0/inline2/CIT6KcVNAECIsBCPUrxDkbWos_OnGvIsl269ZEXZSZ9XbkxOOQYS5_yYf3guIau-Ve8XyMXd85gpjixx4hVJwd9SmLyarhrvHFaWmnEz68-R6yjOfLuNbwZ-2hwxNweVKzGQa15kQgzFe5VztYJbIA7eJc3vcKOQ3V52mgGRkInHUf5WQIpHtQSlM-mdhuiEqciOwdaZ-eRZIAN8S2WHHHZULVMH5xIbrM0NmhpqyD-KJcxP9SDsKjMi33Hwk-2uLpBK8WeAENVp5buY3oz3HrAc36GL8vGy6SizJRXa0qXsheLVviEUbTPpty9f4OOVg45NjNFA-7YN2ewov7kHcEywYWKl4CxbsPgu1fxClExvig/file\n",
            "Reusing existing connection to ucee335ccf10ef66ab61c598eb5e.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9251149 (8.8M) [application/octet-stream]\n",
            "Saving to: ‘Task01_BrainTumour_2D.tar.gz.1’\n",
            "\n",
            "Task01_BrainTumour_ 100%[===================>]   8.82M  6.19MB/s    in 1.4s    \n",
            "\n",
            "2023-11-26 21:07:17 (6.19 MB/s) - ‘Task01_BrainTumour_2D.tar.gz.1’ saved [9251149/9251149]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a dataset class\n",
        "\n",
        "  **Documentation**:\n",
        "  - os.listdir: [https://docs.python.org/3/library/os.html#os.listdir](https://docs.python.org/3/library/os.html#os.listdir)\n",
        "  - os.path.join: [https://docs.python.org/3/library/os.path.html#os.path.join](https://docs.python.org/3/library/os.path.html#os.path.join)\n",
        "  - imageio.imread: [https://imageio.readthedocs.io/en/stable/userapi.html#imageio.imread](https://imageio.readthedocs.io/en/stable/userapi.html#imageio.imread)\n",
        "  - random.sample: [https://docs.python.org/3/library/random.html#random.sample](https://docs.python.org/3/library/random.html#random.sample)\n",
        "  - numpy.expand_dims: [https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html)"
      ],
      "metadata": {
        "id": "J1BYUO-a7UHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalise_intensity(image, ROI_thres=0.1):\n",
        "    \"\"\"\n",
        "    The function identifies the ROI in the image by applying a percentile-based threshold,\n",
        "    then standardizes the pixel values in this region by subtracting the mean and dividing\n",
        "    by the standard deviation.\n",
        "\n",
        "    Args:\n",
        "        image (np.array): Input image as a NumPy array.\n",
        "        ROI_thres (float): Percentile threshold for defining the ROI (default is 0.1).\n",
        "\n",
        "    Returns:\n",
        "        np.array: Image array with normalized intensity in the ROI.\n",
        "    \"\"\"\n",
        "    pixel_thres = np.percentile(image, ROI_thres)\n",
        "    ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "    mean = np.mean(ROI)\n",
        "    std = np.std(ROI)\n",
        "    ROI_norm = (ROI - mean)/std # Normalise ROI\n",
        "    return ROI_norm\n",
        "\n",
        "class BrainImage():\n",
        "    def __init__(self, image_path, label_path, deploy=False):\n",
        "      # Initialise instant variables\n",
        "      self.image_path = image_path\n",
        "      self.label_path = label_path\n",
        "      self.deploy = deploy # If deploy=True this means model is in testing mode\n",
        "      self.images = [] # List of loaded image arrays\n",
        "      self.labels = [] # List of loaded label arrays\n",
        "\n",
        "      image_names = sorted(os.listdir(image_path)) # Sorted list containing image filenames to ensure a consistent order for data processing.\n",
        "      for image_name in image_names:\n",
        "        full_image_path = os.path.join(image_path, image_name)\n",
        "        image = iio.imread(full_image_path) # Loads image into a processable NumPy array.\n",
        "        self.images.append(image)\n",
        "\n",
        "        if deploy == False: # If model is in training mode, load the labels as well\n",
        "          full_label_path = os.path.join(label_path, image_name)\n",
        "          label = iio.imread(full_label_path)\n",
        "          self.labels.append(label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get an image and perform intensity normalisation\n",
        "        image = normalise_intensity(self.images[idx])\n",
        "\n",
        "        # Get its label map\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "    def get_random_batch(self, batch_size):\n",
        "        # Get a batch of paired images and label maps\n",
        "        images_batch, labels_batch = [], []\n",
        "\n",
        "        idx_array = range(0, len(self.images)) # Creates an array of indices ranging from 0 to len(self.images)\n",
        "        batch_idx = random.sample(idx_array, batch_size) # Randomly selects a batch_size number of indices\n",
        "        for i in range(batch_size):\n",
        "            image, label = self.__getitem__(batch_idx[i])\n",
        "            images_batch.append(image)\n",
        "            labels_batch.append(label)\n",
        "\n",
        "        images_batch, labels_batch = np.array(images_batch), np.array(labels_batch)\n",
        "        images_batch = np.expand_dims(images_batch, 1)\n",
        "        return images_batch, labels_batch"
      ],
      "metadata": {
        "id": "TVviaEkj-tF8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a U-net architecture\n",
        "ters. This layer maps the deep features to the output classes or segments.\n",
        "\n",
        "#### Documentation\n",
        "\n",
        "- nn.Module: [https://pytorch.org/docs/stable/nn.html#module](https://pytorch.org/docs/stable/nn.html#module)\n",
        "- Conv2d: [https://pytorch.org/docs/stable/nn.html#conv2d](https://pytorch.org/docs/stable/nn.html#conv2d)\n",
        "- BatchNorm2d: [https://pytorch.org/docs/stable/nn.html#batchnorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d)\n",
        "- ReLU: [https://pytorch.org/docs/stable/nn.html#relu](https://pytorch.org/docs/stable/nn.html#relu)\n",
        "- ConvTranspose2d: [https://pytorch.org/docs/stable/nn.html#convtranspose2d](https://pytorch.org/docs/stable/nn.html#convtranspose2d)\n",
        "- torch.cat: [https://pytorch.org/docs/stable/generated/torch.cat.html](https://pytorch.org/docs/stable/generated/torch.cat.html)"
      ],
      "metadata": {
        "id": "FB3bPQh--vRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Double_Convolution(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    # 2 sets of: 3x3 Convolution layers, a ReLU activation to add non-linearity, and Batch Normalisation\n",
        "    # Bias is set as false, as Batch Normalisation would remove the bias\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "class UNETModel(nn.Module):\n",
        "  # U-Net Model involves an encoder, a bottleneck and decoder section\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Encoder with 4 blocks of Double_Convolution layers\n",
        "    # Includes 4 sets of max pooling operations with kernel 2x2 and stride 2 for downsampling\n",
        "    self.down_1 = nn.Sequential(\n",
        "        Double_Convolution(1, 64),\n",
        "        nn.MaxPool2d(2, 2)\n",
        "    )\n",
        "    self.down_2 = nn.Sequential(\n",
        "        Double_Convolution(64, 128),\n",
        "        nn.MaxPool2d(2, 2)\n",
        "    )\n",
        "    self.down_3 = nn.Sequential(\n",
        "        Double_Convolution(128, 256),\n",
        "        nn.MaxPool2d(2, 2)\n",
        "    )\n",
        "    self.down_4 = nn.Sequential(\n",
        "        Double_Convolution(256, 512),\n",
        "        nn.MaxPool2d(2, 2)\n",
        "    )\n",
        "    # Bottleneck\n",
        "    # Includes a Double_Convolution layer followed by upsampling with ConvTranspose2d\n",
        "    self.bottleneck = nn.Sequential(\n",
        "        Double_Convolution(512, 1024),\n",
        "        nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
        "    )\n",
        "    # Decoder\n",
        "    # Concatinating with skip connections causes feature channels to double\n",
        "    # Double_Convolution and ConvTranspose2d each causes feature channels to halve\n",
        "    self.up_1 = nn.Sequential(\n",
        "        Double_Convolution(1024, 512),\n",
        "        nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
        "    )\n",
        "    self.up_2 = nn.Sequential(\n",
        "        Double_Convolution(512, 256),\n",
        "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
        "    )\n",
        "    self.up_3 = nn.Sequential(\n",
        "        Double_Convolution(256, 128),\n",
        "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
        "    )\n",
        "    # Final Output Layer\n",
        "    self.out = nn.Sequential(\n",
        "        Double_Convolution(128, 64),\n",
        "        nn.Conv2d(64, 1, 3, 1, 1, bias=False)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    # Encoder\n",
        "    x1 = self.down_1(x)  # Size [1, 64, 263, 263]\n",
        "    x2 = self.down_2(x1) # Size [1, 128, 131, 131]\n",
        "    x3 = self.down_3(x2) # Size [1, 256, 65, 65]\n",
        "    x4 = self.down_4(x3) # Size [1, 512, 32, 32]\n",
        "\n",
        "    # Bottleneck\n",
        "    x5 = self.bottleneck(x4) # Size [1, 512, 64, 64]\n",
        "\n",
        "    # Decoder\n",
        "    skip_x4 = F.interpolate(x4, size=x5.size()[2:])\n",
        "    x6 = self.up_1(torch.cat([x5, skip_x4], dim=1))\n",
        "\n",
        "    skip_x3 = F.interpolate(x3, size=x6.size()[2:])\n",
        "    x7 = self.up_2(torch.cat([x6, skip_x3], dim=1))\n",
        "\n",
        "    skip_x2 = F.interpolate(x2, size=x7.size()[2:])\n",
        "    x8 = self.up_3(torch.cat([x7, skip_x2], dim=1))\n",
        "\n",
        "    # Final Output Layer\n",
        "    skip_x1 = F.interpolate(x1, size=x8.size()[2:])\n",
        "    x9 = self.out(torch.cat([x8, skip_x1], dim=1))\n",
        "\n",
        "    return x9\n",
        "\n",
        "model = UNETModel()\n",
        "model(torch.rand(1, 1, 527, 527))"
      ],
      "metadata": {
        "id": "SOcyhyR7Cd6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1beaacda-12c7-4bc8-8d53-38b3a91e964f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 4.0134e-01,  8.0189e-01, -4.9929e-01,  ...,  1.1166e+00,\n",
              "            8.7047e-01,  7.9254e-01],\n",
              "          [ 1.5676e-01,  1.1310e+00,  9.4324e-01,  ...,  1.2408e+00,\n",
              "            1.4608e+00,  6.9818e-01],\n",
              "          [ 4.0672e-01,  4.6674e-01,  7.0603e-01,  ...,  5.7690e-01,\n",
              "           -8.1200e-03,  1.2300e-01],\n",
              "          ...,\n",
              "          [ 7.2112e-01,  5.4620e-01, -4.2740e-02,  ...,  1.1756e-01,\n",
              "           -1.2450e+00, -1.6777e-01],\n",
              "          [ 3.2768e-01,  7.6127e-01, -8.4678e-01,  ...,  1.6581e-01,\n",
              "           -6.0674e-01, -1.0053e-03],\n",
              "          [-1.8598e-01,  7.9071e-03, -3.7258e-01,  ..., -5.8541e-01,\n",
              "           -2.7512e-01, -4.4655e-01]]]], grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the segmentation model\n",
        "\n",
        "#### 6. Train the Model\n",
        "- **Goal**: Implement the training loop.\n",
        "- **Actions**:\n",
        "  - Iterate over a specified number of iterations (`num_iter`).\n",
        "  - In each iteration:\n",
        "    - Set the model to training mode (`model.train()`).\n",
        "    - Fetch a batch of training data and transfer it to the device.\n",
        "    - Perform a forward pass through the model (`logits = model(images)`).\n",
        "    - Clear previous gradients (`optimizer.zero_grad()`).\n",
        "    - Compute loss using `criterion`.\n",
        "    - Perform backpropagation (`loss.backward()`).\n",
        "    - Update model parameters (`optimizer.step()`).\n",
        "    - Print training loss.\n",
        "\n",
        "#### 7. Evaluate the Model\n",
        "- **Goal**: Periodically evaluate the model on the test set.\n",
        "- **Actions**:\n",
        "  - Every few iterations (e.g., `it % 10 == 0`):\n",
        "    - Set the model to evaluation mode (`model.eval()`).\n",
        "    - Disable gradient calculations (`with torch.no_grad():`).\n",
        "    - Fetch a batch of test data and transfer it to the device.\n",
        "    - Perform a forward pass and compute the test loss.\n",
        "    - Print the test loss.\n",
        "\n",
        "#### 8. Save the Model\n",
        "- **Goal**: Save the model's state at certain intervals.\n",
        "- **Action**: Every few iterations (e.g., `it % 5000 == 0`), save the model's state dictionary.\n",
        "\n",
        "#### Documentation\n",
        "- Model Saving and Loading: [PyTorch Saving & Loading](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
        "- Optimizers: [PyTorch Optim](https://pytorch.org/docs/stable/optim.html)\n",
        "- Loss Functions: [PyTorch Losses](https://pytorch.org/docs/stable/nn.html#loss-functions)"
      ],
      "metadata": {
        "id": "yfIE0mrSCfeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if cuda is available\n",
        "device = \"cuda\"\n",
        "print(f\"Device = {device}\")\n",
        "\n",
        "# Instantiate the UNET model\n",
        "model = UNETModel()\n",
        "model = model.to(device)\n",
        "params = model.parameters()\n",
        "\n",
        "# Set up optimizer and loss function\n",
        "optimizer = optim.Adam(params, lr=0.001)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Instantiate BrainImageSet for both training and test sets with appropriate image and label paths\n",
        "BrainImage_train = BrainImage('Task01_BrainTumour_2D/training_images', 'Task01_BrainTumour_2D/training_labels')\n",
        "BrainImage_test = BrainImage('Task01_BrainTumour_2D/test_images', 'Task01_BrainTumour_2D/test_labels')\n",
        "\n",
        "epochs = 5000\n",
        "for epoch in range(epochs):\n",
        "  # Fetch a batch of training data and transfer it to the device\n",
        "  train_images, train_labels = BrainImage_train.get_random_batch(batch_size=30)\n",
        "  train_images, train_labels = torch.from_numpy(train_images).to(device, dtype=torch.float32), torch.from_numpy(train_labels).to(device, dtype=torch.float32)\n",
        "\n",
        "  # Perform a forward pass through the model\n",
        "  model_labels = model(train_images)\n",
        "\n",
        "  # Match model labels size to target labels\n",
        "  model_labels = F.interpolate(model_labels, size=train_labels.shape[1:])\n",
        "\n",
        "  # Ensure model output and labels have the same shape\n",
        "  model_labels = model_labels.squeeze()  # Remove the channel dimension if it's 1\n",
        "  train_labels = train_labels.squeeze()  # Same for labels\n",
        "\n",
        "  optimizer.zero_grad() # Clear previous gradients\n",
        "  loss = loss_fn(model_labels, train_labels) # Compute loss\n",
        "  loss.backward() # Carry out backpropagation and calculate gradients\n",
        "  optimizer.step() # Update model parameters\n",
        "\n",
        "  # Evaluate model\n",
        "  if epoch % 500 == 0:\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      # Fetch a batch of testing data and transfer it to the device\n",
        "      test_images, test_labels = BrainImage_test.get_random_batch(batch_size=30)\n",
        "      test_images, test_labels = torch.from_numpy(test_images).to(device, dtype=torch.float32), torch.from_numpy(test_labels).to(device, dtype=torch.float32)\n",
        "\n",
        "      pred_labels = model(test_images)\n",
        "\n",
        "      pred_labels = F.interpolate(pred_labels, size=test_labels.shape[1:])\n",
        "\n",
        "      # Ensure they have the same shape\n",
        "      pred_labels = pred_labels.squeeze()  # Remove the channel dimension if it's 1\n",
        "      test_labels = test_labels.squeeze()  # Same for labels\n",
        "\n",
        "      test_loss = loss_fn(pred_labels, test_labels)\n",
        "      print(f\"Loss during training is {loss}. Loss turing testing is {test_loss} \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfbExfTOH8Ko",
        "outputId": "77b81e6b-5537-4d6f-faba-5dbc9d8e05db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device = cuda\n",
            "Loss during training is 0.750813364982605. Loss turing testing is 0.6852824091911316 \n"
          ]
        }
      ]
    }
  ]
}